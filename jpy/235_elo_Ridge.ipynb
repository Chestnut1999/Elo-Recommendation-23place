{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "is_stack = [True, False][0]\n",
    "debug = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "sys.path.append('../py/')\n",
    "from s027_kfold_ods import ods_kfold\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "from sklearn.linear_model import Ridge\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "out_part = ['', 'part', 'all'][0]\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "stack_name='ridge'\n",
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "model_type='ridge'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "def get_stack_dataset(lgb_path='', is_clf_out=False, is_no_out_flg=False, is_rm_out=False, is_binary=False, is_nn=False, is_rmf=False, is_ext=False, is_rid=False, is_random=False, seed=seed):\n",
    "    print(\"Preparing dataset...\")\n",
    "    base = utils.read_df_pkl('../input/base_no_out_clf.gz').set_index(key)\n",
    "    \n",
    "    #========================================================================\n",
    "    # Base Model Path\n",
    "    #========================================================================\n",
    "    # Clf Out Model\n",
    "    if is_clf_out: ens_list = glob.glob('../ensemble/clf_min_thres_ensemble/*.gz')\n",
    "    # No Out Flg Model\n",
    "    elif is_no_out_flg: ens_list = glob.glob('../no_out_flg_ensemble/*.gz')\n",
    "    elif is_rm_out: ens_list = glob.glob('../ensemble/rm_outlier_ensemble/*.gz')\n",
    "    elif is_binary:\n",
    "        model_type='lgr'\n",
    "        lgb_list = glob.glob('../stack/*binary*.gz')\n",
    "        nn_list = []\n",
    "        ens_list = lgb_list + nn_list\n",
    "    #========================================================================\n",
    "    # Base Model\n",
    "    else:\n",
    "        if is_random:\n",
    "            np.random.seed(seed)\n",
    "            lgb_list = list(np.random.choice(lgb_list, 10))\n",
    "#             nn_list = list(np.random.choice(nn_list, 1))\n",
    "        nn_list = []\n",
    "        rid_list = []\n",
    "        ext_list = []\n",
    "        rmf_list = []\n",
    "        if is_nn : nn_list = glob.glob('../ensemble/NN_ensemble/*CV3*.gz')\n",
    "        if is_rmf: rmf_list = glob.glob('../ensemble/various_model/*rmf*.gz')\n",
    "        if is_ext: ext_list = glob.glob('../ensemble/various_model/*ext*.gz')\n",
    "        if is_rid: rid_list = glob.glob('../ensemble/various_model/*ridge*.gz')\n",
    "        lgb_list = glob.glob(lgb_path)\n",
    "#         lgb_list = glob.glob(lgb_path) + glob.glob('../ensemble/rm_outlier_ensemble/tmp/*.gz')\n",
    "        ens_list = lgb_list + nn_list + rid_list + rmf_list + ext_list\n",
    "    \n",
    "    #========================================================================\n",
    "    # Stack Models Load\n",
    "    from joblib import Parallel, delayed\n",
    "    def parallel_stack_model(model_path):\n",
    "        try:\n",
    "            cv = re.search(r'CV([^/.]*)_LB.gz', model_path).group(1)\n",
    "        except AttributeError:\n",
    "            cv = re.search(r'CV([^/.]*).gz', model_path.replace('.', '-')).group(1)\n",
    "        tmp = utils.read_pkl_gzip(model_path)\n",
    "        if key not in tmp.columns:\n",
    "            tmp.reset_index(inplace=True)\n",
    "        if 'pred_mean' in tmp.columns:\n",
    "            tmp = tmp[[key, 'pred_mean']]\n",
    "        else:\n",
    "            tmp = tmp[[key, 'prediction']]\n",
    "            \n",
    "        if model_path.count('lgb'):\n",
    "            tmp.columns = [key, f\"base_lgb_{cv}\"]\n",
    "        elif model_path.count('NN'):\n",
    "            tmp.columns = [key, f\"base_NN_{cv}\"]\n",
    "        elif model_path.count('ridge'):\n",
    "            tmp.columns = [key, f\"base_ridge_{cv}\"]\n",
    "        elif model_path.count('rmf'):\n",
    "            tmp.columns = [key, f\"base_rmf_{cv}\"]\n",
    "        elif model_path.count('ext'):\n",
    "            tmp.columns = [key, f\"base_ext_{cv}\"]\n",
    "        else:\n",
    "            tmp.columns = [key, f\"base_model_{cv}\"]\n",
    "        return tmp.set_index(key)\n",
    "    #========================================================================\n",
    "    \n",
    "    p_list = Parallel(n_jobs=-1)([delayed(parallel_stack_model)(model_path) for model_path in ens_list])\n",
    "    df_pred = pd.concat(p_list, axis=1)\n",
    "    if is_rm_out:\n",
    "        cv15 = [col for col in df_pred.columns if col.count('1-5')]\n",
    "        cv8 = [col for col in df_pred.columns if col.count('8-')]\n",
    "        df_pred['tmp_mean'] = df_pred[cv8].mean(axis=1).values\n",
    "        for col in cv15:\n",
    "            df_pred.loc[df_pred[col].isnull(), col] = df_pred.loc[df_pred[col].isnull(), 'tmp_mean']\n",
    "    base = base.join(df_pred)\n",
    "    \n",
    "    #========================================================================\n",
    "    \n",
    "    if key in base.columns:\n",
    "        train = base[~base[target].isnull()]\n",
    "        test = base[base[target].isnull()]\n",
    "    else:\n",
    "        train = base[~base[target].isnull()].reset_index()\n",
    "        test = base[base[target].isnull()].reset_index()\n",
    "    \n",
    "    if is_rm_out:\n",
    "        train = train[~train[target].isnull()]\n",
    "    elif is_clf_out:\n",
    "        train = train[train['clf_pred']<0.01]\n",
    "        test = test[test['clf_pred']<0.01]\n",
    "    elif is_binary:\n",
    "        train[target] = train[target].map(lambda x: 1 if x<-30 else 0)\n",
    "        \n",
    "    display(train.head())\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.81it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>hist_purchase_date_max</th>\n",
       "      <th>hist_purchase_month_max</th>\n",
       "      <th>hist_purchase_date_min</th>\n",
       "      <th>hist_purchase_month_min</th>\n",
       "      <th>new_purchase_date_max</th>\n",
       "      <th>new_purchase_month_max</th>\n",
       "      <th>new_purchase_date_min</th>\n",
       "      <th>new_purchase_month_min</th>\n",
       "      <th>hist_personal_term</th>\n",
       "      <th>new_personal_term</th>\n",
       "      <th>hist_regist_term</th>\n",
       "      <th>new_regist_term</th>\n",
       "      <th>no_out_flg</th>\n",
       "      <th>clf_pred</th>\n",
       "      <th>base_lgb_1-5523344089537545</th>\n",
       "      <th>base_lgb_1-5519658462380022</th>\n",
       "      <th>base_lgb_1-548333413482797</th>\n",
       "      <th>base_lgb_1-5520199746035688</th>\n",
       "      <th>base_lgb_1-5539440125280584</th>\n",
       "      <th>base_lgb_1-548333413482797</th>\n",
       "      <th>base_lgb_1-548333413482797</th>\n",
       "      <th>base_lgb_1-6120376936609138</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2018-02-25 09:31:15</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-06-27 14:18:08</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2018-04-29 11:23:05</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-05 14:04:36</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>-0.331557</td>\n",
       "      <td>-0.307468</td>\n",
       "      <td>-0.307369</td>\n",
       "      <td>-0.305220</td>\n",
       "      <td>-0.261656</td>\n",
       "      <td>-0.307369</td>\n",
       "      <td>-0.307369</td>\n",
       "      <td>-0.321531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-01-31 22:31:09</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2017-01-06 16:29:42</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-03-30 06:48:26</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2018-02-01 17:07:54</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>13</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.177615</td>\n",
       "      <td>0.375093</td>\n",
       "      <td>0.172662</td>\n",
       "      <td>0.446242</td>\n",
       "      <td>0.340820</td>\n",
       "      <td>0.172662</td>\n",
       "      <td>0.172662</td>\n",
       "      <td>0.089840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>2018-02-27 19:08:25</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-01-11 08:21:22</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-04-28 17:43:11</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-04-28 17:43:11</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004074</td>\n",
       "      <td>0.623674</td>\n",
       "      <td>0.774051</td>\n",
       "      <td>1.078970</td>\n",
       "      <td>0.751869</td>\n",
       "      <td>0.717356</td>\n",
       "      <td>1.078970</td>\n",
       "      <td>1.078970</td>\n",
       "      <td>0.427025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2018-02-28 11:44:40</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-09-26 16:22:21</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2018-04-18 11:00:11</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-07 11:55:06</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.237155</td>\n",
       "      <td>0.220907</td>\n",
       "      <td>0.184801</td>\n",
       "      <td>0.130344</td>\n",
       "      <td>0.215311</td>\n",
       "      <td>0.184801</td>\n",
       "      <td>0.184801</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2018-02-28 20:40:41</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-11-12 00:00:00</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2018-04-28 18:50:25</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-02 11:55:43</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>-0.230528</td>\n",
       "      <td>-0.278797</td>\n",
       "      <td>-0.262518</td>\n",
       "      <td>-0.175082</td>\n",
       "      <td>-0.163671</td>\n",
       "      <td>-0.262518</td>\n",
       "      <td>-0.262518</td>\n",
       "      <td>-0.354860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target first_active_month hist_purchase_date_max  \\\n",
       "0  C_ID_92a2005557 -0.820283         2017-06-01    2018-02-25 09:31:15   \n",
       "1  C_ID_3d0044924f  0.392913         2017-01-01    2018-01-31 22:31:09   \n",
       "2  C_ID_d639edf6cd  0.688056         2016-08-01    2018-02-27 19:08:25   \n",
       "3  C_ID_186d6a6901  0.142495         2017-09-01    2018-02-28 11:44:40   \n",
       "4  C_ID_cdbd2c0db2 -0.159749         2017-11-01    2018-02-28 20:40:41   \n",
       "\n",
       "  hist_purchase_month_max hist_purchase_date_min hist_purchase_month_min  \\\n",
       "0              2018-03-01    2017-06-27 14:18:08              2017-06-01   \n",
       "1              2018-02-01    2017-01-06 16:29:42              2017-01-01   \n",
       "2              2018-03-01    2017-01-11 08:21:22              2017-01-01   \n",
       "3              2018-03-01    2017-09-26 16:22:21              2017-09-01   \n",
       "4              2018-03-01    2017-11-12 00:00:00              2017-11-01   \n",
       "\n",
       "  new_purchase_date_max new_purchase_month_max new_purchase_date_min  \\\n",
       "0   2018-04-29 11:23:05             2018-05-01   2018-03-05 14:04:36   \n",
       "1   2018-03-30 06:48:26             2018-04-01   2018-02-01 17:07:54   \n",
       "2   2018-04-28 17:43:11             2018-05-01   2018-04-28 17:43:11   \n",
       "3   2018-04-18 11:00:11             2018-05-01   2018-03-07 11:55:06   \n",
       "4   2018-04-28 18:50:25             2018-05-01   2018-03-02 11:55:43   \n",
       "\n",
       "  new_purchase_month_min  hist_personal_term  new_personal_term  \\\n",
       "0             2018-03-01                   9                2.0   \n",
       "1             2018-02-01                  13                2.0   \n",
       "2             2018-04-01                  14                1.0   \n",
       "3             2018-03-01                   6                2.0   \n",
       "4             2018-03-01                   4                2.0   \n",
       "\n",
       "   hist_regist_term  new_regist_term  no_out_flg  clf_pred  \\\n",
       "0                 9             11.0         1.0  0.000444   \n",
       "1                13             15.0         0.0  0.007831   \n",
       "2                19             18.0         0.0  0.004074   \n",
       "3                 6              8.0         0.0  0.000797   \n",
       "4                 4              6.0         1.0  0.000251   \n",
       "\n",
       "   base_lgb_1-5523344089537545  base_lgb_1-5519658462380022  \\\n",
       "0                    -0.331557                    -0.307468   \n",
       "1                     0.177615                     0.375093   \n",
       "2                     0.623674                     0.774051   \n",
       "3                     0.237155                     0.220907   \n",
       "4                    -0.230528                    -0.278797   \n",
       "\n",
       "   base_lgb_1-548333413482797  base_lgb_1-5520199746035688  \\\n",
       "0                   -0.307369                    -0.305220   \n",
       "1                    0.172662                     0.446242   \n",
       "2                    1.078970                     0.751869   \n",
       "3                    0.184801                     0.130344   \n",
       "4                   -0.262518                    -0.175082   \n",
       "\n",
       "   base_lgb_1-5539440125280584  base_lgb_1-548333413482797  \\\n",
       "0                    -0.261656                   -0.307369   \n",
       "1                     0.340820                    0.172662   \n",
       "2                     0.717356                    1.078970   \n",
       "3                     0.215311                    0.184801   \n",
       "4                    -0.163671                   -0.262518   \n",
       "\n",
       "   base_lgb_1-548333413482797  base_lgb_1-6120376936609138  \n",
       "0                   -0.307369                    -0.321531  \n",
       "1                    0.172662                     0.089840  \n",
       "2                    1.078970                     0.427025  \n",
       "3                    0.184801                     0.326829  \n",
       "4                   -0.262518                    -0.354860  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (166425, 14) | Valid: (33285, 14) | Test: (123623, 14)\n",
      "RMSE: 1.5367152258797705 | SUM ERROR: 97.18596026566395\n",
      "Train: (166425, 14) | Valid: (33285, 14) | Test: (123623, 14)\n",
      "RMSE: 1.557248851921376 | SUM ERROR: 50.47111475958603\n",
      "Train: (166425, 14) | Valid: (33285, 14) | Test: (123623, 14)\n",
      "RMSE: 1.5451879893948202 | SUM ERROR: 3.2823174323282913\n",
      "Train: (166425, 14) | Valid: (33285, 14) | Test: (123623, 14)\n",
      "RMSE: 1.540700713945601 | SUM ERROR: 56.41614956965871\n",
      "Train: (166425, 14) | Valid: (33285, 14) | Test: (123623, 14)\n",
      "RMSE: 1.5480472603914097 | SUM ERROR: -242.52512810677712\n",
      "Train: (166425, 14) | Valid: (33285, 14) | Test: (123623, 14)\n",
      "RMSE: 1.5462053957231616 | SUM ERROR: 29.42315150899819\n",
      "Stacking Shape: (323333, 3)\n",
      "\n",
      "#========================================================================\n",
      "# CV SCORE AVG: 1.54568423954269\n",
      "# OUT SCORE: 0\n",
      "#========================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base = utils.read_df_pkl('../input/base_no_out_clf.gz')[[key, target]].set_index(key)\n",
    "valid_type = ['ods', 'pmo' ,'pm' ,'term'][0]\n",
    "# lgb_path = '../ensemble/pmo_all_stack_level1/*.gz'\n",
    "# lgb_path = '../model/LB3664_set/*_lgb_*.gz'\n",
    "# lgb_path = '../ensemble/dir_stack_blend/*_lgb_*.gz'\n",
    "lgb_path = '../ensemble/rm_outlier_ensemble/tmp/*_lgb_*.gz'\n",
    "#========================================================================\n",
    "# Make Dataset \n",
    "is_clf_out = [True, False][1]\n",
    "is_no_out_flg = [True, False][1]\n",
    "is_rm_out = [True, False][0]\n",
    "is_binary = [True, False][1]\n",
    "is_blend = [True, False][1]\n",
    "is_nn = 0\n",
    "is_rid = 0\n",
    "is_rmf = 0\n",
    "is_ext = 0\n",
    "is_random = 0\n",
    "seed_size = 1\n",
    "pred_col = 'prediction'\n",
    "#========================================================================\n",
    "    \n",
    "#========================================================================\n",
    "# CVの準備\n",
    "seed = 328\n",
    "fold_seed = 328\n",
    "fold_seed = 1208\n",
    "seed_list = [328, 1208]\n",
    "seed_list = [328]\n",
    "fold = 6\n",
    "\n",
    "if is_rm_out:\n",
    "    set_type = 'rm_out'\n",
    "else:\n",
    "    set_type = 'all'\n",
    "\n",
    "#========================================================================\n",
    "# Dataset\n",
    "submit = pd.read_csv('../input/sample_submission.csv').set_index(key)\n",
    "result_list = []\n",
    "score_list = []\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg', 'clf_pred']\n",
    "#========================================================================\n",
    "    \n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "params = {}\n",
    "if is_binary:\n",
    "    params['n_jobs']=-1\n",
    "    params['C']=1.0\n",
    "    params['solver'] ='liblinear'\n",
    "    params['fit_intercept']=True\n",
    "    params['max_iter']=1000\n",
    "    params['tol']=0.01\n",
    "    params['random_state']=seed\n",
    "    model = LogisticRegression(**params)\n",
    "else:\n",
    "    params['solver'] ='auto'\n",
    "    params['fit_intercept']=True\n",
    "    params['alpha']=0.4\n",
    "    params['max_iter']=1000\n",
    "    params['normalize']=False\n",
    "    params['tol']=0.01\n",
    "    params['random_state']=seed\n",
    "    model = Ridge(**params)\n",
    "\n",
    "# np.random.seed(int(time.time()))\n",
    "# seed_list = np.random.randint(10**7, size=seed_size)\n",
    "\n",
    "# for seed in seed_list:\n",
    "for fold_seed in seed_list:\n",
    "    \n",
    "    if is_rm_out:\n",
    "        kfold = utils.read_pkl_gzip('../input/kfold_ods_no_out_fold6_seed328.gz')\n",
    "    elif is_clf_out:\n",
    "        kfold = utils.read_pkl_gzip('../input/kfold_ods_clf_out_fold6_seed328.gz')\n",
    "    else:\n",
    "#         kfold = utils.read_pkl_gzip(f'../input/kfold_{valid_type}_all_fold6_seed{fold_seed}.gz')\n",
    "        kfold = utils.read_pkl_gzip(f'../input/kfold_ods_equal_seed328.gz')\n",
    "    \n",
    "    train, test = get_stack_dataset(lgb_path=lgb_path, is_rmf=is_rmf, is_ext=is_ext, is_random=is_random, seed=seed)\n",
    "    if is_rm_out:\n",
    "        train = train[train[target]>-30]\n",
    "        \n",
    "    #========================================================================\n",
    "    # Preset\n",
    "    use_cols = sorted([col for col in train.columns if col.count('base_')])\n",
    "    lgb_list = [col for col in use_cols if col.count('lgb')]\n",
    "    nn_list = [col for col in use_cols if col.count('NN')]\n",
    "    ext_list = [col for col in use_cols if col.count('ext')]\n",
    "    best_score = 100\n",
    "    best_score_list = []\n",
    "    test_pred = np.zeros(len(test))\n",
    "    Y = train[target]\n",
    "    result_list = []\n",
    "    #========================================================================\n",
    "\n",
    "    #========================================================================\n",
    "    # Train & Prediction Start\n",
    "    for fold_no, (trn_idx, val_idx) in enumerate(zip(*kfold)):\n",
    "        if is_blend:\n",
    "            break\n",
    "            \n",
    "        if key not in train.columns:\n",
    "            train = train.reset_index()\n",
    "            test = test.reset_index() \n",
    "             \n",
    "        #========================================================================\n",
    "        # Make Dataset\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(pd.concat([train[use_cols], test[use_cols]]))\n",
    "        x_test = scaler.transform(test[use_cols])\n",
    "\n",
    "        X_train, y_train = train.loc[train[key].isin(trn_idx), :][use_cols], Y.loc[train[key].isin(trn_idx)]\n",
    "        X_val, y_val = train.loc[train[key].isin(val_idx), :][use_cols], Y.loc[train[key].isin(val_idx)]\n",
    "        \n",
    "        X_train[:] = scaler.transform(X_train)\n",
    "        X_val[:] = scaler.transform(X_val)\n",
    "        X_train = X_train.as_matrix()\n",
    "        X_val = X_val.as_matrix()\n",
    "    \n",
    "        print(f\"Train: {X_train.shape} | Valid: {X_val.shape} | Test: {x_test.shape}\")\n",
    "        #========================================================================\n",
    "        \n",
    "        # Fitting\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Prediction\n",
    "        if is_binary:\n",
    "            y_pred = model.predict_proba(X_val)[:, 1]\n",
    "#         elif is_rm_out:\n",
    "#             X_val = train.loc[~train[key].isin(trn_idx), :]\n",
    "#             y_pred = model.predict(X_val[use_cols])\n",
    "#             y_val = X_val[target].values\n",
    "        else:\n",
    "            y_pred = model.predict(X_val)\n",
    "        \n",
    "        test_pred += model.predict(x_test)\n",
    "        \n",
    "        # Stack Prediction\n",
    "#         if is_rm_out:\n",
    "        if False:\n",
    "            if fold_no==0:\n",
    "                df_pred = train[[key, target]].set_index(key)\n",
    "            self_valid = X_val[[key, target]].set_index(key)\n",
    "            self_valid[f'pred_{fold_no}'] = y_pred\n",
    "            df_pred = df_pred.join(self_valid.drop(target, axis=1))\n",
    "        else:\n",
    "            df_pred = train.loc[train[key].isin(val_idx), :][[key, target]].copy()\n",
    "            df_pred['prediction'] = y_pred\n",
    "            result_list.append(df_pred)\n",
    "        \n",
    "        # Scoring\n",
    "        err = (y_val - y_pred)\n",
    "        if is_binary:\n",
    "            score = np.sqrt(roc_auc_score(y_val, y_pred))\n",
    "            print(f'AUC: {score} | SUM ERROR: {err.sum()}')\n",
    "        else:\n",
    "            score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "        score_list.append(score)\n",
    "        #========================================================================\n",
    "    \n",
    "    if not(is_blend):\n",
    "        cv_score = np.mean(score_list)\n",
    "        \n",
    "        #========================================================================\n",
    "        # Stacking\n",
    "        test_pred /= fold_no+1\n",
    "        test['prediction'] = test_pred\n",
    "        stack_test = test[[key, 'prediction']]\n",
    "        \n",
    "#         if is_rm_out:\n",
    "#             pred_col_list = [col for col in df_pred.columns if col.count('pred_')]\n",
    "#             df_pred['prediction'] = df_pred[pred_col_list].mean(axis=1)\n",
    "#             result_list.append(df_pred.reset_index())\n",
    "        \n",
    "        result_list.append(stack_test)\n",
    "        df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "        if key not in base:\n",
    "            base.reset_index(inplace=True)\n",
    "        df_pred = base[[key, target]].merge(df_pred, how='inner', on=key)\n",
    "    else:\n",
    "        #========================================================================\n",
    "        # Blender\n",
    "        train[pred_col] = train[use_cols].mean(axis=1)\n",
    "        test[pred_col] = test[use_cols].mean(axis=1)\n",
    "        y_pred = train[pred_col].values\n",
    "        y_val = train[target].values\n",
    "        \n",
    "        score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "        #========================================================================\n",
    "            \n",
    "        stack_col = [key, target, pred_col]\n",
    "        df_pred = pd.concat([train[stack_col], test[stack_col]], axis=0, ignore_index=True)\n",
    "    print(f\"Stacking Shape: {df_pred.shape}\")\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # outlierに対するスコアを出す\n",
    "    if is_rm_out or is_binary:\n",
    "        out_score = 0\n",
    "    else:\n",
    "        if key not in train.columns:\n",
    "            train.reset_index(inplace=True)\n",
    "        out_ids = train.loc[train.target<-30, key].values\n",
    "        out_val = train.loc[train.target<-30, target].values\n",
    "        out_pred = df_pred[df_pred[key].isin(out_ids)]['prediction'].values\n",
    "        out_score = np.sqrt(mean_squared_error(out_val, out_pred))\n",
    "    #========================================================================\n",
    "    \n",
    "    if cv_score<best_score:\n",
    "        print(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "# OUT SCORE: {out_score}\n",
    "#========================================================================''')\n",
    "    \n",
    "        best_score = cv_score\n",
    "        best_score_list = use_cols\n",
    "    \n",
    "        #========================================================================\n",
    "        # Save Stack\n",
    "        utils.to_pkl_gzip(path=f\"../stack/{start_time[4:12]}_stack_{model_type}_set-{set_type}_valid-{valid_type}-seed{fold_seed}_lgb{len(lgb_list)}_NN{is_nn}_ridge{is_rid}_ext{is_ext}_rmf{is_rmf}_OUT{str(out_score)[:7]}_CV{cv_score}_LB\" , obj=df_pred[[key, 'prediction']])\n",
    "        #========================================================================\n",
    "sys.exit()\n",
    "    \n",
    "#========================================================================\n",
    "# Submission\n",
    "df_pred.set_index(key, inplace=True)\n",
    "submit[target] = df_pred['prediction']\n",
    "submit_path = f'../submit/{start_time[4:12]}_submit_{model_type}_set-{set_type}_lgb{len(lgb_list)}_NN{len(nn_list)}_other{len(other_list)}_OUT{str(out_score)[:7]}_CV{cv_score}_LB.csv'\n",
    "submit.to_csv(submit_path, index=True)\n",
    "display(submit.head())\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_lgb_1-5486783250130114\n",
      "base_lgb_1-5520199746035688\n",
      "base_lgb_1-5520923703589151\n",
      "base_lgb_1-553086826800966\n",
      "base_lgb_1-5540396414459288\n"
     ]
    }
   ],
   "source": [
    "for col in use_cols:\n",
    "    tmp = train[col].isnull().sum()\n",
    "    if tmp:\n",
    "        print(col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
