{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.73it/s]\n"
     ]
    }
   ],
   "source": [
    "is_stack = [True, False][0]\n",
    "debug = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "sys.path.append('../py/')\n",
    "from s027_kfold_ods import ods_kfold\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "from sklearn.linear_model import Ridge\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "out_part = ['', 'part', 'all'][0]\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "stack_name='ridge'\n",
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "model_type='ridge'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "print(\"Preparing dataset...\")\n",
    "\n",
    "if is_stack:\n",
    "    is_clf_out = [True, False][1]\n",
    "    is_no_out_flg = [True, False][1]\n",
    "    is_rm_out = [True, False][1]\n",
    "    #========================================================================\n",
    "    # Base Model Load\n",
    "    base = utils.read_df_pkl('../input/base_no_out_clf.gz').set_index(key)\n",
    "    ens_list = glob.glob('../ensemble/*.gz')\n",
    "    # ens_list = ['../ensemble/0206_125_stack_lgb_lr0.01_235feats_10seed_70leaves_iter1164_OUT29.8269_CV3-6215750935280235_LB.gz']\n",
    "    for path in ens_list:\n",
    "        try:\n",
    "            cv = re.search(r'CV([^/.]*)_LB.gz', path).group(1)\n",
    "        except AttributeError:\n",
    "            cv = re.search(r'CV([^/.]*).gz', path.replace('.', '-')).group(1)\n",
    "        \n",
    "        try:\n",
    "            blend = utils.read_df_pkl(path).set_index(key)['pred_mean']\n",
    "        except KeyError:\n",
    "            blend = utils.read_df_pkl(path).set_index(key)['prediction']\n",
    "        if path.count('lgb'):\n",
    "            base[f'base_lgb_{cv}'] = blend\n",
    "        elif path.count('NN'):\n",
    "            base[f'base_NN_{cv}'] = blend\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # Clf Out Model\n",
    "    if is_clf_out:\n",
    "        out_list = glob.glob('../clf_min_thres_ensemble/*.gz')\n",
    "        out_list = ['../clf_min_thres_ensemble/0212_005_clf_out_lgb_out_partclf_out_row167982_lr0.01_235feats_10seed_57leaves_iter1062_OUT0_CV2-1684031012798815_LB.gz', '../clf_min_thres_ensemble/0211_144_elo_NN_stack_E1_row167982_235feat_const2_lr0.001_batch128_epoch30_CV2.1341374191726357.gz']\n",
    "        for path in out_list:\n",
    "            cv = re.search(r'CV([^/.]*).gz', path.replace('.', '-')).group(1)\n",
    "            try:\n",
    "                blend = utils.read_pkl_gzip(path).set_index(key)['pred_mean']\n",
    "            except KeyError:\n",
    "                blend = utils.read_pkl_gzip(path).set_index(key)['prediction']\n",
    "            base[f\"clf_out_{cv}\"] = blend\n",
    "        out_cols = [col for col in base.columns if col.count('clf_out_2')]\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # No Out Flg Model\n",
    "    if is_no_out_flg:\n",
    "        no_out_flg_list = glob.glob('../no_out_flg_ensemble/*.gz')\n",
    "        no_out_flg_list = ['../no_out_flg_ensemble/0212_010_no_out_flg_lgb_out_partno_out_flg_row40858_lr0.01_235feats_10seed_57leaves_iter784_OUT0_CV1-4795858837810465_LB.gz']\n",
    "    \n",
    "        for path in no_out_flg_list:\n",
    "            cv = re.search(r'CV([^/.]*).gz', path.replace('.', '-')).group(1)\n",
    "            try:\n",
    "                blend = utils.read_pkl_gzip(path).set_index(key)['pred_mean']\n",
    "            except KeyError:\n",
    "                blend = utils.read_pkl_gzip(path).set_index(key)['prediction']\n",
    "            base[f\"no_out_flg_{cv}\"] = blend\n",
    "        no_out_cols = [col for col in base.columns if col.count('no_out_flg_1')]\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # Classifier\n",
    "    clf = utils.read_pkl_gzip('../stack/0207_224_outlier_classify_9seed_lgb_binary_CV0-9099420278047783_235features.gz')[[key, 'pred_mean']].set_index(key)\n",
    "    clf_2 = utils.read_pkl_gzip('../stack/0207_212_outlier_classify_9seed_lgb_binary_CV0-9084737642836664_235features.gz')[[key, 'pred_mean']].set_index(key)\n",
    "    clf['pred_mean_2'] = clf_2['pred_mean']\n",
    "    clf['clf_pred'] =  clf['pred_mean'].values*0.9 + clf['pred_mean_2'].values*0.1\n",
    "    # clf['clf_pred'] =  clf['pred_mean']\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # Indexをそろえる\n",
    "    base['clf_pred'] = clf['clf_pred']\n",
    "#     base_cols = [col for col in base.columns if col.count('base_')]\n",
    "    base_lgb_cols = [col for col in base.columns if col.count('base_lgb')]\n",
    "    base_NN_cols = [col for col in base.columns if col.count('base_NN')]\n",
    "    # base = base[[target, 'clf_pred', 'no_out_flg'] + base_cols + out_cols + no_out_cols]\n",
    "    df_stack = base[[target, 'no_out_flg'] + base_lgb_cols + base_NN_cols]\n",
    "    #========================================================================\n",
    "    \n",
    "else:\n",
    "    win_path = f'../features/4_winner/*.gz'\n",
    "    # Ensemble 1\n",
    "    win_path = f'../model/LB3670_70leaves_colsam0322/*.gz'\n",
    "    # Ensemble 2\n",
    "    # win_path = f'../model/E2_lift_set/*.gz'\n",
    "    # Ensemble 3\n",
    "    # win_path = f'../model/E3_PCA_set/*.gz'\n",
    "    \n",
    "    win_path_list = glob.glob(win_path)\n",
    "    \n",
    "    base = utils.read_df_pkl('../input/base_term*0*')[[key, target, 'first_active_month']]\n",
    "    base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "    base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "    feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
    "    df = pd.concat(feature_list, axis=1)\n",
    "    train = pd.concat([base_train, df.iloc[:len(base_train), :]], axis=1)\n",
    "    test = pd.concat([base_test, df.iloc[len(base_train):, :].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "    test.reset_index(inplace=True , drop=True)\n",
    "\n",
    "if debug:\n",
    "    train = train.head(10000)\n",
    "    test = test.head(2000)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# 正規化の前処理(Null埋め, inf, -infの処理) \n",
    "for col in train.columns:\n",
    "    if col in ignore_list: continue\n",
    "        \n",
    "    train[col] = impute_feature(train, col)\n",
    "    test[col] = impute_feature(test, col)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Cleansing Check\n",
    "def clean_check(df, col):\n",
    "#     if col in ignore_list: continue\n",
    "#     train[col] = impute_feature(train, col)\n",
    "#     test[col] = impute_feature(test, col)\n",
    "    length = len(df)\n",
    "    tmp = df[col].dropna().shape[0]\n",
    "    if length - tmp>0:\n",
    "        print(f\"Null is {length-tmp}\")\n",
    "        \n",
    "    inf_max = df[col].max()\n",
    "    inf_min = df[col].min()\n",
    "    inf_max_2 = df[col].sort_values().values[-1]\n",
    "    inf_min_2 = df[col].sort_values().values[0]\n",
    "    if inf_max==np.inf or inf_min==-np.inf:\n",
    "        print(1, col, inf_max, inf_min)\n",
    "    if inf_max_2==np.inf or inf_min_2==-np.inf:\n",
    "        print(2, col, inf_max, inf_min)\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "      <th>no_out_flg</th>\n",
       "      <th>base_lgb_3-6206463759490277</th>\n",
       "      <th>base_lgb_3-63947267133467</th>\n",
       "      <th>base_lgb_3-64344499327249</th>\n",
       "      <th>base_lgb_3-6439834621816565</th>\n",
       "      <th>base_NN_3-766855322520765</th>\n",
       "      <th>base_NN_3-7790332090708425</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.346904</td>\n",
       "      <td>-0.298933</td>\n",
       "      <td>-0.208776</td>\n",
       "      <td>-0.355493</td>\n",
       "      <td>-0.210441</td>\n",
       "      <td>-0.276462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.568272</td>\n",
       "      <td>-0.405721</td>\n",
       "      <td>-0.318021</td>\n",
       "      <td>-0.485598</td>\n",
       "      <td>0.808078</td>\n",
       "      <td>0.175963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.683088</td>\n",
       "      <td>0.568874</td>\n",
       "      <td>0.524642</td>\n",
       "      <td>0.592692</td>\n",
       "      <td>0.895201</td>\n",
       "      <td>0.644744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173934</td>\n",
       "      <td>0.159495</td>\n",
       "      <td>0.168799</td>\n",
       "      <td>0.016254</td>\n",
       "      <td>0.482479</td>\n",
       "      <td>0.332322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.109142</td>\n",
       "      <td>-0.302391</td>\n",
       "      <td>-0.029702</td>\n",
       "      <td>-0.133293</td>\n",
       "      <td>-0.339484</td>\n",
       "      <td>-0.057478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target  no_out_flg  base_lgb_3-6206463759490277  \\\n",
       "0  C_ID_92a2005557 -0.820283         1.0                    -0.346904   \n",
       "1  C_ID_3d0044924f  0.392913         0.0                    -0.568272   \n",
       "2  C_ID_d639edf6cd  0.688056         0.0                     0.683088   \n",
       "3  C_ID_186d6a6901  0.142495         0.0                     0.173934   \n",
       "4  C_ID_cdbd2c0db2 -0.159749         1.0                    -0.109142   \n",
       "\n",
       "   base_lgb_3-63947267133467  base_lgb_3-64344499327249  \\\n",
       "0                  -0.298933                  -0.208776   \n",
       "1                  -0.405721                  -0.318021   \n",
       "2                   0.568874                   0.524642   \n",
       "3                   0.159495                   0.168799   \n",
       "4                  -0.302391                  -0.029702   \n",
       "\n",
       "   base_lgb_3-6439834621816565  base_NN_3-766855322520765  \\\n",
       "0                    -0.355493                  -0.210441   \n",
       "1                    -0.485598                   0.808078   \n",
       "2                     0.592692                   0.895201   \n",
       "3                     0.016254                   0.482479   \n",
       "4                    -0.133293                  -0.339484   \n",
       "\n",
       "   base_NN_3-7790332090708425  \n",
       "0                   -0.276462  \n",
       "1                    0.175963  \n",
       "2                    0.644744  \n",
       "3                    0.332322  \n",
       "4                   -0.057478  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_lgb_cols = sorted(base_lgb_cols)\n",
    "base_NN_cols = sorted(base_NN_cols)\n",
    "stack_cols = base_lgb_cols + base_NN_cols\n",
    "# stack_cols = base_lgb_cols\n",
    "if key in base.columns:\n",
    "    df_stack = base[[key, target, 'no_out_flg'] + stack_cols]\n",
    "else:\n",
    "    df_stack = base[[target, 'no_out_flg'] + stack_cols]\n",
    "\n",
    "if key in df_stack.columns:\n",
    "    train = df_stack[~df_stack[target].isnull()]\n",
    "    test = df_stack[df_stack[target].isnull()]\n",
    "else:\n",
    "    train = df_stack[~df_stack[target].isnull()].reset_index()\n",
    "    test = df_stack[df_stack[target].isnull()].reset_index()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (201917, 9) | Test: (123623, 9)\n",
      "RMSE: 3.5417760271899903 | SUM ERROR: 1233.645569206881\n",
      "RMSE: 3.6520669388353353 | SUM ERROR: 151.69401459318834\n",
      "RMSE: 3.645319536331285 | SUM ERROR: -1050.706610428527\n",
      "RMSE: 3.5946274509871237 | SUM ERROR: -1444.359444298121\n",
      "RMSE: 3.600140492619795 | SUM ERROR: 1842.1538672907138\n",
      "RMSE: 3.628358406389664 | SUM ERROR: -629.295183034454\n",
      "Stacking Shape: (325540, 3)\n",
      "\n",
      "    #========================================================================\n",
      "    # CV SCORE AVG: 3.6103814753921992\n",
      "    # OUT SCORE: 29.93321181151248\n",
      "    #========================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_ID_0ab67a22ab</th>\n",
       "      <td>-1.645973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_130fd0cbdd</th>\n",
       "      <td>-0.655524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_b709037bc5</th>\n",
       "      <td>-1.969370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_d27d835a9f</th>\n",
       "      <td>-0.222077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_2b5e3df5c2</th>\n",
       "      <td>-1.431340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   target\n",
       "card_id                  \n",
       "C_ID_0ab67a22ab -1.645973\n",
       "C_ID_130fd0cbdd -0.655524\n",
       "C_ID_b709037bc5 -1.969370\n",
       "C_ID_d27d835a9f -0.222077\n",
       "C_ID_2b5e3df5c2 -1.431340"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#========================================================================\n",
    "# CVの準備\n",
    "fold = 6\n",
    "# kfold = ods_kfold(train=train, seed=328, fold=fold)\n",
    "# train.drop('rounded_target', axis=1, inplace=True)\n",
    "kfold = utils.read_pkl_gzip('../input/ods_kfold.gz')\n",
    "submit = pd.read_csv('../input/sample_submission.csv').set_index(key)\n",
    "model_list = []\n",
    "result_list = []\n",
    "score_list = []\n",
    "val_pred_list = []\n",
    "test_pred = np.zeros(len(test))\n",
    "\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "# ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term']\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([train[use_cols], test[use_cols]]))\n",
    "x_test = scaler.transform(test[use_cols])\n",
    "\n",
    "Y = train[target]\n",
    "y_mean = Y.mean()\n",
    "#========================================================================\n",
    "    \n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\")\n",
    "    \n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "fit_intercept = True\n",
    "alpha = 0.4\n",
    "max_iter = 1000\n",
    "normalize = False\n",
    "tol = 0.01\n",
    "param_list = np.arange(1, 10, 1)\n",
    "param_list = [0]\n",
    "# param_list = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "for param in param_list:\n",
    "\n",
    "    result_list = []\n",
    "    ridge = Ridge(solver='auto', fit_intercept=fit_intercept, alpha=alpha, max_iter=max_iter, normalize=normalize, tol=tol)\n",
    "#     ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # Train & Prediction Start\n",
    "    for fold_no, (trn_idx, val_idx) in enumerate(zip(*kfold)):\n",
    "        if key not in train.columns:\n",
    "            train = train[~train[target].isnull()].reset_index()\n",
    "            test = test[test[target].isnull()].reset_index() \n",
    "    \n",
    "        #========================================================================\n",
    "        # Make Dataset\n",
    "        X_train, y_train = train.loc[train[key].isin(trn_idx), :][use_cols], Y.loc[train[key].isin(trn_idx)]\n",
    "        X_val, y_val = train.loc[train[key].isin(val_idx), :][use_cols], Y.loc[train[key].isin(val_idx)]\n",
    "        \n",
    "        X_train[:] = scaler.transform(X_train)\n",
    "        X_val[:] = scaler.transform(X_val)\n",
    "        X_train = X_train.as_matrix()\n",
    "        X_val = X_val.as_matrix()\n",
    "        #========================================================================\n",
    "        \n",
    "        # Fitting\n",
    "        ridge.fit(X_train, y_train)\n",
    "        \n",
    "        # Prediction\n",
    "        y_pred = ridge.predict(X_val)\n",
    "        test_pred += ridge.predict(x_test)\n",
    "        \n",
    "        # Stack Prediction\n",
    "    #     df_pred = train.iloc[val_idx, :][[key, target]].copy()\n",
    "        df_pred = train.loc[train[key].isin(val_idx), :][[key, target]].copy()\n",
    "        df_pred['prediction'] = y_pred\n",
    "        result_list.append(df_pred)\n",
    "        \n",
    "        # Scoring\n",
    "        err = (y_val - y_pred)\n",
    "        score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "        score_list.append(score)\n",
    "        #========================================================================\n",
    "    \n",
    "    cv_score = np.mean(score_list)\n",
    "    \n",
    "    #========================================================================\n",
    "    # Stacking\n",
    "    test_pred /= fold_no+1\n",
    "    test['prediction'] = test_pred\n",
    "    stack_test = test[[key, 'prediction']]\n",
    "    result_list.append(stack_test)\n",
    "    df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "    if key not in base:\n",
    "        base.reset_index(inplace=True)\n",
    "    df_pred = base[[key, target]].merge(df_pred, how='inner', on=key)\n",
    "    print(f\"Stacking Shape: {df_pred.shape}\")\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # outlierに対するスコアを出す\n",
    "    if key not in train.columns:\n",
    "        train.reset_index(inplace=True)\n",
    "    out_ids = train.loc[train.target<-30, key].values\n",
    "    out_val = train.loc[train.target<-30, target].values\n",
    "    out_pred = df_pred[df_pred[key].isin(out_ids)]['prediction'].values\n",
    "    out_score = np.sqrt(mean_squared_error(out_val, out_pred))\n",
    "    #========================================================================\n",
    "    \n",
    "    print(f'''\n",
    "    #========================================================================\n",
    "    # CV SCORE AVG: {cv_score}\n",
    "    # OUT SCORE: {out_score}\n",
    "    #========================================================================''')\n",
    "    \n",
    "#========================================================================\n",
    "# Submission\n",
    "df_pred.set_index(key, inplace=True)\n",
    "submit[target] = df_pred['prediction']\n",
    "submit_path = f'../submit/{start_time[4:12]}_submit_RIDGE_STACKING_{model_type}_{len(use_cols)}models_OUT{str(out_score)[:7]}_CV{cv_score}_LB.csv'\n",
    "submit.to_csv(submit_path, index=True)\n",
    "display(submit.head())\n",
    "#========================================================================\n",
    "\n",
    "# Save Stack\n",
    "# utils.to_pkl_gzip(path=f\"../stack/{start_time[4:12]}_stack_{model_type}_alpha{alpha}_{len(use_cols)}feats_tol{tol}_iter{max_iter}_OUT{str(out_score)[:7]}_CV{str(cv_score).replace('.', '-')}_LB\" , obj=df_pred)\n",
    "#========================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
