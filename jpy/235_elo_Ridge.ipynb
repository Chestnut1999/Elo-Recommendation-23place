{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "is_stack = [True, False][0]\n",
    "debug = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "sys.path.append('../py/')\n",
    "from s027_kfold_ods import ods_kfold\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "from sklearn.linear_model import Ridge\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "out_part = ['', 'part', 'all'][0]\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "stack_name='ridge'\n",
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "model_type='ridge'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "def get_stack_dataset(lgb_path='', is_clf_out=False, is_no_out_flg=False, is_rm_out=False, is_binary=False, is_nn=False, is_rmf=False, is_ext=False, is_rid=False, is_random=False, seed=seed):\n",
    "    print(\"Preparing dataset...\")\n",
    "    base = utils.read_df_pkl('../input/base_no_out_clf.gz').set_index(key)\n",
    "    \n",
    "    #========================================================================\n",
    "    # Base Model Path\n",
    "    #========================================================================\n",
    "    # Clf Out Model\n",
    "    if is_clf_out: ens_list = glob.glob('../ensemble/clf_min_thres_ensemble/*.gz')\n",
    "    # No Out Flg Model\n",
    "    elif is_no_out_flg: ens_list = glob.glob('../no_out_flg_ensemble/*.gz')\n",
    "    elif is_rm_out: ens_list = glob.glob('../ensemble/rm_outlier_ensemble/*.gz')\n",
    "    elif is_binary:\n",
    "        model_type='lgr'\n",
    "        lgb_list = glob.glob('../stack/*binary*.gz')\n",
    "        nn_list = []\n",
    "        ens_list = lgb_list + nn_list\n",
    "    #========================================================================\n",
    "    # Base Model\n",
    "    else:\n",
    "        if is_random:\n",
    "            np.random.seed(seed)\n",
    "            lgb_list = list(np.random.choice(lgb_list, 10))\n",
    "#             nn_list = list(np.random.choice(nn_list, 1))\n",
    "        nn_list = []\n",
    "        rid_list = []\n",
    "        ext_list = []\n",
    "        rmf_list = []\n",
    "        if is_nn : nn_list = glob.glob('../ensemble/NN_ensemble/*CV3*.gz')\n",
    "        if is_rmf: rmf_list = glob.glob('../ensemble/various_model/*rmf*.gz')\n",
    "        if is_ext: ext_list = glob.glob('../ensemble/various_model/*ext*.gz')\n",
    "        if is_rid: rid_list = glob.glob('../ensemble/various_model/*ridge*.gz')\n",
    "        lgb_list = glob.glob(lgb_path)\n",
    "        ens_list = lgb_list + nn_list + rid_list + rmf_list + ext_list\n",
    "    \n",
    "    #========================================================================\n",
    "    # Stack Models Load\n",
    "    from joblib import Parallel, delayed\n",
    "    def parallel_stack_model(model_path):\n",
    "        try:\n",
    "            cv = re.search(r'CV([^/.]*)_LB.gz', model_path).group(1)\n",
    "        except AttributeError:\n",
    "            cv = re.search(r'CV([^/.]*).gz', model_path.replace('.', '-')).group(1)\n",
    "        tmp = utils.read_pkl_gzip(model_path)\n",
    "        if key not in tmp.columns:\n",
    "            tmp.reset_index(inplace=True)\n",
    "        if 'pred_mean' in tmp.columns:\n",
    "            tmp = tmp[[key, 'pred_mean']]\n",
    "        else:\n",
    "            tmp = tmp[[key, 'prediction']]\n",
    "            \n",
    "        if model_path.count('lgb'):\n",
    "            tmp.columns = [key, f\"base_lgb_{cv}\"]\n",
    "        elif model_path.count('NN'):\n",
    "            tmp.columns = [key, f\"base_NN_{cv}\"]\n",
    "        elif model_path.count('ridge'):\n",
    "            tmp.columns = [key, f\"base_ridge_{cv}\"]\n",
    "        elif model_path.count('rmf'):\n",
    "            tmp.columns = [key, f\"base_rmf_{cv}\"]\n",
    "        elif model_path.count('ext'):\n",
    "            tmp.columns = [key, f\"base_ext_{cv}\"]\n",
    "        else:\n",
    "            tmp.columns = [key, f\"base_model_{cv}\"]\n",
    "        return tmp.set_index(key)\n",
    "    #========================================================================\n",
    "    \n",
    "    p_list = Parallel(n_jobs=-1)([delayed(parallel_stack_model)(model_path) for model_path in ens_list])\n",
    "    df_pred = pd.concat(p_list, axis=1)\n",
    "    if is_rm_out:\n",
    "        cv15 = [col for col in df_pred.columns if col.count('1-5')]\n",
    "        cv8 = [col for col in df_pred.columns if col.count('8-')]\n",
    "        df_pred['tmp_mean'] = df_pred[cv8].mean(axis=1).values\n",
    "        for col in cv15:\n",
    "            df_pred.loc[df_pred[col].isnull(), col] = df_pred.loc[df_pred[col].isnull(), 'tmp_mean']\n",
    "    base = base.join(df_pred)\n",
    "    \n",
    "    #========================================================================\n",
    "    \n",
    "    if key in base.columns:\n",
    "        train = base[~base[target].isnull()]\n",
    "        test = base[base[target].isnull()]\n",
    "    else:\n",
    "        train = base[~base[target].isnull()].reset_index()\n",
    "        test = base[base[target].isnull()].reset_index()\n",
    "    \n",
    "    if is_rm_out:\n",
    "        train = train[~train[target].isnull()]\n",
    "    elif is_clf_out:\n",
    "        train = train[train['clf_pred']<0.01]\n",
    "        test = test[test['clf_pred']<0.01]\n",
    "    elif is_binary:\n",
    "        train[target] = train[target].map(lambda x: 1 if x<-30 else 0)\n",
    "        \n",
    "    display(train.head())\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>hist_purchase_date_max</th>\n",
       "      <th>hist_purchase_month_max</th>\n",
       "      <th>hist_purchase_date_min</th>\n",
       "      <th>hist_purchase_month_min</th>\n",
       "      <th>new_purchase_date_max</th>\n",
       "      <th>new_purchase_month_max</th>\n",
       "      <th>new_purchase_date_min</th>\n",
       "      <th>...</th>\n",
       "      <th>base_lgb_3-6256427264251463</th>\n",
       "      <th>base_lgb_3-6246353407463157</th>\n",
       "      <th>base_lgb_3-625177466404422</th>\n",
       "      <th>base_lgb_3-627555_LB3-675</th>\n",
       "      <th>base_lgb_3-6236254858483243</th>\n",
       "      <th>base_lgb_3-6206463759490277</th>\n",
       "      <th>base_lgb_3-6333204401002663</th>\n",
       "      <th>base_lgb_3-6226545066935465</th>\n",
       "      <th>base_lgb_3-6221003122973614</th>\n",
       "      <th>base_lgb_3-653674740088933</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2018-02-25 09:31:15</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-06-27 14:18:08</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2018-04-29 11:23:05</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-05 14:04:36</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405107</td>\n",
       "      <td>-0.348467</td>\n",
       "      <td>-0.373584</td>\n",
       "      <td>-0.349346</td>\n",
       "      <td>-0.344292</td>\n",
       "      <td>-0.346904</td>\n",
       "      <td>-0.331135</td>\n",
       "      <td>-0.340694</td>\n",
       "      <td>-0.362157</td>\n",
       "      <td>-0.181378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-01-31 22:31:09</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2017-01-06 16:29:42</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-03-30 06:48:26</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2018-02-01 17:07:54</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108786</td>\n",
       "      <td>-0.746501</td>\n",
       "      <td>-0.309417</td>\n",
       "      <td>-0.627140</td>\n",
       "      <td>-0.349898</td>\n",
       "      <td>-0.568272</td>\n",
       "      <td>-0.484170</td>\n",
       "      <td>-0.675536</td>\n",
       "      <td>-0.074936</td>\n",
       "      <td>-0.652553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>2018-02-27 19:08:25</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-01-11 08:21:22</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-04-28 17:43:11</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-04-28 17:43:11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795536</td>\n",
       "      <td>0.677786</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.648794</td>\n",
       "      <td>0.803631</td>\n",
       "      <td>0.683088</td>\n",
       "      <td>0.642721</td>\n",
       "      <td>0.759212</td>\n",
       "      <td>0.788112</td>\n",
       "      <td>0.584152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2018-02-28 11:44:40</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-09-26 16:22:21</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2018-04-18 11:00:11</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-07 11:55:06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176037</td>\n",
       "      <td>0.153740</td>\n",
       "      <td>0.150416</td>\n",
       "      <td>0.111918</td>\n",
       "      <td>0.137893</td>\n",
       "      <td>0.173934</td>\n",
       "      <td>0.108993</td>\n",
       "      <td>0.168531</td>\n",
       "      <td>0.153302</td>\n",
       "      <td>0.205327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2018-02-28 20:40:41</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-11-12 00:00:00</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2018-04-28 18:50:25</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-02 11:55:43</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076840</td>\n",
       "      <td>-0.078756</td>\n",
       "      <td>-0.116971</td>\n",
       "      <td>-0.182233</td>\n",
       "      <td>-0.120741</td>\n",
       "      <td>-0.109142</td>\n",
       "      <td>-0.106126</td>\n",
       "      <td>-0.087869</td>\n",
       "      <td>-0.078456</td>\n",
       "      <td>-0.205778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target first_active_month hist_purchase_date_max  \\\n",
       "0  C_ID_92a2005557 -0.820283         2017-06-01    2018-02-25 09:31:15   \n",
       "1  C_ID_3d0044924f  0.392913         2017-01-01    2018-01-31 22:31:09   \n",
       "2  C_ID_d639edf6cd  0.688056         2016-08-01    2018-02-27 19:08:25   \n",
       "3  C_ID_186d6a6901  0.142495         2017-09-01    2018-02-28 11:44:40   \n",
       "4  C_ID_cdbd2c0db2 -0.159749         2017-11-01    2018-02-28 20:40:41   \n",
       "\n",
       "  hist_purchase_month_max hist_purchase_date_min hist_purchase_month_min  \\\n",
       "0              2018-03-01    2017-06-27 14:18:08              2017-06-01   \n",
       "1              2018-02-01    2017-01-06 16:29:42              2017-01-01   \n",
       "2              2018-03-01    2017-01-11 08:21:22              2017-01-01   \n",
       "3              2018-03-01    2017-09-26 16:22:21              2017-09-01   \n",
       "4              2018-03-01    2017-11-12 00:00:00              2017-11-01   \n",
       "\n",
       "  new_purchase_date_max new_purchase_month_max new_purchase_date_min  \\\n",
       "0   2018-04-29 11:23:05             2018-05-01   2018-03-05 14:04:36   \n",
       "1   2018-03-30 06:48:26             2018-04-01   2018-02-01 17:07:54   \n",
       "2   2018-04-28 17:43:11             2018-05-01   2018-04-28 17:43:11   \n",
       "3   2018-04-18 11:00:11             2018-05-01   2018-03-07 11:55:06   \n",
       "4   2018-04-28 18:50:25             2018-05-01   2018-03-02 11:55:43   \n",
       "\n",
       "              ...             base_lgb_3-6256427264251463  \\\n",
       "0             ...                               -0.405107   \n",
       "1             ...                               -0.108786   \n",
       "2             ...                                0.795536   \n",
       "3             ...                                0.176037   \n",
       "4             ...                               -0.076840   \n",
       "\n",
       "   base_lgb_3-6246353407463157  base_lgb_3-625177466404422  \\\n",
       "0                    -0.348467                   -0.373584   \n",
       "1                    -0.746501                   -0.309417   \n",
       "2                     0.677786                    0.571900   \n",
       "3                     0.153740                    0.150416   \n",
       "4                    -0.078756                   -0.116971   \n",
       "\n",
       "   base_lgb_3-627555_LB3-675  base_lgb_3-6236254858483243  \\\n",
       "0                  -0.349346                    -0.344292   \n",
       "1                  -0.627140                    -0.349898   \n",
       "2                   0.648794                     0.803631   \n",
       "3                   0.111918                     0.137893   \n",
       "4                  -0.182233                    -0.120741   \n",
       "\n",
       "   base_lgb_3-6206463759490277  base_lgb_3-6333204401002663  \\\n",
       "0                    -0.346904                    -0.331135   \n",
       "1                    -0.568272                    -0.484170   \n",
       "2                     0.683088                     0.642721   \n",
       "3                     0.173934                     0.108993   \n",
       "4                    -0.109142                    -0.106126   \n",
       "\n",
       "   base_lgb_3-6226545066935465  base_lgb_3-6221003122973614  \\\n",
       "0                    -0.340694                    -0.362157   \n",
       "1                    -0.675536                    -0.074936   \n",
       "2                     0.759212                     0.788112   \n",
       "3                     0.168531                     0.153302   \n",
       "4                    -0.087869                    -0.078456   \n",
       "\n",
       "   base_lgb_3-653674740088933  \n",
       "0                   -0.181378  \n",
       "1                   -0.652553  \n",
       "2                    0.584152  \n",
       "3                    0.205327  \n",
       "4                   -0.205778  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (168250, 23) | Test: (123623, 23)\n",
      "RMSE: 3.6062391304330563 | SUM ERROR: 211.90575635372068\n",
      "Train: (168252, 23) | Test: (123623, 23)\n",
      "RMSE: 3.604735022876224 | SUM ERROR: -384.773959455969\n",
      "Train: (168261, 23) | Test: (123623, 23)\n",
      "RMSE: 3.6170146025631005 | SUM ERROR: -661.8128949605887\n",
      "Train: (168270, 23) | Test: (123623, 23)\n",
      "RMSE: 3.6397685031709086 | SUM ERROR: 491.48112999400485\n",
      "Train: (168274, 23) | Test: (123623, 23)\n",
      "RMSE: 3.633461690998905 | SUM ERROR: 82.22980478608544\n",
      "Train: (168278, 23) | Test: (123623, 23)\n",
      "RMSE: 3.60288596412451 | SUM ERROR: 253.23574947328927\n",
      "Stacking Shape: (325540, 3)\n",
      "\n",
      "#========================================================================\n",
      "# CV SCORE AVG: 3.6173508190277843\n",
      "# OUT SCORE: 29.70522110784238\n",
      "#========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>hist_purchase_date_max</th>\n",
       "      <th>hist_purchase_month_max</th>\n",
       "      <th>hist_purchase_date_min</th>\n",
       "      <th>hist_purchase_month_min</th>\n",
       "      <th>new_purchase_date_max</th>\n",
       "      <th>new_purchase_month_max</th>\n",
       "      <th>new_purchase_date_min</th>\n",
       "      <th>...</th>\n",
       "      <th>base_lgb_3-6256427264251463</th>\n",
       "      <th>base_lgb_3-6246353407463157</th>\n",
       "      <th>base_lgb_3-625177466404422</th>\n",
       "      <th>base_lgb_3-627555_LB3-675</th>\n",
       "      <th>base_lgb_3-6236254858483243</th>\n",
       "      <th>base_lgb_3-6206463759490277</th>\n",
       "      <th>base_lgb_3-6333204401002663</th>\n",
       "      <th>base_lgb_3-6226545066935465</th>\n",
       "      <th>base_lgb_3-6221003122973614</th>\n",
       "      <th>base_lgb_3-653674740088933</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2018-02-25 09:31:15</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-06-27 14:18:08</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2018-04-29 11:23:05</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-05 14:04:36</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405107</td>\n",
       "      <td>-0.348467</td>\n",
       "      <td>-0.373584</td>\n",
       "      <td>-0.349346</td>\n",
       "      <td>-0.344292</td>\n",
       "      <td>-0.346904</td>\n",
       "      <td>-0.331135</td>\n",
       "      <td>-0.340694</td>\n",
       "      <td>-0.362157</td>\n",
       "      <td>-0.181378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-01-31 22:31:09</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2017-01-06 16:29:42</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-03-30 06:48:26</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2018-02-01 17:07:54</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108786</td>\n",
       "      <td>-0.746501</td>\n",
       "      <td>-0.309417</td>\n",
       "      <td>-0.627140</td>\n",
       "      <td>-0.349898</td>\n",
       "      <td>-0.568272</td>\n",
       "      <td>-0.484170</td>\n",
       "      <td>-0.675536</td>\n",
       "      <td>-0.074936</td>\n",
       "      <td>-0.652553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>2018-02-27 19:08:25</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-01-11 08:21:22</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-04-28 17:43:11</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-04-28 17:43:11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795536</td>\n",
       "      <td>0.677786</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.648794</td>\n",
       "      <td>0.803631</td>\n",
       "      <td>0.683088</td>\n",
       "      <td>0.642721</td>\n",
       "      <td>0.759212</td>\n",
       "      <td>0.788112</td>\n",
       "      <td>0.584152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2018-02-28 11:44:40</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-09-26 16:22:21</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2018-04-18 11:00:11</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-07 11:55:06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176037</td>\n",
       "      <td>0.153740</td>\n",
       "      <td>0.150416</td>\n",
       "      <td>0.111918</td>\n",
       "      <td>0.137893</td>\n",
       "      <td>0.173934</td>\n",
       "      <td>0.108993</td>\n",
       "      <td>0.168531</td>\n",
       "      <td>0.153302</td>\n",
       "      <td>0.205327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2018-02-28 20:40:41</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-11-12 00:00:00</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2018-04-28 18:50:25</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-02 11:55:43</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076840</td>\n",
       "      <td>-0.078756</td>\n",
       "      <td>-0.116971</td>\n",
       "      <td>-0.182233</td>\n",
       "      <td>-0.120741</td>\n",
       "      <td>-0.109142</td>\n",
       "      <td>-0.106126</td>\n",
       "      <td>-0.087869</td>\n",
       "      <td>-0.078456</td>\n",
       "      <td>-0.205778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target first_active_month hist_purchase_date_max  \\\n",
       "0  C_ID_92a2005557 -0.820283         2017-06-01    2018-02-25 09:31:15   \n",
       "1  C_ID_3d0044924f  0.392913         2017-01-01    2018-01-31 22:31:09   \n",
       "2  C_ID_d639edf6cd  0.688056         2016-08-01    2018-02-27 19:08:25   \n",
       "3  C_ID_186d6a6901  0.142495         2017-09-01    2018-02-28 11:44:40   \n",
       "4  C_ID_cdbd2c0db2 -0.159749         2017-11-01    2018-02-28 20:40:41   \n",
       "\n",
       "  hist_purchase_month_max hist_purchase_date_min hist_purchase_month_min  \\\n",
       "0              2018-03-01    2017-06-27 14:18:08              2017-06-01   \n",
       "1              2018-02-01    2017-01-06 16:29:42              2017-01-01   \n",
       "2              2018-03-01    2017-01-11 08:21:22              2017-01-01   \n",
       "3              2018-03-01    2017-09-26 16:22:21              2017-09-01   \n",
       "4              2018-03-01    2017-11-12 00:00:00              2017-11-01   \n",
       "\n",
       "  new_purchase_date_max new_purchase_month_max new_purchase_date_min  \\\n",
       "0   2018-04-29 11:23:05             2018-05-01   2018-03-05 14:04:36   \n",
       "1   2018-03-30 06:48:26             2018-04-01   2018-02-01 17:07:54   \n",
       "2   2018-04-28 17:43:11             2018-05-01   2018-04-28 17:43:11   \n",
       "3   2018-04-18 11:00:11             2018-05-01   2018-03-07 11:55:06   \n",
       "4   2018-04-28 18:50:25             2018-05-01   2018-03-02 11:55:43   \n",
       "\n",
       "              ...             base_lgb_3-6256427264251463  \\\n",
       "0             ...                               -0.405107   \n",
       "1             ...                               -0.108786   \n",
       "2             ...                                0.795536   \n",
       "3             ...                                0.176037   \n",
       "4             ...                               -0.076840   \n",
       "\n",
       "   base_lgb_3-6246353407463157  base_lgb_3-625177466404422  \\\n",
       "0                    -0.348467                   -0.373584   \n",
       "1                    -0.746501                   -0.309417   \n",
       "2                     0.677786                    0.571900   \n",
       "3                     0.153740                    0.150416   \n",
       "4                    -0.078756                   -0.116971   \n",
       "\n",
       "   base_lgb_3-627555_LB3-675  base_lgb_3-6236254858483243  \\\n",
       "0                  -0.349346                    -0.344292   \n",
       "1                  -0.627140                    -0.349898   \n",
       "2                   0.648794                     0.803631   \n",
       "3                   0.111918                     0.137893   \n",
       "4                  -0.182233                    -0.120741   \n",
       "\n",
       "   base_lgb_3-6206463759490277  base_lgb_3-6333204401002663  \\\n",
       "0                    -0.346904                    -0.331135   \n",
       "1                    -0.568272                    -0.484170   \n",
       "2                     0.683088                     0.642721   \n",
       "3                     0.173934                     0.108993   \n",
       "4                    -0.109142                    -0.106126   \n",
       "\n",
       "   base_lgb_3-6226545066935465  base_lgb_3-6221003122973614  \\\n",
       "0                    -0.340694                    -0.362157   \n",
       "1                    -0.675536                    -0.074936   \n",
       "2                     0.759212                     0.788112   \n",
       "3                     0.168531                     0.153302   \n",
       "4                    -0.087869                    -0.078456   \n",
       "\n",
       "   base_lgb_3-653674740088933  \n",
       "0                   -0.181378  \n",
       "1                   -0.652553  \n",
       "2                    0.584152  \n",
       "3                    0.205327  \n",
       "4                   -0.205778  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (168250, 23) | Test: (123623, 23)\n",
      "RMSE: 3.6344127390247163 | SUM ERROR: -1.1143795029709622\n",
      "Train: (168252, 23) | Test: (123623, 23)\n",
      "RMSE: 3.6090662098190114 | SUM ERROR: -362.1538157195994\n",
      "Train: (168261, 23) | Test: (123623, 23)\n",
      "RMSE: 3.626120207522003 | SUM ERROR: 131.83835472313496\n",
      "Train: (168270, 23) | Test: (123623, 23)\n",
      "RMSE: 3.6138963885894646 | SUM ERROR: 135.5430129000438\n",
      "Train: (168274, 23) | Test: (123623, 23)\n",
      "RMSE: 3.604017192093228 | SUM ERROR: -173.35577404585612\n",
      "Train: (168278, 23) | Test: (123623, 23)\n",
      "RMSE: 3.612002447697715 | SUM ERROR: 291.93206873206907\n",
      "Stacking Shape: (325540, 3)\n",
      "\n",
      "#========================================================================\n",
      "# CV SCORE AVG: 3.6169683415760705\n",
      "# OUT SCORE: 29.69770926785604\n",
      "#========================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "valid_type = ['ods', 'pmo' ,'pm' ,'term'][3]\n",
    "lgb_path = '../ensemble/pmo_all_stack_level1/*.gz'\n",
    "lgb_path = '../model/LB3664_set/*_lgb_*.gz'\n",
    "lgb_path = '../ensemble/dir_stack_blend/tmp/*_lgb_*.gz'\n",
    "#========================================================================\n",
    "# Make Dataset \n",
    "is_clf_out = [True, False][1]\n",
    "is_no_out_flg = [True, False][1]\n",
    "is_rm_out = [True, False][1]\n",
    "is_binary = [True, False][1]\n",
    "is_nn = 0\n",
    "is_rid = 0\n",
    "is_rmf = 0\n",
    "is_ext = 0\n",
    "is_random = 0\n",
    "seed_size = 1\n",
    "#========================================================================\n",
    "    \n",
    "#========================================================================\n",
    "# CVの準備\n",
    "seed = 328\n",
    "fold_seed = 328\n",
    "fold_seed = 1208\n",
    "seed_list = [328, 1208]\n",
    "fold = 6\n",
    "\n",
    "if is_rm_out:\n",
    "    set_type = 'rm_out'\n",
    "else:\n",
    "    set_type = 'all'\n",
    "\n",
    "#========================================================================\n",
    "# Dataset\n",
    "submit = pd.read_csv('../input/sample_submission.csv').set_index(key)\n",
    "result_list = []\n",
    "score_list = []\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg', 'clf_pred']\n",
    "#========================================================================\n",
    "    \n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "params = {}\n",
    "if is_binary:\n",
    "    params['n_jobs']=-1\n",
    "    params['C']=1.0\n",
    "    params['solver'] ='liblinear'\n",
    "    params['fit_intercept']=True\n",
    "    params['max_iter']=1000\n",
    "    params['tol']=0.01\n",
    "    params['random_state']=seed\n",
    "    model = LogisticRegression(**params)\n",
    "else:\n",
    "    params['solver'] ='auto'\n",
    "    params['fit_intercept']=True\n",
    "    params['alpha']=0.4\n",
    "    params['max_iter']=1000\n",
    "    params['normalize']=False\n",
    "    params['tol']=0.01\n",
    "    params['random_state']=seed\n",
    "    model = Ridge(**params)\n",
    "\n",
    "# np.random.seed(int(time.time()))\n",
    "# seed_list = np.random.randint(10**7, size=seed_size)\n",
    "\n",
    "# for seed in seed_list:\n",
    "for fold_seed in seed_list:\n",
    "    \n",
    "    if is_rm_out:\n",
    "        kfold = utils.read_pkl_gzip('../input/kfold_ods_no_out_fold6_seed328.gz')\n",
    "    elif is_clf_out:\n",
    "        kfold = utils.read_pkl_gzip('../input/kfold_ods_clf_out_fold6_seed328.gz')\n",
    "    else:\n",
    "        kfold = utils.read_pkl_gzip(f'../input/kfold_{valid_type}_all_fold6_seed{fold_seed}.gz')\n",
    "    \n",
    "    train, test = get_stack_dataset(lgb_path=lgb_path, is_rmf=is_rmf, is_ext=is_ext, is_random=is_random, seed=seed)\n",
    "        \n",
    "    #========================================================================\n",
    "    # Preset\n",
    "    use_cols = sorted([col for col in train.columns if col.count('base_')])\n",
    "    best_score = 100\n",
    "    best_score_list = []\n",
    "    test_pred = np.zeros(len(test))\n",
    "    Y = train[target]\n",
    "    result_list = []\n",
    "    #========================================================================\n",
    "\n",
    "    #========================================================================\n",
    "    # Train & Prediction Start\n",
    "    for fold_no, (trn_idx, val_idx) in enumerate(zip(*kfold)):\n",
    "        if key not in train.columns:\n",
    "            train = train.reset_index()\n",
    "            test = test.reset_index() \n",
    "             \n",
    "        #========================================================================\n",
    "        # Make Dataset\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(pd.concat([train[use_cols], test[use_cols]]))\n",
    "        x_test = scaler.transform(test[use_cols])\n",
    "\n",
    "        X_train, y_train = train.loc[train[key].isin(trn_idx), :][use_cols], Y.loc[train[key].isin(trn_idx)]\n",
    "        X_val, y_val = train.loc[train[key].isin(val_idx), :][use_cols], Y.loc[train[key].isin(val_idx)]\n",
    "        \n",
    "        X_train[:] = scaler.transform(X_train)\n",
    "        X_val[:] = scaler.transform(X_val)\n",
    "        X_train = X_train.as_matrix()\n",
    "        X_val = X_val.as_matrix()\n",
    "    \n",
    "        print(f\"Train: {X_train.shape} | Test: {x_test.shape}\")\n",
    "        #========================================================================\n",
    "        \n",
    "        # Fitting\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Prediction\n",
    "        if is_binary:\n",
    "            y_pred = model.predict_proba(X_val)[:, 1]\n",
    "        elif is_rm_out:\n",
    "            X_val = train.loc[~train[key].isin(trn_idx), :]\n",
    "            y_pred = model.predict(X_val[use_cols])\n",
    "            y_val = X_val[target].values\n",
    "        else:\n",
    "            y_pred = model.predict(X_val)\n",
    "        \n",
    "        test_pred += model.predict(x_test)\n",
    "        \n",
    "        # Stack Prediction\n",
    "        if is_rm_out:\n",
    "            if fold_no==0:\n",
    "                df_pred = train[[key, target]].set_index(key)\n",
    "            self_valid = X_val[[key, target]].set_index(key)\n",
    "            self_valid[f'pred_{fold_no}'] = y_pred\n",
    "            df_pred = df_pred.join(self_valid.drop(target, axis=1))\n",
    "        else:\n",
    "            df_pred = train.loc[train[key].isin(val_idx), :][[key, target]].copy()\n",
    "            df_pred['prediction'] = y_pred\n",
    "            result_list.append(df_pred)\n",
    "        \n",
    "        # Scoring\n",
    "        err = (y_val - y_pred)\n",
    "        if is_binary:\n",
    "            score = np.sqrt(roc_auc_score(y_val, y_pred))\n",
    "            print(f'AUC: {score} | SUM ERROR: {err.sum()}')\n",
    "        else:\n",
    "            score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "        score_list.append(score)\n",
    "        #========================================================================\n",
    "    \n",
    "    cv_score = np.mean(score_list)\n",
    "    \n",
    "    #========================================================================\n",
    "    # Stacking\n",
    "    test_pred /= fold_no+1\n",
    "    test['prediction'] = test_pred\n",
    "    stack_test = test[[key, 'prediction']]\n",
    "    \n",
    "    if is_rm_out:\n",
    "        pred_col_list = [col for col in df_pred.columns if col.count('pred_')]\n",
    "        df_pred['prediction'] = df_pred[pred_col_list].mean(axis=1)\n",
    "        result_list.append(df_pred.reset_index())\n",
    "    \n",
    "    result_list.append(stack_test)\n",
    "    df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "    if key not in base:\n",
    "        base.reset_index(inplace=True)\n",
    "    df_pred = base[[key, target]].merge(df_pred, how='inner', on=key)\n",
    "    print(f\"Stacking Shape: {df_pred.shape}\")\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # outlierに対するスコアを出す\n",
    "    if is_rm_out or is_binary:\n",
    "        out_score = 0\n",
    "    else:\n",
    "        if key not in train.columns:\n",
    "            train.reset_index(inplace=True)\n",
    "        out_ids = train.loc[train.target<-30, key].values\n",
    "        out_val = train.loc[train.target<-30, target].values\n",
    "        out_pred = df_pred[df_pred[key].isin(out_ids)]['prediction'].values\n",
    "        out_score = np.sqrt(mean_squared_error(out_val, out_pred))\n",
    "    #========================================================================\n",
    "    \n",
    "    if cv_score<best_score:\n",
    "        print(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "# OUT SCORE: {out_score}\n",
    "#========================================================================''')\n",
    "    \n",
    "        best_score = cv_score\n",
    "        best_score_list = use_cols\n",
    "    \n",
    "        #========================================================================\n",
    "        # Save Stack\n",
    "        utils.to_pkl_gzip(path=f\"../stack/{start_time[4:12]}_stack_{model_type}_set-{set_type}_valid-{valid_type}-seed{fold_seed}_lgb{len(lgb_list)}_NN{is_nn}_ridge{is_rid}_ext{is_ext}_rmf{is_rmf}_level1{is_level1}_OUT{str(out_score)[:7]}_CV{cv_score}_LB\" , obj=df_pred[[key, 'prediction']])\n",
    "        #========================================================================\n",
    "sys.exit()\n",
    "    \n",
    "#========================================================================\n",
    "# Submission\n",
    "df_pred.set_index(key, inplace=True)\n",
    "submit[target] = df_pred['prediction']\n",
    "submit_path = f'../submit/{start_time[4:12]}_submit_{model_type}_set-{set_type}_lgb{len(lgb_list)}_NN{len(nn_list)}_other{len(other_list)}_OUT{str(out_score)[:7]}_CV{cv_score}_LB.csv'\n",
    "submit.to_csv(submit_path, index=True)\n",
    "display(submit.head())\n",
    "#========================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
