{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>hist_purchase_date_max</th>\n",
       "      <th>hist_purchase_month_max</th>\n",
       "      <th>hist_purchase_date_min</th>\n",
       "      <th>hist_purchase_month_min</th>\n",
       "      <th>new_purchase_date_max</th>\n",
       "      <th>new_purchase_month_max</th>\n",
       "      <th>new_purchase_date_min</th>\n",
       "      <th>new_purchase_month_min</th>\n",
       "      <th>hist_personal_term</th>\n",
       "      <th>new_personal_term</th>\n",
       "      <th>hist_regist_term</th>\n",
       "      <th>new_regist_term</th>\n",
       "      <th>no_out_flg</th>\n",
       "      <th>clf_pred</th>\n",
       "      <th>base_lgb_3-643008963792204</th>\n",
       "      <th>base_lgb_3-6335126722555198</th>\n",
       "      <th>base_lgb_3-6675809308648986</th>\n",
       "      <th>base_lgb_3-621674473874384</th>\n",
       "      <th>base_lgb_3-6213553126969393</th>\n",
       "      <th>base_lgb_3-674335359033703</th>\n",
       "      <th>base_lgb_3-6297296108420443</th>\n",
       "      <th>base_lgb_3-640200217753496</th>\n",
       "      <th>base_lgb_3-2381280632566187</th>\n",
       "      <th>base_lgb_3-240003171265297</th>\n",
       "      <th>base_lgb_3-6431931116870206</th>\n",
       "      <th>base_lgb_3-651455934282057</th>\n",
       "      <th>base_lgb_3-643556624877276</th>\n",
       "      <th>base_lgb_3-6246353407463157</th>\n",
       "      <th>base_lgb_3-6707091161311785</th>\n",
       "      <th>base_lgb_3-6812204136943496</th>\n",
       "      <th>base_lgb_3-6236254858483243</th>\n",
       "      <th>base_lgb_3-6333204401002663</th>\n",
       "      <th>base_lgb_3-627555_LB3-675</th>\n",
       "      <th>base_lgb_3-653674740088933</th>\n",
       "      <th>base_lgb_3-665786223163817</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2018-02-25 09:31:15</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-06-27 14:18:08</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2018-04-29 11:23:05</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-05 14:04:36</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>-0.533682</td>\n",
       "      <td>-0.321566</td>\n",
       "      <td>-0.449476</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.360447</td>\n",
       "      <td>-0.348939</td>\n",
       "      <td>-0.346031</td>\n",
       "      <td>-0.316781</td>\n",
       "      <td>-0.403515</td>\n",
       "      <td>-0.344339</td>\n",
       "      <td>-0.262261</td>\n",
       "      <td>-0.257492</td>\n",
       "      <td>-0.193392</td>\n",
       "      <td>-0.348467</td>\n",
       "      <td>-0.478001</td>\n",
       "      <td>-0.502368</td>\n",
       "      <td>-0.344292</td>\n",
       "      <td>-0.331135</td>\n",
       "      <td>-0.349346</td>\n",
       "      <td>-0.181378</td>\n",
       "      <td>-0.517733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-01-31 22:31:09</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2017-01-06 16:29:42</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-03-30 06:48:26</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2018-02-01 17:07:54</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>13</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>-0.669546</td>\n",
       "      <td>-0.581695</td>\n",
       "      <td>-0.971562</td>\n",
       "      <td>-0.598341</td>\n",
       "      <td>-0.199257</td>\n",
       "      <td>-0.493447</td>\n",
       "      <td>-0.557220</td>\n",
       "      <td>-0.308836</td>\n",
       "      <td>-1.136101</td>\n",
       "      <td>-0.955046</td>\n",
       "      <td>-0.586575</td>\n",
       "      <td>-0.052061</td>\n",
       "      <td>-0.180970</td>\n",
       "      <td>-0.746501</td>\n",
       "      <td>-0.847191</td>\n",
       "      <td>-0.892496</td>\n",
       "      <td>-0.349898</td>\n",
       "      <td>-0.484170</td>\n",
       "      <td>-0.627140</td>\n",
       "      <td>-0.652553</td>\n",
       "      <td>-1.128744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>2018-02-27 19:08:25</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-01-11 08:21:22</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-04-28 17:43:11</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-04-28 17:43:11</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004074</td>\n",
       "      <td>0.581285</td>\n",
       "      <td>0.849526</td>\n",
       "      <td>0.447654</td>\n",
       "      <td>0.655104</td>\n",
       "      <td>0.589762</td>\n",
       "      <td>0.801311</td>\n",
       "      <td>0.915316</td>\n",
       "      <td>0.586407</td>\n",
       "      <td>0.744735</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>0.610568</td>\n",
       "      <td>0.759758</td>\n",
       "      <td>0.647221</td>\n",
       "      <td>0.677786</td>\n",
       "      <td>0.356035</td>\n",
       "      <td>0.454059</td>\n",
       "      <td>0.803631</td>\n",
       "      <td>0.642721</td>\n",
       "      <td>0.648794</td>\n",
       "      <td>0.584152</td>\n",
       "      <td>0.428851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2018-02-28 11:44:40</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-09-26 16:22:21</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2018-04-18 11:00:11</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-07 11:55:06</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.118323</td>\n",
       "      <td>0.214690</td>\n",
       "      <td>0.136579</td>\n",
       "      <td>0.147356</td>\n",
       "      <td>0.149292</td>\n",
       "      <td>0.027062</td>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.143492</td>\n",
       "      <td>0.132069</td>\n",
       "      <td>0.012830</td>\n",
       "      <td>0.040901</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.170643</td>\n",
       "      <td>0.153740</td>\n",
       "      <td>0.126440</td>\n",
       "      <td>0.189152</td>\n",
       "      <td>0.137893</td>\n",
       "      <td>0.108993</td>\n",
       "      <td>0.111918</td>\n",
       "      <td>0.205327</td>\n",
       "      <td>0.095453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2018-02-28 20:40:41</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2017-11-12 00:00:00</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2018-04-28 18:50:25</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-03-02 11:55:43</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>-0.282362</td>\n",
       "      <td>-0.137133</td>\n",
       "      <td>-0.216872</td>\n",
       "      <td>-0.079374</td>\n",
       "      <td>-0.105797</td>\n",
       "      <td>-0.200462</td>\n",
       "      <td>-0.223672</td>\n",
       "      <td>-0.296002</td>\n",
       "      <td>-0.126540</td>\n",
       "      <td>-0.218226</td>\n",
       "      <td>-0.071952</td>\n",
       "      <td>-0.180205</td>\n",
       "      <td>-0.003285</td>\n",
       "      <td>-0.078756</td>\n",
       "      <td>-0.303689</td>\n",
       "      <td>-0.308326</td>\n",
       "      <td>-0.120741</td>\n",
       "      <td>-0.106126</td>\n",
       "      <td>-0.182233</td>\n",
       "      <td>-0.205778</td>\n",
       "      <td>-0.489964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target first_active_month hist_purchase_date_max  \\\n",
       "0  C_ID_92a2005557 -0.820283         2017-06-01    2018-02-25 09:31:15   \n",
       "1  C_ID_3d0044924f  0.392913         2017-01-01    2018-01-31 22:31:09   \n",
       "2  C_ID_d639edf6cd  0.688056         2016-08-01    2018-02-27 19:08:25   \n",
       "3  C_ID_186d6a6901  0.142495         2017-09-01    2018-02-28 11:44:40   \n",
       "4  C_ID_cdbd2c0db2 -0.159749         2017-11-01    2018-02-28 20:40:41   \n",
       "\n",
       "  hist_purchase_month_max hist_purchase_date_min hist_purchase_month_min  \\\n",
       "0              2018-03-01    2017-06-27 14:18:08              2017-06-01   \n",
       "1              2018-02-01    2017-01-06 16:29:42              2017-01-01   \n",
       "2              2018-03-01    2017-01-11 08:21:22              2017-01-01   \n",
       "3              2018-03-01    2017-09-26 16:22:21              2017-09-01   \n",
       "4              2018-03-01    2017-11-12 00:00:00              2017-11-01   \n",
       "\n",
       "  new_purchase_date_max new_purchase_month_max new_purchase_date_min  \\\n",
       "0   2018-04-29 11:23:05             2018-05-01   2018-03-05 14:04:36   \n",
       "1   2018-03-30 06:48:26             2018-04-01   2018-02-01 17:07:54   \n",
       "2   2018-04-28 17:43:11             2018-05-01   2018-04-28 17:43:11   \n",
       "3   2018-04-18 11:00:11             2018-05-01   2018-03-07 11:55:06   \n",
       "4   2018-04-28 18:50:25             2018-05-01   2018-03-02 11:55:43   \n",
       "\n",
       "  new_purchase_month_min  hist_personal_term  new_personal_term  \\\n",
       "0             2018-03-01                   9                2.0   \n",
       "1             2018-02-01                  13                2.0   \n",
       "2             2018-04-01                  14                1.0   \n",
       "3             2018-03-01                   6                2.0   \n",
       "4             2018-03-01                   4                2.0   \n",
       "\n",
       "   hist_regist_term  new_regist_term  no_out_flg  clf_pred  \\\n",
       "0                 9             11.0         1.0  0.000444   \n",
       "1                13             15.0         0.0  0.007831   \n",
       "2                19             18.0         0.0  0.004074   \n",
       "3                 6              8.0         0.0  0.000797   \n",
       "4                 4              6.0         1.0  0.000251   \n",
       "\n",
       "   base_lgb_3-643008963792204  base_lgb_3-6335126722555198  \\\n",
       "0                   -0.533682                    -0.321566   \n",
       "1                   -0.669546                    -0.581695   \n",
       "2                    0.581285                     0.849526   \n",
       "3                    0.118323                     0.214690   \n",
       "4                   -0.282362                    -0.137133   \n",
       "\n",
       "   base_lgb_3-6675809308648986  base_lgb_3-621674473874384  \\\n",
       "0                    -0.449476                   -0.349492   \n",
       "1                    -0.971562                   -0.598341   \n",
       "2                     0.447654                    0.655104   \n",
       "3                     0.136579                    0.147356   \n",
       "4                    -0.216872                   -0.079374   \n",
       "\n",
       "   base_lgb_3-6213553126969393  base_lgb_3-674335359033703  \\\n",
       "0                    -0.360447                   -0.348939   \n",
       "1                    -0.199257                   -0.493447   \n",
       "2                     0.589762                    0.801311   \n",
       "3                     0.149292                    0.027062   \n",
       "4                    -0.105797                   -0.200462   \n",
       "\n",
       "   base_lgb_3-6297296108420443  base_lgb_3-640200217753496  \\\n",
       "0                    -0.346031                   -0.316781   \n",
       "1                    -0.557220                   -0.308836   \n",
       "2                     0.915316                    0.586407   \n",
       "3                     0.122011                    0.143492   \n",
       "4                    -0.223672                   -0.296002   \n",
       "\n",
       "   base_lgb_3-2381280632566187  base_lgb_3-240003171265297  \\\n",
       "0                    -0.403515                   -0.344339   \n",
       "1                    -1.136101                   -0.955046   \n",
       "2                     0.744735                    0.651148   \n",
       "3                     0.132069                    0.012830   \n",
       "4                    -0.126540                   -0.218226   \n",
       "\n",
       "   base_lgb_3-6431931116870206  base_lgb_3-651455934282057  \\\n",
       "0                    -0.262261                   -0.257492   \n",
       "1                    -0.586575                   -0.052061   \n",
       "2                     0.610568                    0.759758   \n",
       "3                     0.040901                    0.000617   \n",
       "4                    -0.071952                   -0.180205   \n",
       "\n",
       "   base_lgb_3-643556624877276  base_lgb_3-6246353407463157  \\\n",
       "0                   -0.193392                    -0.348467   \n",
       "1                   -0.180970                    -0.746501   \n",
       "2                    0.647221                     0.677786   \n",
       "3                    0.170643                     0.153740   \n",
       "4                   -0.003285                    -0.078756   \n",
       "\n",
       "   base_lgb_3-6707091161311785  base_lgb_3-6812204136943496  \\\n",
       "0                    -0.478001                    -0.502368   \n",
       "1                    -0.847191                    -0.892496   \n",
       "2                     0.356035                     0.454059   \n",
       "3                     0.126440                     0.189152   \n",
       "4                    -0.303689                    -0.308326   \n",
       "\n",
       "   base_lgb_3-6236254858483243  base_lgb_3-6333204401002663  \\\n",
       "0                    -0.344292                    -0.331135   \n",
       "1                    -0.349898                    -0.484170   \n",
       "2                     0.803631                     0.642721   \n",
       "3                     0.137893                     0.108993   \n",
       "4                    -0.120741                    -0.106126   \n",
       "\n",
       "   base_lgb_3-627555_LB3-675  base_lgb_3-653674740088933  \\\n",
       "0                  -0.349346                   -0.181378   \n",
       "1                  -0.627140                   -0.652553   \n",
       "2                   0.648794                    0.584152   \n",
       "3                   0.111918                    0.205327   \n",
       "4                  -0.182233                   -0.205778   \n",
       "\n",
       "   base_lgb_3-665786223163817  \n",
       "0                   -0.517733  \n",
       "1                   -1.128744  \n",
       "2                    0.428851  \n",
       "3                    0.095453  \n",
       "4                   -0.489964  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "is_stack = [True, False][0]\n",
    "debug = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "sys.path.append('../py/')\n",
    "from s027_kfold_ods import ods_kfold\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "from sklearn.linear_model import Ridge\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "out_part = ['', 'part', 'all'][0]\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "stack_name='ridge'\n",
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "model_type='ridge'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "print(\"Preparing dataset...\")\n",
    "\n",
    "if is_stack:\n",
    "    is_clf_out = [True, False][1]\n",
    "    is_no_out_flg = [True, False][1]\n",
    "    is_rm_out = [True, False][1]\n",
    "    \n",
    "    base = utils.read_df_pkl('../input/base_no_out_clf.gz').set_index(key)\n",
    "    \n",
    "    #========================================================================\n",
    "    # Base Model Path\n",
    "    #========================================================================\n",
    "    # Clf Out Model\n",
    "    if is_clf_out:\n",
    "        ens_list = glob.glob('../ensemble/clf_min_thres_ensemble/*.gz')\n",
    "    # No Out Flg Model\n",
    "    elif is_no_out_flg:\n",
    "        ens_list = glob.glob('../no_out_flg_ensemble/*.gz')\n",
    "    #========================================================================\n",
    "    elif is_rm_out:\n",
    "        ens_list = glob.glob('../ensemble/rm_outlier_ensemble/*.gz')\n",
    "    #========================================================================\n",
    "    else:\n",
    "        lgb_list = glob.glob('../ensemble/lgb_ensemble/*.gz')\n",
    "#         nn_list = glob.glob('../ensemble/NN_ensemble/*.gz')\n",
    "        nn_list = []\n",
    "        ens_list = lgb_list + nn_list\n",
    "    \n",
    "    #========================================================================\n",
    "    # Stack Models Load\n",
    "    from joblib import Parallel, delayed\n",
    "    def parallel_stack_model(model_path):\n",
    "        try:\n",
    "            cv = re.search(r'CV([^/.]*)_LB.gz', model_path).group(1)\n",
    "        except AttributeError:\n",
    "            cv = re.search(r'CV([^/.]*).gz', model_path.replace('.', '-')).group(1)\n",
    "        tmp = utils.read_pkl_gzip(model_path)\n",
    "        if 'pred_mean' in tmp.columns:\n",
    "            tmp = tmp[[key, 'pred_mean']]\n",
    "        else:\n",
    "            tmp = tmp[[key, 'prediction']]\n",
    "            \n",
    "        if model_path.count('lgb'):\n",
    "            tmp.columns = [key, f\"base_lgb_{cv}\"]\n",
    "        elif model_path.count('NN'):\n",
    "            tmp.columns = [key, f\"base_NN_{cv}\"]\n",
    "        else:\n",
    "            tmp.columns = [key, f\"base_model_{cv}\"]\n",
    "        return tmp.set_index(key)\n",
    "    #========================================================================\n",
    "    \n",
    "    p_list = Parallel(n_jobs=-1)([delayed(parallel_stack_model)(model_path) for model_path in ens_list])\n",
    "    df_pred = pd.concat(p_list, axis=1)\n",
    "    base = base.join(df_pred)\n",
    "    \n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # Classifier\n",
    "#     clf = utils.read_pkl_gzip('../stack/0207_224_outlier_classify_9seed_lgb_binary_CV0-9099420278047783_235features.gz')[[key, 'pred_mean']].set_index(key)\n",
    "#     clf_2 = utils.read_pkl_gzip('../stack/0207_212_outlier_classify_9seed_lgb_binary_CV0-9084737642836664_235features.gz')[[key, 'pred_mean']].set_index(key)\n",
    "#     clf['pred_mean_2'] = clf_2['pred_mean']\n",
    "#     clf['clf_pred'] =  clf['pred_mean'].values*0.9 + clf['pred_mean_2'].values*0.1\n",
    "    # clf['clf_pred'] =  clf['pred_mean']\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # Indexをそろえる\n",
    "#     base['clf_pred'] = clf['clf_pred']\n",
    "#     base_cols = [col for col in base.columns if col.count('base_')]\n",
    "    # base = base[[target, 'clf_pred', 'no_out_flg'] + base_cols + out_cols + no_out_cols]\n",
    "    #========================================================================\n",
    "    \n",
    "    if key in base.columns:\n",
    "        train = base[~base[target].isnull()]\n",
    "        test = base[base[target].isnull()]\n",
    "    else:\n",
    "        train = base[~base[target].isnull()].reset_index()\n",
    "        test = base[base[target].isnull()].reset_index()\n",
    "    \n",
    "    if is_rm_out:\n",
    "        train = train[~train[target].isnull()]\n",
    "    elif is_clf_out:\n",
    "        train = train[train['clf_pred']<0.01]\n",
    "        test = test[test['clf_pred']<0.01]\n",
    "        \n",
    "    display(train.head())\n",
    "    \n",
    "    \n",
    "else:\n",
    "    win_path = f'../features/4_winner/*.gz'\n",
    "    # Ensemble 1\n",
    "    win_path = f'../model/LB3670_70leaves_colsam0322/*.gz'\n",
    "    # Ensemble 2\n",
    "    # win_path = f'../model/E2_lift_set/*.gz'\n",
    "    # Ensemble 3\n",
    "    # win_path = f'../model/E3_PCA_set/*.gz'\n",
    "    \n",
    "    win_path_list = glob.glob(win_path)\n",
    "    \n",
    "    base = utils.read_df_pkl('../input/base_term*0*')[[key, target, 'first_active_month']]\n",
    "    base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "    base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "    feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
    "    df = pd.concat(feature_list, axis=1)\n",
    "    train = pd.concat([base_train, df.iloc[:len(base_train), :]], axis=1)\n",
    "    test = pd.concat([base_test, df.iloc[len(base_train):, :].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "    test.reset_index(inplace=True , drop=True)\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (168264, 20) | Test: (123623, 20)\n",
      "RMSE: 3.5996719566106026 | SUM ERROR: -201.5692463894596\n",
      "Train: (168264, 20) | Test: (123623, 20)\n",
      "RMSE: 3.6266220063391947 | SUM ERROR: 583.8262460140897\n",
      "Train: (168264, 20) | Test: (123623, 20)\n",
      "RMSE: 3.631527933137074 | SUM ERROR: 113.61580443130168\n",
      "Train: (168264, 20) | Test: (123623, 20)\n",
      "RMSE: 3.6037343997838645 | SUM ERROR: -220.27374647276704\n",
      "Train: (168264, 20) | Test: (123623, 20)\n",
      "RMSE: 3.6425257968675515 | SUM ERROR: 41.47717159608197\n",
      "Train: (168265, 20) | Test: (123623, 20)\n",
      "RMSE: 3.6063264227887837 | SUM ERROR: -337.630805106333\n",
      "Stacking Shape: (325540, 3)\n",
      "\n",
      "#========================================================================\n",
      "# CV SCORE AVG: 3.618401419254512\n",
      "# OUT SCORE: 29.72956767421913\n",
      "#========================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# CVの準備\n",
    "fold = 6\n",
    "if is_rm_out:\n",
    "    set_type = 'rm_out'\n",
    "else:\n",
    "    set_type = 'all'\n",
    "# kfold = ods_kfold(train=train, seed=328, fold=fold)\n",
    "# train.drop('rounded_target', axis=1, inplace=True)\n",
    "if is_rm_out:\n",
    "    kfold = utils.read_pkl_gzip('../input/kfold_ods_no_out_fold6_seed328.gz')\n",
    "elif is_clf_out:\n",
    "    kfold = utils.read_pkl_gzip('../input/kfold_ods_clf_out_fold6_seed328.gz')\n",
    "else:\n",
    "    kfold = utils.read_pkl_gzip('../input/kfold_ods_all_fold6_seed328.gz')\n",
    "\n",
    "#========================================================================\n",
    "# Dataset\n",
    "submit = pd.read_csv('../input/sample_submission.csv').set_index(key)\n",
    "model_list = []\n",
    "result_list = []\n",
    "score_list = []\n",
    "val_pred_list = []\n",
    "test_pred = np.zeros(len(test))\n",
    "\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term']\n",
    "Y = train[target]\n",
    "y_mean = Y.mean()\n",
    "#========================================================================\n",
    "    \n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "fit_intercept = True\n",
    "alpha = 0.4\n",
    "# alpha = 1.0\n",
    "max_iter = 1000\n",
    "normalize = False\n",
    "tol = 0.01\n",
    "param_list = np.arange(1, 10, 1)\n",
    "param_list = [0]\n",
    "# param_list = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "#========================================================================\n",
    "# 学習に使うカラム設定 \n",
    "# base_lgb_cols = sorted([col for col in base.columns if col.count('base_lgb')])[2:20]\n",
    "# use_cols = sorted([col for col in base.columns if col.count('base_')])\n",
    "lgb_cols = sorted([col for col in base.columns if col.count('lgb_')])[:20]\n",
    "nn_cols = []\n",
    "other_cols = []\n",
    "use_cols = lgb_cols + nn_cols + other_cols\n",
    "# nn_cols = sorted([col for col in base.columns if col.count('NN_')])\n",
    "\n",
    "# lgb_cols = list(set([col for col in train.columns if col not in ignore_list and col.count('lgb')]) - set(top3_lgb))\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "seed_list = np.arange(100)\n",
    "seed_list = [1]\n",
    "best_score = 100\n",
    "best_score_list = []\n",
    "# stack_num = 6 - len(top3_lgb)\n",
    "\n",
    "for seed in seed_list:\n",
    "\n",
    "#     np.random.seed(seed)\n",
    "#     use_cols = top3_lgb + list(np.random.choice(lgb_cols, size=stack_num, replace=False))\n",
    "#     use_cols = lgb_cols[:10] + nn_cols[:3]\n",
    "    \n",
    "    result_list = []\n",
    "    ridge = Ridge(solver='auto', fit_intercept=fit_intercept, alpha=alpha, max_iter=max_iter, normalize=normalize, tol=tol)\n",
    "#     ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # Train & Prediction Start\n",
    "    for fold_no, (trn_idx, val_idx) in enumerate(zip(*kfold)):\n",
    "        if key not in train.columns:\n",
    "            train = train.reset_index()\n",
    "            test = test.reset_index() \n",
    "            \n",
    "    \n",
    "        #========================================================================\n",
    "        # Make Dataset\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(pd.concat([train[use_cols], test[use_cols]]))\n",
    "        x_test = scaler.transform(test[use_cols])\n",
    "\n",
    "        X_train, y_train = train.loc[train[key].isin(trn_idx), :][use_cols], Y.loc[train[key].isin(trn_idx)]\n",
    "        X_val, y_val = train.loc[train[key].isin(val_idx), :][use_cols], Y.loc[train[key].isin(val_idx)]\n",
    "        \n",
    "        X_train[:] = scaler.transform(X_train)\n",
    "        X_val[:] = scaler.transform(X_val)\n",
    "        X_train = X_train.as_matrix()\n",
    "        X_val = X_val.as_matrix()\n",
    "    \n",
    "        print(f\"Train: {X_train.shape} | Test: {x_test.shape}\")\n",
    "        #========================================================================\n",
    "        \n",
    "        # Fitting\n",
    "        ridge.fit(X_train, y_train)\n",
    "        \n",
    "        # Prediction\n",
    "        y_pred = ridge.predict(X_val)\n",
    "        test_pred += ridge.predict(x_test)\n",
    "        \n",
    "        # Stack Prediction\n",
    "    #     df_pred = train.iloc[val_idx, :][[key, target]].copy()\n",
    "        df_pred = train.loc[train[key].isin(val_idx), :][[key, target]].copy()\n",
    "        df_pred['prediction'] = y_pred\n",
    "        result_list.append(df_pred)\n",
    "        \n",
    "        # Scoring\n",
    "        err = (y_val - y_pred)\n",
    "        score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "        score_list.append(score)\n",
    "        #========================================================================\n",
    "    \n",
    "    cv_score = np.mean(score_list)\n",
    "    \n",
    "    #========================================================================\n",
    "    # Stacking\n",
    "    test_pred /= fold_no+1\n",
    "    test['prediction'] = test_pred\n",
    "    stack_test = test[[key, 'prediction']]\n",
    "    result_list.append(stack_test)\n",
    "    df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "    if key not in base:\n",
    "        base.reset_index(inplace=True)\n",
    "    df_pred = base[[key, target]].merge(df_pred, how='inner', on=key)\n",
    "    print(f\"Stacking Shape: {df_pred.shape}\")\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # outlierに対するスコアを出す\n",
    "    if is_rm_out:\n",
    "        out_score = 0\n",
    "    else:\n",
    "        if key not in train.columns:\n",
    "            train.reset_index(inplace=True)\n",
    "        out_ids = train.loc[train.target<-30, key].values\n",
    "        out_val = train.loc[train.target<-30, target].values\n",
    "        out_pred = df_pred[df_pred[key].isin(out_ids)]['prediction'].values\n",
    "        out_score = np.sqrt(mean_squared_error(out_val, out_pred))\n",
    "    #========================================================================\n",
    "    \n",
    "    if cv_score<best_score:\n",
    "        print(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "# OUT SCORE: {out_score}\n",
    "#========================================================================''')\n",
    "    \n",
    "        best_score = cv_score\n",
    "        best_score_list = use_cols\n",
    "    \n",
    "        #========================================================================\n",
    "        # Save Stack\n",
    "        utils.to_pkl_gzip(path=f\"../stack/{start_time[4:12]}_stack_{model_type}_set-{set_type}_lgb{len(lgb_cols)}_NN{len(nn_cols)}_other{len(other_cols)}_OUT{str(out_score)[:7]}_CV{cv_score}_LB\" , obj=df_pred[[key, 'prediction']])\n",
    "        #========================================================================\n",
    "sys.exit()\n",
    "    \n",
    "#========================================================================\n",
    "# Submission\n",
    "df_pred.set_index(key, inplace=True)\n",
    "submit[target] = df_pred['prediction']\n",
    "submit_path = f'../submit/{start_time[4:12]}_submit_{model_type}_set-{set_type}_lgb{len(lgb_cols)}_NN{len(nn_cols)}_other{len(other_cols)}_OUT{str(out_score)[:7]}_CV{cv_score}_LB.csv'\n",
    "submit.to_csv(submit_path, index=True)\n",
    "display(submit.head())\n",
    "#========================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
