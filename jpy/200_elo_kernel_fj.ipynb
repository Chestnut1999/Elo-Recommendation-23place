{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 201917, test samples: 123623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train & test - done in 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2054.51 Mb (71.1% reduction)\n",
      "Mem. usage decreased to 56.19 Mb (57.6% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "historical transactions - done in 515s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 132.92 Mb (69.4% reduction)\n",
      "Mem. usage decreased to 44.53 Mb (57.6% reduction)\n",
      "new merchants - done in 240s\n",
      "additional features - done in 0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>outliers</th>\n",
       "      <th>target</th>\n",
       "      <th>quarter</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>days_feature1</th>\n",
       "      <th>days_feature2</th>\n",
       "      <th>...</th>\n",
       "      <th>price_max</th>\n",
       "      <th>duration_mean</th>\n",
       "      <th>duration_min</th>\n",
       "      <th>duration_max</th>\n",
       "      <th>amount_month_ratio_mean</th>\n",
       "      <th>amount_month_ratio_min</th>\n",
       "      <th>amount_month_ratio_max</th>\n",
       "      <th>new_CLV</th>\n",
       "      <th>hist_CLV</th>\n",
       "      <th>CLV_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_ID_92a2005557</th>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>2.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>3025.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>-13.484375</td>\n",
       "      <td>-16.781250</td>\n",
       "      <td>5.539062</td>\n",
       "      <td>-0.110352</td>\n",
       "      <td>-0.133057</td>\n",
       "      <td>0.045837</td>\n",
       "      <td>-27.688210</td>\n",
       "      <td>-3917.187012</td>\n",
       "      <td>0.007068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_3d0044924f</th>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>756.0</td>\n",
       "      <td>3024.0</td>\n",
       "      <td>756.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>-16.171875</td>\n",
       "      <td>-18.468750</td>\n",
       "      <td>1.179688</td>\n",
       "      <td>-0.111267</td>\n",
       "      <td>-0.123474</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>-2.177734</td>\n",
       "      <td>-6217.010254</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_d639edf6cd</th>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>3.0</td>\n",
       "      <td>909.0</td>\n",
       "      <td>1818.0</td>\n",
       "      <td>1818.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-15.218750</td>\n",
       "      <td>-16.437500</td>\n",
       "      <td>-9.304688</td>\n",
       "      <td>-0.124939</td>\n",
       "      <td>-0.130127</td>\n",
       "      <td>-0.076904</td>\n",
       "      <td>-0.063654</td>\n",
       "      <td>-113.311218</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_186d6a6901</th>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>3.0</td>\n",
       "      <td>513.0</td>\n",
       "      <td>2052.0</td>\n",
       "      <td>1539.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058228</td>\n",
       "      <td>-14.500000</td>\n",
       "      <td>-16.921875</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>-0.119507</td>\n",
       "      <td>-0.134033</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>-2.963068</td>\n",
       "      <td>-350.128998</td>\n",
       "      <td>0.008463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_cdbd2c0db2</th>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>4.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>1356.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-12.421875</td>\n",
       "      <td>-17.656250</td>\n",
       "      <td>13.757812</td>\n",
       "      <td>-0.100952</td>\n",
       "      <td>-0.134766</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>-64.375877</td>\n",
       "      <td>-905.339905</td>\n",
       "      <td>0.071107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature_1  feature_2  feature_3 first_active_month  outliers  \\\n",
       "card_id                                                                         \n",
       "C_ID_92a2005557   0.013145   0.008752   0.011428         2017-06-01       0.0   \n",
       "C_ID_3d0044924f   0.010712   0.011385   0.010283         2017-01-01       0.0   \n",
       "C_ID_d639edf6cd   0.010610   0.008752   0.010283         2016-08-01       0.0   \n",
       "C_ID_186d6a6901   0.010712   0.014166   0.010283         2017-09-01       0.0   \n",
       "C_ID_cdbd2c0db2   0.008058   0.014166   0.010283         2017-11-01       0.0   \n",
       "\n",
       "                   target  quarter  elapsed_time  days_feature1  \\\n",
       "card_id                                                           \n",
       "C_ID_92a2005557 -0.820283      2.0         605.0         3025.0   \n",
       "C_ID_3d0044924f  0.392913      1.0         756.0         3024.0   \n",
       "C_ID_d639edf6cd  0.688056      3.0         909.0         1818.0   \n",
       "C_ID_186d6a6901  0.142495      3.0         513.0         2052.0   \n",
       "C_ID_cdbd2c0db2 -0.159749      4.0         452.0          452.0   \n",
       "\n",
       "                 days_feature2    ...      price_max  duration_mean  \\\n",
       "card_id                           ...                                 \n",
       "C_ID_92a2005557         1210.0    ...       0.503906     -13.484375   \n",
       "C_ID_3d0044924f          756.0    ...       0.008919     -16.171875   \n",
       "C_ID_d639edf6cd         1818.0    ...           -inf     -15.218750   \n",
       "C_ID_186d6a6901         1539.0    ...       0.058228     -14.500000   \n",
       "C_ID_cdbd2c0db2         1356.0    ...       0.089355     -12.421875   \n",
       "\n",
       "                 duration_min  duration_max  amount_month_ratio_mean  \\\n",
       "card_id                                                                \n",
       "C_ID_92a2005557    -16.781250      5.539062                -0.110352   \n",
       "C_ID_3d0044924f    -18.468750      1.179688                -0.111267   \n",
       "C_ID_d639edf6cd    -16.437500     -9.304688                -0.124939   \n",
       "C_ID_186d6a6901    -16.921875      2.562500                -0.119507   \n",
       "C_ID_cdbd2c0db2    -17.656250     13.757812                -0.100952   \n",
       "\n",
       "                 amount_month_ratio_min  amount_month_ratio_max    new_CLV  \\\n",
       "card_id                                                                      \n",
       "C_ID_92a2005557               -0.133057                0.045837 -27.688210   \n",
       "C_ID_3d0044924f               -0.123474                0.008148  -2.177734   \n",
       "C_ID_d639edf6cd               -0.130127               -0.076904  -0.063654   \n",
       "C_ID_186d6a6901               -0.134033                0.021240  -2.963068   \n",
       "C_ID_cdbd2c0db2               -0.134766                0.113770 -64.375877   \n",
       "\n",
       "                    hist_CLV  CLV_ratio  \n",
       "card_id                                  \n",
       "C_ID_92a2005557 -3917.187012   0.007068  \n",
       "C_ID_3d0044924f -6217.010254   0.000350  \n",
       "C_ID_d639edf6cd  -113.311218   0.000562  \n",
       "C_ID_186d6a6901  -350.128998   0.008463  \n",
       "C_ID_cdbd2c0db2  -905.339905   0.071107  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model run - done in 756s\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import os\n",
    "import sys\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import get_categorical_features, get_numeric_features, reduce_mem_usage\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
    "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "                  'OOF_PRED', 'month_0']\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "    \n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "\n",
    "    \n",
    "# preprocessing train & test\n",
    "def train_test(num_rows=None):\n",
    "\n",
    "    # load csv\n",
    "    train_df = pd.read_csv('../input/train.csv', index_col=['card_id'], nrows=num_rows)\n",
    "    test_df = pd.read_csv('../input/test.csv', index_col=['card_id'], nrows=num_rows)\n",
    "\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(train_df), len(test_df)))\n",
    "\n",
    "    # outlier\n",
    "    train_df['outliers'] = 0\n",
    "    train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
    "\n",
    "    # set target as nan\n",
    "    test_df['target'] = np.nan\n",
    "\n",
    "    # merge\n",
    "    df = train_df.append(test_df)\n",
    "\n",
    "    del train_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "    # to datetime\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "\n",
    "    # datetime features\n",
    "    df['quarter'] = df['first_active_month'].dt.quarter\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "\n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    # one hot encoding\n",
    "    df, cols = one_hot_encoder(df, nan_as_category=False)\n",
    "\n",
    "    for f in ['feature_1','feature_2','feature_3']:\n",
    "        order_label = df.groupby([f])['outliers'].mean()\n",
    "        df[f] = df[f].map(order_label)\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# preprocessing historical transactions\n",
    "def historical_transactions(num_rows=None):\n",
    "    # load csv\n",
    "#     hist_df = pd.read_csv('../input/historical_transactions.csv', nrows=num_rows)\n",
    "    hist_df = utils.read_df_pkl('../input/histori*0*')\n",
    "\n",
    "    # fillna\n",
    "    hist_df['category_2'].fillna(1.0,inplace=True)\n",
    "    hist_df['category_3'].fillna('A',inplace=True)\n",
    "    hist_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    hist_df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    hist_df['installments'].replace(999, np.nan,inplace=True)\n",
    "\n",
    "    # trim\n",
    "    hist_df['purchase_amount'] = hist_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    hist_df['authorized_flag'] = hist_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_1'] = hist_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_3'] = hist_df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "\n",
    "    # datetime features\n",
    "    hist_df['purchase_date'] = pd.to_datetime(hist_df['purchase_date'])\n",
    "    hist_df['month'] = hist_df['purchase_date'].dt.month\n",
    "    hist_df['day'] = hist_df['purchase_date'].dt.day\n",
    "    hist_df['hour'] = hist_df['purchase_date'].dt.hour\n",
    "    hist_df['weekofyear'] = hist_df['purchase_date'].dt.weekofyear\n",
    "    hist_df['weekday'] = hist_df['purchase_date'].dt.weekday\n",
    "    hist_df['weekend'] = (hist_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "\n",
    "    # additional features\n",
    "    hist_df['price'] = hist_df['purchase_amount'] / hist_df['installments']\n",
    "\n",
    "    #Christmas : December 25 2017\n",
    "    hist_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Mothers Day: May 14 2017\n",
    "    hist_df['Mothers_Day_2017']=(pd.to_datetime('2017-06-04')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #fathers day: August 13 2017\n",
    "    hist_df['fathers_day_2017']=(pd.to_datetime('2017-08-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Childrens day: October 12 2017\n",
    "    hist_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Valentine's Day : 12th June, 2017\n",
    "    hist_df['Valentine_Day_2017']=(pd.to_datetime('2017-06-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Black Friday : 24th November 2017\n",
    "    hist_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    #2018\n",
    "    #Mothers Day: May 13 2018\n",
    "    hist_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    hist_df['month_diff'] = ((datetime.datetime.today() - hist_df['purchase_date']).dt.days)//30\n",
    "    hist_df['month_diff'] += hist_df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    hist_df['duration'] = hist_df['purchase_amount']*hist_df['month_diff']\n",
    "    hist_df['amount_month_ratio'] = hist_df['purchase_amount']/hist_df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    hist_df = utils.reduce_mem_usage(hist_df)\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['authorized_flag'] = ['mean']\n",
    "    aggs['weekend'] = ['mean'] # overwrite\n",
    "    aggs['weekday'] = ['mean'] # overwrite\n",
    "    aggs['day'] = ['nunique', 'mean', 'min'] # overwrite\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['sum','mean','max','min','var']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2017'] = ['mean']\n",
    "    aggs['fathers_day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Valentine_Day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        hist_df[col+'_mean'] = hist_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        hist_df[col+'_min'] = hist_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        hist_df[col+'_max'] = hist_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        hist_df[col+'_sum'] = hist_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    hist_df = hist_df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    hist_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in hist_df.columns.tolist()])\n",
    "    hist_df.columns = ['hist_'+ c for c in hist_df.columns]\n",
    "\n",
    "    hist_df['hist_purchase_date_diff'] = (hist_df['hist_purchase_date_max']-hist_df['hist_purchase_date_min']).dt.days\n",
    "    hist_df['hist_purchase_date_average'] = hist_df['hist_purchase_date_diff']/hist_df['hist_card_id_size']\n",
    "    hist_df['hist_purchase_date_uptonow'] = (datetime.datetime.today()-hist_df['hist_purchase_date_max']).dt.days\n",
    "    hist_df['hist_purchase_date_uptomin'] = (datetime.datetime.today()-hist_df['hist_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    hist_df = utils.reduce_mem_usage(hist_df)\n",
    "\n",
    "    return hist_df\n",
    "    \n",
    "# preprocessing new_merchant_transactions\n",
    "def new_merchant_transactions(num_rows=None):\n",
    "    # load csv\n",
    "    new_merchant_df = utils.read_df_pkl('../input/new_mer*0*')\n",
    "\n",
    "    # fillna\n",
    "#     new_merchant_df['category_2'].fillna(1.0,inplace=True)\n",
    "    new_merchant_df['category_2'] = new_merchant_df['category_2'].map(lambda x: 1 if x=='NA' else x)\n",
    "    new_merchant_df['category_3'].fillna('A',inplace=True)\n",
    "    new_merchant_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    new_merchant_df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    new_merchant_df['installments'].replace(999, np.nan,inplace=True)\n",
    "\n",
    "    # trim\n",
    "    new_merchant_df['purchase_amount'] = new_merchant_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    new_merchant_df['authorized_flag'] = new_merchant_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_1'] = new_merchant_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_3'] = new_merchant_df['category_3'].map({'A':0, 'B':1, 'C':2, 'NA':0}).astype(int)\n",
    "\n",
    "    # datetime features\n",
    "    new_merchant_df['purchase_date'] = pd.to_datetime(new_merchant_df['purchase_date'])\n",
    "    new_merchant_df['month'] = new_merchant_df['purchase_date'].dt.month\n",
    "    new_merchant_df['day'] = new_merchant_df['purchase_date'].dt.day\n",
    "    new_merchant_df['hour'] = new_merchant_df['purchase_date'].dt.hour\n",
    "    new_merchant_df['weekofyear'] = new_merchant_df['purchase_date'].dt.weekofyear\n",
    "    new_merchant_df['weekday'] = new_merchant_df['purchase_date'].dt.weekday\n",
    "    new_merchant_df['weekend'] = (new_merchant_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "\n",
    "    # additional features\n",
    "    new_merchant_df['price'] = new_merchant_df['purchase_amount'] / new_merchant_df['installments']\n",
    "\n",
    "    #Christmas : December 25 2017\n",
    "    new_merchant_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Childrens day: October 12 2017\n",
    "    new_merchant_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Black Friday : 24th November 2017\n",
    "    new_merchant_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    #Mothers Day: May 13 2018\n",
    "    new_merchant_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    new_merchant_df['month_diff'] = ((datetime.datetime.today() - new_merchant_df['purchase_date']).dt.days)//30\n",
    "    new_merchant_df['month_diff'] += new_merchant_df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    new_merchant_df['duration'] = new_merchant_df['purchase_amount']*new_merchant_df['month_diff']\n",
    "    new_merchant_df['amount_month_ratio'] = new_merchant_df['purchase_amount']/new_merchant_df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = utils.reduce_mem_usage(new_merchant_df)\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['mean','var','skew']\n",
    "    aggs['weekend'] = ['mean']\n",
    "    aggs['month'] = ['mean', 'min', 'max']\n",
    "    aggs['weekday'] = ['mean', 'min', 'max']\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['mean','max','min','var']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        new_merchant_df[col+'_mean'] = new_merchant_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        new_merchant_df[col+'_min'] = new_merchant_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        new_merchant_df[col+'_max'] = new_merchant_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        new_merchant_df[col+'_sum'] = new_merchant_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    new_merchant_df = new_merchant_df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    new_merchant_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_merchant_df.columns.tolist()])\n",
    "    new_merchant_df.columns = ['new_'+ c for c in new_merchant_df.columns]\n",
    "\n",
    "    new_merchant_df['new_purchase_date_diff'] = (new_merchant_df['new_purchase_date_max']-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "    new_merchant_df['new_purchase_date_average'] = new_merchant_df['new_purchase_date_diff']/new_merchant_df['new_card_id_size']\n",
    "    new_merchant_df['new_purchase_date_uptonow'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_max']).dt.days\n",
    "    new_merchant_df['new_purchase_date_uptomin'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = utils.reduce_mem_usage(new_merchant_df)\n",
    "\n",
    "    return new_merchant_df\n",
    "\n",
    "# additional features\n",
    "def additional_features(df):\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    date_features=['hist_purchase_date_max','hist_purchase_date_min',\n",
    "                   'new_purchase_date_max', 'new_purchase_date_min']\n",
    "\n",
    "    for f in date_features:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "\n",
    "    df['card_id_total'] = df['new_card_id_size']+df['hist_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['new_card_id_count']+df['hist_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_purchase_amount_mean']+df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_purchase_amount_max']+df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_purchase_amount_min']+df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum']/df['hist_purchase_amount_sum']\n",
    "    df['month_diff_mean'] = df['new_month_diff_mean']+df['hist_month_diff_mean']\n",
    "    df['month_diff_ratio'] = df['new_month_diff_mean']/df['hist_month_diff_mean']\n",
    "    df['month_lag_mean'] = df['new_month_lag_mean']+df['hist_month_lag_mean']\n",
    "    df['month_lag_max'] = df['new_month_lag_max']+df['hist_month_lag_max']\n",
    "    df['month_lag_min'] = df['new_month_lag_min']+df['hist_month_lag_min']\n",
    "    df['category_1_mean'] = df['new_category_1_mean']+df['hist_category_1_mean']\n",
    "    df['installments_total'] = df['new_installments_sum']+df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_installments_mean']+df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_installments_max']+df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_installments_sum']/df['hist_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "    df['duration_mean'] = df['new_duration_mean']+df['hist_duration_mean']\n",
    "    df['duration_min'] = df['new_duration_min']+df['hist_duration_min']\n",
    "    df['duration_max'] = df['new_duration_max']+df['hist_duration_max']\n",
    "    df['amount_month_ratio_mean']=df['new_amount_month_ratio_mean']+df['hist_amount_month_ratio_mean']\n",
    "    df['amount_month_ratio_min']=df['new_amount_month_ratio_min']+df['hist_amount_month_ratio_min']\n",
    "    df['amount_month_ratio_max']=df['new_amount_month_ratio_max']+df['hist_amount_month_ratio_max']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "\n",
    "    return df\n",
    "\n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "def kfold_lightgbm(train_df, test_df, num_folds, stratified = False, debug= False):\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
    "\n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['outliers'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
    "\n",
    "        # set data structure\n",
    "        lgb_train = lgb.Dataset(train_x,\n",
    "                                label=train_y,\n",
    "                                free_raw_data=False)\n",
    "        lgb_test = lgb.Dataset(valid_x,\n",
    "                               label=valid_y,\n",
    "                               free_raw_data=False)\n",
    "\n",
    "        # params optimized by optuna\n",
    "        params ={\n",
    "                'task': 'train',\n",
    "                'boosting': 'goss',\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'learning_rate': 0.01,\n",
    "                'subsample': 0.9855232997390695,\n",
    "                'max_depth': 7,\n",
    "                'top_rate': 0.9064148448434349,\n",
    "                'num_leaves': 63,\n",
    "                'min_child_weight': 41.9612869171337,\n",
    "                'other_rate': 0.0721768246018207,\n",
    "                'reg_alpha': 9.677537745007898,\n",
    "                'colsample_bytree': 0.5665320670155495,\n",
    "                'min_split_gain': 9.820197773625843,\n",
    "                'reg_lambda': 8.2532317400459,\n",
    "                'min_data_in_leaf': 21,\n",
    "                'verbose': -1,\n",
    "                'seed':int(2**n_fold),\n",
    "                'bagging_seed':int(2**n_fold),\n",
    "                'drop_seed':int(2**n_fold)\n",
    "                }\n",
    "\n",
    "        reg = lgb.train(\n",
    "                        params,\n",
    "                        lgb_train,\n",
    "                        valid_sets=[lgb_train, lgb_test],\n",
    "                        valid_names=['train', 'test'],\n",
    "                        num_boost_round=10000,\n",
    "                        early_stopping_rounds= 200,\n",
    "                        verbose_eval=100\n",
    "                        )\n",
    "\n",
    "        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "        sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_preds[valid_idx])))\n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # display importances\n",
    "    display_importances(feature_importance_df)\n",
    "\n",
    "    if not debug:\n",
    "        # save submission file\n",
    "        test_df.loc[:,'target'] = sub_preds\n",
    "        test_df = test_df.reset_index()\n",
    "        test_df[['card_id', 'target']].to_csv(submission_file_name, index=False)\n",
    "\n",
    "def main(debug=False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    with timer(\"train & test\"):\n",
    "        df = train_test(num_rows)\n",
    "    with timer(\"historical transactions\"):\n",
    "        df = pd.merge(df, historical_transactions(num_rows), on='card_id', how='outer')\n",
    "    with timer(\"new merchants\"):\n",
    "        df = pd.merge(df, new_merchant_transactions(num_rows), on='card_id', how='outer')\n",
    "    with timer(\"additional features\"):\n",
    "        df = additional_features(df)\n",
    "#     with timer(\"split train & test\"):\n",
    "#         train_df = df[df['target'].notnull()]\n",
    "#         test_df = df[df['target'].isnull()]\n",
    "#         del df\n",
    "#         gc.collect()\n",
    "#     with timer(\"Run LightGBM with kfold\"):\n",
    "#         kfold_lightgbm(train_df, test_df, num_folds=11, stratified=False, debug=debug)\n",
    "\n",
    "    return df\n",
    "        \n",
    "        \n",
    "submission_file_name = \"submission.csv\"\n",
    "with timer(\"Full model run\"):\n",
    "    df_feat = main(debug=False)\n",
    "    display(df_feat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_1\n",
      "feature_2\n",
      "feature_3\n",
      "first_active_month\n",
      "outliers\n",
      "target\n",
      "quarter\n",
      "elapsed_time\n",
      "days_feature1\n",
      "days_feature2\n",
      "days_feature3\n",
      "days_feature1_ratio\n",
      "days_feature2_ratio\n",
      "days_feature3_ratio\n",
      "feature_sum\n",
      "feature_mean\n",
      "feature_max\n",
      "feature_min\n",
      "feature_var\n",
      "hist_subsector_id_nunique\n",
      "hist_merchant_id_nunique\n",
      "hist_merchant_category_id_nunique\n",
      "hist_month_nunique\n",
      "hist_month_mean\n",
      "hist_month_min\n",
      "hist_month_max\n",
      "hist_hour_nunique\n",
      "hist_hour_mean\n",
      "hist_hour_min\n",
      "hist_hour_max\n",
      "hist_weekofyear_nunique\n",
      "hist_weekofyear_mean\n",
      "hist_weekofyear_min\n",
      "hist_weekofyear_max\n",
      "hist_weekday_mean\n",
      "hist_day_nunique\n",
      "hist_day_mean\n",
      "hist_day_min\n",
      "hist_purchase_amount_sum\n",
      "hist_purchase_amount_max\n",
      "hist_purchase_amount_min\n",
      "hist_purchase_amount_mean\n",
      "hist_purchase_amount_var\n",
      "hist_purchase_amount_skew\n",
      "hist_installments_sum\n",
      "hist_installments_max\n",
      "hist_installments_mean\n",
      "hist_installments_var\n",
      "hist_installments_skew\n",
      "hist_purchase_date_max\n",
      "hist_purchase_date_min\n",
      "hist_month_lag_max\n",
      "hist_month_lag_min\n",
      "hist_month_lag_mean\n",
      "hist_month_lag_var\n",
      "hist_month_lag_skew\n",
      "hist_month_diff_max\n",
      "hist_month_diff_min\n",
      "hist_month_diff_mean\n",
      "hist_month_diff_var\n",
      "hist_month_diff_skew\n",
      "hist_authorized_flag_mean\n",
      "hist_weekend_mean\n",
      "hist_category_1_mean\n",
      "hist_category_2_mean\n",
      "hist_category_3_mean\n",
      "hist_card_id_size\n",
      "hist_card_id_count\n",
      "hist_price_sum\n",
      "hist_price_mean\n",
      "hist_price_max\n",
      "hist_price_min\n",
      "hist_price_var\n",
      "hist_Christmas_Day_2017_mean\n",
      "hist_Mothers_Day_2017_mean\n",
      "hist_fathers_day_2017_mean\n",
      "hist_Children_day_2017_mean\n",
      "hist_Valentine_Day_2017_mean\n",
      "hist_Black_Friday_2017_mean\n",
      "hist_Mothers_Day_2018_mean\n",
      "hist_duration_mean\n",
      "hist_duration_min\n",
      "hist_duration_max\n",
      "hist_duration_var\n",
      "hist_duration_skew\n",
      "hist_amount_month_ratio_mean\n",
      "hist_amount_month_ratio_min\n",
      "hist_amount_month_ratio_max\n",
      "hist_amount_month_ratio_var\n",
      "hist_amount_month_ratio_skew\n",
      "hist_category_2_mean_mean\n",
      "hist_category_3_mean_mean\n",
      "hist_purchase_date_diff\n",
      "hist_purchase_date_average\n",
      "hist_purchase_date_uptonow\n",
      "hist_purchase_date_uptomin\n",
      "new_subsector_id_nunique\n",
      "new_merchant_id_nunique\n",
      "new_merchant_category_id_nunique\n",
      "new_month_mean\n",
      "new_month_min\n",
      "new_month_max\n",
      "new_hour_nunique\n",
      "new_hour_mean\n",
      "new_hour_min\n",
      "new_hour_max\n",
      "new_weekofyear_nunique\n",
      "new_weekofyear_mean\n",
      "new_weekofyear_min\n",
      "new_weekofyear_max\n",
      "new_weekday_mean\n",
      "new_weekday_min\n",
      "new_weekday_max\n",
      "new_day_nunique\n",
      "new_day_mean\n",
      "new_day_min\n",
      "new_day_max\n",
      "new_purchase_amount_sum\n",
      "new_purchase_amount_max\n",
      "new_purchase_amount_min\n",
      "new_purchase_amount_mean\n",
      "new_purchase_amount_var\n",
      "new_purchase_amount_skew\n",
      "new_installments_sum\n",
      "new_installments_max\n",
      "new_installments_mean\n",
      "new_installments_var\n",
      "new_installments_skew\n",
      "new_purchase_date_max\n",
      "new_purchase_date_min\n",
      "new_month_lag_max\n",
      "new_month_lag_min\n",
      "new_month_lag_mean\n",
      "new_month_lag_var\n",
      "new_month_lag_skew\n",
      "new_month_diff_mean\n",
      "new_month_diff_var\n",
      "new_month_diff_skew\n",
      "new_weekend_mean\n",
      "new_category_1_mean\n",
      "new_category_2_mean\n",
      "new_category_3_mean\n",
      "new_card_id_size\n",
      "new_card_id_count\n",
      "new_price_mean\n",
      "new_price_max\n",
      "new_price_min\n",
      "new_price_var\n",
      "new_Christmas_Day_2017_mean\n",
      "new_Children_day_2017_mean\n",
      "new_Black_Friday_2017_mean\n",
      "new_Mothers_Day_2018_mean\n",
      "new_duration_mean\n",
      "new_duration_min\n",
      "new_duration_max\n",
      "new_duration_var\n",
      "new_duration_skew\n",
      "new_amount_month_ratio_mean\n",
      "new_amount_month_ratio_min\n",
      "new_amount_month_ratio_max\n",
      "new_amount_month_ratio_var\n",
      "new_amount_month_ratio_skew\n",
      "new_category_2_mean_mean\n",
      "new_category_3_mean_mean\n",
      "new_purchase_date_diff\n",
      "new_purchase_date_average\n",
      "new_purchase_date_uptonow\n",
      "new_purchase_date_uptomin\n",
      "hist_first_buy\n",
      "hist_last_buy\n",
      "new_first_buy\n",
      "new_last_buy\n",
      "card_id_total\n",
      "card_id_cnt_total\n",
      "card_id_cnt_ratio\n",
      "purchase_amount_total\n",
      "purchase_amount_mean\n",
      "purchase_amount_max\n",
      "purchase_amount_min\n",
      "purchase_amount_ratio\n",
      "month_diff_mean\n",
      "month_diff_ratio\n",
      "month_lag_mean\n",
      "month_lag_max\n",
      "month_lag_min\n",
      "category_1_mean\n",
      "installments_total\n",
      "installments_mean\n",
      "installments_max\n",
      "installments_ratio\n",
      "price_total\n",
      "price_mean\n",
      "price_max\n",
      "duration_mean\n",
      "duration_min\n",
      "duration_max\n",
      "amount_month_ratio_mean\n",
      "amount_month_ratio_min\n",
      "amount_month_ratio_max\n",
      "new_CLV\n",
      "hist_CLV\n",
      "CLV_ratio\n"
     ]
    }
   ],
   "source": [
    "for col in df_feat.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '200_ker'\n",
    "ignore_features = ['first_active_month', 'target', 'outliers']\n",
    "\n",
    "for col in df_feat.columns:\n",
    "    if col in ignore_features: continue\n",
    "        \n",
    "    feature = df_feat[col].values.astype('float32')\n",
    "    utils.to_pkl_gzip(path = f'../features/2_second_valid/{prefix}_{col}@', obj=feature)\n",
    "#     utils.to_pkl_gzip(path = f'../features/2_second_valid/{prefix}_{col}@', obj=feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
