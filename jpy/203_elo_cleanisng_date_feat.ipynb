{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "import os\n",
    "import sys\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import get_categorical_features, get_numeric_features, reduce_mem_usage\n",
    "from preprocessing import get_dummies\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "os.listdir('../input/')\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_avtive_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 96.96it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 193.23it/s]\n",
      "100%|██████████| 3/3 [00:11<00:00,  3.75s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n",
      "Mem. usage decreased to 2554.26 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 183.47 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = utils.read_df_pkl('../input/train0*')\n",
    "df_test = utils.read_df_pkl('../input/test0*')\n",
    "df_train.set_index(key, inplace=True)\n",
    "df_test.set_index(key, inplace=True)\n",
    "train_test = pd.concat([df_train, df_test], axis=0)\n",
    "df_hist = utils.read_df_pkl('../input/hist_clean*')\n",
    "df_new = utils.read_df_pkl('../input/new_clean*')\n",
    "\n",
    "df_train = utils.reduce_mem_usage(df_train)\n",
    "df_test  = utils.reduce_mem_usage(df_test )\n",
    "df_hist  = utils.reduce_mem_usage(df_hist )\n",
    "df_new   = utils.reduce_mem_usage(df_new  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_max_date = df_hist.groupby(key)['purchase_date'].max()\n",
    "hist_min_date = df_hist.groupby(key)['purchase_date'].min()\n",
    "hist_max_date.name = 'hist_purchase_date_max'\n",
    "hist_min_date.name = 'hist_purchase_date_min'\n",
    "hist_max_date = hist_max_date.to_frame()\n",
    "hist_min_date = hist_min_date.to_frame()\n",
    "\n",
    "# month_maxは翌月の数字にする\n",
    "hist_max_date['hist_purchase_month_max'] = hist_max_date['hist_purchase_date_max'].map(lambda x: \n",
    "                                                                           str(int(str(x)[:4])+1) + '-' + '01' \n",
    "                                                                              if str(x)[:7][-2:]=='12' \n",
    "                                                                           else str(x)[:4] + '-0' + str(int(str(x)[:7][-1:])+1)\n",
    "                                                                           if str(x)[:7][-2:]!='09' \n",
    "                                                                           else str(x)[:4] + '-10')\n",
    "\n",
    "hist_min_date['hist_purchase_month_min'] = hist_min_date['hist_purchase_date_min'].map(lambda x: str(x)[:7])\n",
    "\n",
    "\n",
    "new_max_date = df_new.groupby(key)['purchase_date'].max()\n",
    "new_min_date = df_new.groupby(key)['purchase_date'].min()\n",
    "new_max_date.name = 'new_purchase_date_max'\n",
    "new_min_date.name = 'new_purchase_date_min'\n",
    "new_max_date = new_max_date.to_frame()\n",
    "new_min_date = new_min_date.to_frame()\n",
    "\n",
    "# month_maxは翌月の数字にする\n",
    "new_max_date['new_purchase_month_max'] = new_max_date['new_purchase_date_max'].map(lambda x: \n",
    "                                                                           str(int(str(x)[:4])+1) + '-' + '01' \n",
    "                                                                              if str(x)[:7][-2:]=='12' \n",
    "                                                                           else str(x)[:4] + '-0' + str(int(str(x)[:7][-1:])+1)\n",
    "                                                                           if str(x)[:7][-2:]!='09' \n",
    "                                                                           else str(x)[:4] + '-10')\n",
    "new_min_date['new_purchase_month_min'] = new_min_date['new_purchase_date_min'].map(lambda x: str(x)[:7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist.set_index(key, inplace=True)\n",
    "df_new.set_index(key, inplace=True)\n",
    "df_hist = df_hist.join(hist_max_date).join(hist_min_date).join(new_max_date).join(new_min_date)\n",
    "df_new = df_new.join(hist_max_date).join(hist_min_date).join(new_max_date).join(new_min_date)\n",
    "\n",
    "df_hist['hist_purchase_month_max'] = pd.to_datetime(df_hist['hist_purchase_month_max'])\n",
    "df_hist['hist_purchase_month_min'] = pd.to_datetime(df_hist['hist_purchase_month_min'])\n",
    "df_hist['new_purchase_month_max'] = pd.to_datetime(df_hist['new_purchase_month_max'])\n",
    "df_hist['new_purchase_month_min'] = pd.to_datetime(df_hist['new_purchase_month_min'])\n",
    "\n",
    "df_new['new_purchase_month_max'] = pd.to_datetime(df_new['new_purchase_month_max'])\n",
    "df_new['new_purchase_month_min'] = pd.to_datetime(df_new['new_purchase_month_min'])\n",
    "df_new['hist_purchase_month_max'] = pd.to_datetime(df_new['hist_purchase_month_max'])\n",
    "df_new['hist_purchase_month_min'] = pd.to_datetime(df_new['hist_purchase_month_min'])\n",
    "\n",
    "# display(df_hist.head())\n",
    "# display(df_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26595452, 28)\n",
      "(2516909, 28)\n"
     ]
    }
   ],
   "source": [
    "df_new_lag1 = df_new[df_new['month_lag']==1]\n",
    "df_new_lag2 = df_new[df_new['month_lag']==2]\n",
    "\n",
    "df = df_hist\n",
    "\n",
    "auth1 = df[df.authorized_flag==1]\n",
    "auth0 = df[df.authorized_flag==0]\n",
    "print(auth1.shape)\n",
    "print(auth0.shape)\n",
    "\n",
    "cat1_0 = False\n",
    "# cat1_0 = True\n",
    "if cat1_0:\n",
    "    auth1_cat1 = auth1[auth1.category_1==1]\n",
    "    auth1_cat0 = auth1[auth1.category_1==0]\n",
    "    auth0_cat1 = auth0[auth0.category_1==1]\n",
    "    auth0_cat0 = auth0[auth0.category_1==0]\n",
    "    new_cat1 = df_new[df_new.category_1==1]\n",
    "    new_cat0 = df_new[df_new.category_1==0]\n",
    "#     del auth1, auth0\n",
    "#     gc.collect()\n",
    "\n",
    "auth1_lag0 = auth1[auth1['month_lag']==0]\n",
    "auth1_lag1 = auth1[auth1['month_lag']==-1]\n",
    "auth1_lag2 = auth1[auth1['month_lag']==-2]\n",
    "auth1_lag02 = auth1[auth1['month_lag']>=-2]\n",
    "auth1_lag05 = auth1[auth1['month_lag']>=-5]\n",
    "\n",
    "auth0_lag0 = auth0[auth0['month_lag']==0]\n",
    "auth0_lag1 = auth0[auth0['month_lag']==-1]\n",
    "auth0_lag2 = auth0[auth0['month_lag']==-2]\n",
    "auth0_lag02 = auth0[auth0['month_lag']>=-2]\n",
    "auth0_lag05 = auth0[auth0['month_lag']>=-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 109.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 173.83it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(1963031, 39)\n"
     ]
    }
   ],
   "source": [
    "prefix = '203_pst'\n",
    "new_df_list = [df_new, df_new_lag1, df_new_lag2]\n",
    "new_fname_list = ['new', 'new_lag1', 'new_lag2']\n",
    "\n",
    "debug = False\n",
    "if debug:\n",
    "    train_test = train_test.head(10000)\n",
    "        \n",
    "\n",
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# 集計が必要なFeature\n",
    "#========================================================================\n",
    "for df, fname in zip(tqdm(new_df_list), new_fname_list):\n",
    "    if debug:\n",
    "        df = df.head(3000)\n",
    "#     df.drop(['level_0', 'index'], axis=1, inplace=True)\n",
    "#     sys.exit()\n",
    "    \n",
    "    df_train = utils.read_df_pkl('../input/train0*')\n",
    "    df_test = utils.read_df_pkl('../input/test0*')\n",
    "    df_train.set_index(key, inplace=True)\n",
    "    df_test.set_index(key, inplace=True)\n",
    "    train_test = pd.concat([df_train, df_test], axis=0)\n",
    "    \n",
    "    # new\n",
    "    # new_first_buyと同じだな\n",
    "    # df['personal_new_elapsed_month'] = (df['new_purchase_month_min'] - df['first_active_month']).dt.days\n",
    "#     df['diff_date_from_new_min_month'] = (df['new_purchase_month_min'] - df['purchase_date']).dt.days\n",
    "#     df['diff_date_from_new_max_month'] = (df['new_purchase_month_max'] - df['purchase_date']).dt.days\n",
    "#     df['diff_date_from_hist_min_month'] = (df['hist_purchase_month_min'] - df['purchase_date']).dt.days\n",
    "#     df['diff_date_from_hist_max_month'] = (df['hist_purchase_month_max'] - df['purchase_date']).dt.days\n",
    "\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    aggs = {}\n",
    "#     aggs['diff_date_from_new_min_month'] = ['mean', 'max', 'min', 'std', 'skew']\n",
    "#     aggs['diff_date_from_new_max_month'] = ['mean', 'max', 'min', 'std', 'skew']\n",
    "#     aggs['diff_date_from_hist_min_month'] = ['mean', 'max', 'min', 'std', 'skew']\n",
    "#     aggs['diff_date_from_hist_max_month'] = ['mean', 'max', 'min', 'std', 'skew']\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id', 'yyyymmdd', 'yyyy_week']\n",
    "    \n",
    "    df = pd.get_dummies(df, columns=['category_2', 'category_3'])\n",
    "    aggs = {\n",
    "    'category_1': ['sum', 'mean'],\n",
    "    'category_2_1.0': ['mean'],\n",
    "    'category_2_2.0': ['mean'],\n",
    "    'category_2_3.0': ['mean'],\n",
    "    'category_2_4.0': ['mean'],\n",
    "    'category_2_5.0': ['mean'],\n",
    "    'category_3_A': ['mean'],\n",
    "    'category_3_B': ['mean'],\n",
    "    'category_3_C': ['mean'],\n",
    "    }\n",
    "    \n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum', 'max','mean','var','skew']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['mean','var','skew']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    new_columns = get_new_columns(fname, aggs)\n",
    "    \n",
    "    print('Aggregation Start!')\n",
    "    print(df.shape)\n",
    "    df_agg = df.groupby(key).agg(aggs)\n",
    "    df_agg.columns = new_columns\n",
    "    \n",
    "    df_agg[f'{fname}_amount_sum_per_installments_sum'] = df_agg[f'{fname}_purchase_amount_sum'] / (df_agg[f'{fname}_installments_sum'] + 1.0)\n",
    "    df_agg[f'{fname}_amount_mean_per_installments_mean'] = df_agg[f'{fname}_purchase_amount_mean'] / (df_agg[f'{fname}_installments_mean'] + 1.0)\n",
    "    \n",
    "    \n",
    "    #========================================================================\n",
    "    # monthly agg\n",
    "    #========================================================================\n",
    "    new_columns = get_new_columns(fname + '_monthly_avg', aggs)\n",
    "    month_agg = df.groupby([key, 'yyyymm']).agg(aggs)\n",
    "        \n",
    "    month_agg.columns = new_columns\n",
    "    month_agg = month_agg.reset_index().drop('yyyymm', axis=1).groupby([key]).mean()\n",
    "    \n",
    "#     train_test = train_test.join(df_agg)\n",
    "    train_test = train_test.join(df_agg).join(month_agg)\n",
    "    print(train_test.shape)\n",
    "    \n",
    "    #========================================================================\n",
    "    # Save Feature\n",
    "    ignore_features = ['first_active_month', 'card_id', target]\n",
    "    for col in train_test.columns:\n",
    "        if col in ignore_features: continue\n",
    "        if col.count('feature_'):continue\n",
    "        feature = train_test[col].astype('float32').values\n",
    "        col = col.replace('.', '_')\n",
    "    #     utils.to_pkl_gzip(path = f'../features/1_first_valid/{prefix}_{col}@', obj=feature)\n",
    "        utils.to_pkl_gzip(path = f'../features/3_third_valid/{prefix}_{col}@', obj=feature)\n",
    "    #========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 123.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 178.74it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Time Feature.\n",
      "(325540, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:20<00:20, 20.08s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 113.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 175.03it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Time Feature.\n",
      "(325540, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:40<00:00, 20.09s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "hist_df_list = [auth1, auth1_lag0, auth1_lag1, auth1_lag2, auth1_lag02, auth1_lag05, auth0, auth0_lag0, auth0_lag1, auth0_lag2, auth0_lag02, auth0_lag05] \n",
    "hist_fname_list = ['auth1', 'auth1_lag0', 'auth1_lag1', 'auth1_lag2', 'auth1_lag02', 'auth1_lag05', 'auth0', 'auth0_lag0', 'auth0_lag1', 'auth0_lag2', 'auth0_lag02', 'auth0_lag05']\n",
    "\n",
    "for df, fname in zip(tqdm(hist_df_list), hist_fname_list):\n",
    "    if debug:\n",
    "        df = df.head(3000)\n",
    "    \n",
    "    df_train = utils.read_df_pkl('../input/train0*')\n",
    "    df_test = utils.read_df_pkl('../input/test0*')\n",
    "    df_train.set_index(key, inplace=True)\n",
    "    df_test.set_index(key, inplace=True)\n",
    "    train_test = pd.concat([df_train, df_test], axis=0)\n",
    "    \n",
    "#     hist\n",
    "    print('Personal Time Feature.')\n",
    "    df['diff_date_from_new_min_month'] = (df['new_purchase_month_min'] - df['purchase_date']).dt.days\n",
    "    df['diff_date_from_new_max_month'] = (df['new_purchase_month_max'] - df['purchase_date']).dt.days\n",
    "    df['diff_date_from_hist_min_month'] = (df['hist_purchase_month_min'] - df['purchase_date']).dt.days\n",
    "    df['diff_date_from_hist_max_month'] = (df['hist_purchase_month_max'] - df['purchase_date']).dt.days\n",
    "    \n",
    "    aggs = {}\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id', 'yyyymmdd', 'yyyy_week']\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df = pd.get_dummies(df, columns=['category_2', 'category_3'])\n",
    "    aggs = {\n",
    "    'category_2_1.0': ['mean'],\n",
    "    'category_2_2.0': ['mean'],\n",
    "    'category_2_3.0': ['mean'],\n",
    "    'category_2_4.0': ['mean'],\n",
    "    'category_2_5.0': ['mean'],\n",
    "    'category_3_A': ['mean'],\n",
    "    'category_3_B': ['mean'],\n",
    "    'category_3_C': ['mean'],\n",
    "    }\n",
    "    \n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    aggs['diff_date_from_new_min_month'] = ['mean', 'max', 'min', 'std', 'skew']\n",
    "    aggs['diff_date_from_new_max_month'] = ['mean', 'max', 'min', 'std', 'skew']\n",
    "    aggs['diff_date_from_hist_min_month'] = ['mean', 'max', 'min', 'std', 'skew']\n",
    "    aggs['diff_date_from_hist_max_month'] = ['mean', 'max', 'min', 'std', 'skew']\n",
    "    \n",
    "    aggs['category_1'] = ['sum', 'mean']\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['mean','var','skew']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    \n",
    "    new_columns = get_new_columns(fname, aggs)\n",
    "    df_agg = df.groupby(key).agg(aggs)\n",
    "    df_agg.columns = new_columns\n",
    "    \n",
    "    df_agg[f'{fname}_amount_sum_per_installments_sum'] = df_agg[f'{fname}_purchase_amount_sum'] / (df_agg[f'{fname}_installments_sum'] + 1.0)\n",
    "    df_agg[f'{fname}_amount_mean_per_installments_mean'] = df_agg[f'{fname}_purchase_amount_mean'] / (df_agg[f'{fname}_installments_mean'] + 1.0)\n",
    "    \n",
    "    #========================================================================\n",
    "    # monthly agg\n",
    "    #========================================================================\n",
    "    new_columns = get_new_columns(fname + '_monthly_avg', aggs)\n",
    "    month_agg = df.groupby([key, 'yyyymm']).agg(aggs)\n",
    "        \n",
    "    month_agg.columns = new_columns\n",
    "    month_agg = month_agg.reset_index().drop('yyyymm', axis=1).groupby([key]).mean()\n",
    "    \n",
    "    train_test = train_test.join(df_agg).join(month_agg)\n",
    "    print(train_test.shape)\n",
    "    \n",
    "    #========================================================================\n",
    "    # Save Feature\n",
    "    ignore_features = ['first_active_month', 'card_id', target]\n",
    "    for col in train_test.columns:\n",
    "        if col in ignore_features: continue\n",
    "        if col.count('feature_'):continue\n",
    "        feature = train_test[col].astype('float32').values\n",
    "        col = col.replace('.', '_')\n",
    "    #     utils.to_pkl_gzip(path = f'../features/1_first_valid/{prefix}_{col}@', obj=feature)\n",
    "        utils.to_pkl_gzip(path = f'../features/3_third_valid/{prefix}_{col}@', obj=feature)\n",
    "    #========================================================================    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# これは作成済のFeatureを読み込んで計算する\n",
    "#========================================================================\n",
    "combi_list = [ ['new', 'new_lag1'] ,['new', 'auth1'] ,['new', 'auth1_lag0'] ,['new', 'auth1_lag02'] ,['new', 'auth1_lag05'] ,['new', 'auth0_lag0'] ,\n",
    "['new', 'auth0_lag02'] ,['new', 'auth0_lag05'] ,['auth1_lag0', 'auth1_lag02'] ,['auth1_lag0', 'auth1_lag05'] ,['auth1_lag02', 'auth1_lag05'] ,['auth0_lag02', 'auth0_lag05']\n",
    "]\n",
    "\n",
    "\n",
    "comp_cols = [ 'amount_sum_per_installments_sum'\n",
    "             ,'amount_mean_per_installments_mean'\n",
    "             ,'purchase_amount_sum' \n",
    "             ,'purchase_amount_mean' \n",
    "             ,'purchase_amount_max' \n",
    "             ,'purchase_amount_min' \n",
    "             ,'installments_max' \n",
    "             ,'installments_var' \n",
    "             ,'category_1_sum' \n",
    "             ,'category_1_mean' \n",
    "             ,'month_lag_mean' \n",
    "             ,'month_lag_var' \n",
    "             ,'month_lag_skew' \n",
    "             ,'month_diff_mean' \n",
    "             ,'month_diff_var' \n",
    "             ,'month_diff_skew' \n",
    "             ,'card_id_size' \n",
    "             ,'yyyymmdd_nunique'\n",
    "             ,'subsector_id_nunique'\n",
    "             ,'merchant_id_nunique'\n",
    "             ,'merchant_category_id_nunique'\n",
    "             ,'diff_date_from_new_min_month_mean'\n",
    "             ,'diff_date_from_hist_max_month_mean'\n",
    "             ,'diff_date_from_new_min_month_max'\n",
    "             ,'diff_date_from_hist_max_month_max'\n",
    "             ,'diff_date_from_new_min_month_min'\n",
    "             ,'diff_date_from_hist_min_month_min'\n",
    "            ]\n",
    "for (fm1, fm2) in tqdm(combi_list):\n",
    "    for col in comp_cols:\n",
    "        try:\n",
    "            train_test[f\"ratio_{fm1}_{fm2}_{col}\"] = train_test[f\"{fm1}_{col}\"] / train_test[f\"{fm2}_{col}\"]\n",
    "            train_test[f\"diff_{fm1}_{fm2}_{col}\"] = train_test[f\"{fm1}_{col}\"] - train_test[f\"{fm2}_{col}\"]\n",
    "        except KeyError:\n",
    "            print(fm1, fm2, col)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-05-01    167857\n",
       "2018-04-01     55349\n",
       "2018-03-01     16554\n",
       "2018-02-01     11655\n",
       "2018-01-01      8537\n",
       "2017-02-01      6396\n",
       "2017-01-01      5411\n",
       "2017-10-01      4657\n",
       "2017-09-01      4014\n",
       "2017-08-01      3602\n",
       "2017-07-01      3069\n",
       "2017-06-01      2441\n",
       "2017-05-01       434\n",
       "2017-04-01        25\n",
       "Name: new_purchase_month_max, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "    train_test['new_CLV'] = train_test['new_card_id_count'] * train_test['new_purchase_amount_sum'] / train_test['new_month_diff_mean']\n",
    "    train_test['hist_CLV'] = train_test['hist_card_id_count'] * train_test['hist_purchase_amount_sum'] / train_test['hist_month_diff_mean']\n",
    "    train_test['CLV_ratio'] = train_test['new_CLV'] / train_test['hist_CLV']\n",
    "    \n",
    "    #========================================================================\n",
    "    # Save Feature\n",
    "    ignore_features = ['first_active_month', 'card_id', target]\n",
    "    for col in train_test.columns:\n",
    "        if col in ignore_features: continue\n",
    "        if not(col.count('diff')):continue\n",
    "        feature = train_test[col].astype('float32').values\n",
    "        col = col.replace('.', '_')\n",
    "    #     utils.to_pkl_gzip(path = f'../features/1_first_valid/{prefix}_{col}@', obj=feature)\n",
    "        utils.to_pkl_gzip(path = f'../features/3_third_valid/{prefix}_{col}@', obj=feature)\n",
    "    #========================================================================\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Feature\n",
    "# train_test = train_test.join(hist_max_date).join(hist_min_date).join(new_max_date).join(new_min_date)\n",
    "train_test['hist_purchase_month_max'] = pd.to_datetime(train_test['hist_purchase_month_max'])\n",
    "train_test['hist_purchase_month_min'] = pd.to_datetime(train_test['hist_purchase_month_min'])\n",
    "train_test['new_purchase_month_max'] = pd.to_datetime(train_test['new_purchase_month_max'])\n",
    "train_test['new_purchase_month_min'] = pd.to_datetime(train_test['new_purchase_month_min'])\n",
    "train_test.head()\n",
    "\n",
    "# 静的なFeature\n",
    "train_test['personal_term_new_min_hist_max'] = (train_test['new_purchase_month_min'] - train_test['hist_purchase_month_max']).dt.days\n",
    "train_test['personal_term_new_min_hist_min'] = (train_test['new_purchase_month_min'] - train_test['hist_purchase_month_min']).dt.days\n",
    "train_test['personal_days_new_max_date_from_hist_max_month'] = (train_test['new_purchase_date_max'] - train_test['hist_purchase_month_max']).dt.days\n",
    "train_test['personal_days_new_min_date_from_hist_max_month'] = (train_test['new_purchase_date_min'] - train_test['hist_purchase_month_max']).dt.days\n",
    "        \n",
    "prefix = '203_pst'\n",
    "ignore_features = ['first_active_month', 'card_id', target]\n",
    "\n",
    "for col in train_test.columns:\n",
    "    if col in ignore_features: continue\n",
    "    if not(col.count('personal')):continue\n",
    "    feature = train_test[col].astype('float32').values\n",
    "    \n",
    "#     utils.to_pkl_gzip(path = f'../features/1_first_valid/{prefix}_{col}@', obj=feature)\n",
    "    utils.to_pkl_gzip(path = f'../features/3_third_valid/{prefix}_{col}@', obj=feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
