{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 08:36:54,142 utils 400 [INFO]    [logger_func] start \n",
      "100%|██████████| 3/3 [00:00<00:00, 45.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "from sklearn.linear_model import Ridge\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "out_part = ['', 'part', 'all'][0]\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "stack_name='ridge'\n",
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "model_type='ridge'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "print(\"Preparing dataset...\")\n",
    "win_path = f'../features/4_winner/*.gz'\n",
    "# Ensemble 1\n",
    "win_path = f'../model/LB3670_70leaves_colsam0322/*.gz'\n",
    "# Ensemble 2\n",
    "# win_path = f'../model/E2_lift_set/*.gz'\n",
    "# Ensemble 3\n",
    "# win_path = f'../model/E3_PCA_set/*.gz'\n",
    "\n",
    "win_path_list = glob.glob(win_path)\n",
    "\n",
    "base = utils.read_df_pkl('../input/base_term*0*')[[key, target, 'first_active_month']]\n",
    "base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
    "df = pd.concat(feature_list, axis=1)\n",
    "train = pd.concat([base_train, df.iloc[:len(base_train), :]], axis=1)\n",
    "test = pd.concat([base_test, df.iloc[len(base_train):, :].reset_index(drop=True)], axis=1)\n",
    "\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True , drop=True)\n",
    "\n",
    "if debug:\n",
    "    train = train.head(10000)\n",
    "    test = test.head(2000)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# 正規化の前処理(Null埋め, inf, -infの処理) \n",
    "for col in train.columns:\n",
    "    if col in ignore_list: continue\n",
    "        \n",
    "    train[col] = impute_feature(train, col)\n",
    "    test[col] = impute_feature(test, col)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Cleansing Check\n",
    "def clean_check(df, col):\n",
    "#     if col in ignore_list: continue\n",
    "#     train[col] = impute_feature(train, col)\n",
    "#     test[col] = impute_feature(test, col)\n",
    "    length = len(df)\n",
    "    tmp = df[col].dropna().shape[0]\n",
    "    if length - tmp>0:\n",
    "        print(f\"Null is {length-tmp}\")\n",
    "        \n",
    "    inf_max = df[col].max()\n",
    "    inf_min = df[col].min()\n",
    "    inf_max_2 = df[col].sort_values().values[-1]\n",
    "    inf_min_2 = df[col].sort_values().values[0]\n",
    "    if inf_max==np.inf or inf_min==-np.inf:\n",
    "        print(1, col, inf_max, inf_min)\n",
    "    if inf_max_2==np.inf or inf_min_2==-np.inf:\n",
    "        print(2, col, inf_max, inf_min)\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (201917, 238) | Test: (123623, 238)\n",
      "[ 0.99697211  0.78249097  0.14174135 -1.34486152  0.8852682 ]\n",
      "[-1.02641299 -0.04536111 -1.84705051  0.10180674 -1.86672799]\n",
      "RMSE: 4.011442136729887 | SUM ERROR: 100.5750275942226\n",
      "[-0.03257652 -1.17443632 -1.06043177 -0.53119587  0.03396996]\n",
      "[-1.05694313 -0.18659525 -2.02246871  0.12677646 -1.9036637 ]\n",
      "RMSE: 3.761707809324424 | SUM ERROR: 169.26633058000203\n",
      "[ 0.09131669  0.75997011 -3.17174433 -0.37193898  0.0429754 ]\n",
      "[-1.12916361 -0.09940926 -1.8020578   0.15182906 -1.8671233 ]\n",
      "RMSE: 3.745846640199936 | SUM ERROR: 33.42384167673712\n",
      "[-0.6414107  -1.11844584  0.60120408 -2.70641091 -0.84972024]\n",
      "[-0.88476002 -0.10663662 -1.78115175  0.09870754 -1.94257072]\n",
      "RMSE: 3.751276391400474 | SUM ERROR: 128.90535382813457\n",
      "[0.19849595 0.19404899 0.53667933 0.02031036 0.73416636]\n",
      "[-0.9767019  -0.22361714 -1.94315387  0.11037092 -2.00735908]\n",
      "RMSE: 3.746859865379433 | SUM ERROR: -393.1466454453067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 08:39:41,935 utils 104 [INFO]    [<module>] \n",
      "#========================================================================\n",
      "# CV SCORE AVG: 3.796164433554916\n",
      "#======================================================================== \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16192962 -0.28737432  0.04328056 -0.09242657 -0.21318359]\n",
      "[-1.07598326 -0.13462438 -1.67160833  0.21337728 -1.85160505]\n",
      "RMSE: 3.7598537582953435 | SUM ERROR: 175.00139290675273\n",
      "Stacking Shape: (325540, 4)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# ods.ai 3rd kernel\n",
    "# https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/78903\n",
    "# KFold: n_splits=6(or 7)!, shuffle=False!\n",
    "# train['rounded_target'] = train['target'].round(0)\n",
    "# train = train.sort_values('rounded_target').reset_index(drop=True)\n",
    "# vc = train['rounded_target'].value_counts()\n",
    "# vc = dict(sorted(vc.items()))\n",
    "# df = pd.DataFrame()\n",
    "# train['indexcol'],idx = 0,1\n",
    "# for k,v in vc.items():\n",
    "#     step = train.shape[0]/v\n",
    "#     indent = train.shape[0]/(v+1)\n",
    "#     df2 = train[train['rounded_target'] == k].sample(v, random_state=seed).reset_index(drop=True)\n",
    "#     for j in range(0, v):\n",
    "#         df2.at[j, 'indexcol'] = indent + j*step + 0.000001*idx\n",
    "#     df = pd.concat([df2,df])\n",
    "#     idx+=1\n",
    "# train = df.sort_values('indexcol', ascending=True).reset_index(drop=True)\n",
    "# del train['indexcol'], train['rounded_target']\n",
    "# fold_type = 'self'\n",
    "# fold = 6\n",
    "# folds = KFold(n_splits=fold, shuffle=False, random_state=seed)\n",
    "# kfold = list(folds.split(train, train[target].values))\n",
    "# utils.to_pkl_gzip(obj=kfold, path='../input/ods_kfold')\n",
    "kfold = utils.read_pkl_gzip('../input/ods_kfold.gz')\n",
    "# =======================================================================\n",
    "\n",
    "#========================================================================\n",
    "# CVの準備\n",
    "model_list = []\n",
    "result_list = []\n",
    "score_list = []\n",
    "val_pred_list = []\n",
    "test_pred = np.zeros(len(test))\n",
    "\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([train[use_cols], test[use_cols]]))\n",
    "x_test = scaler.transform(test[use_cols])\n",
    "\n",
    "Y = train[target]\n",
    "y_mean = Y.mean()\n",
    "#========================================================================\n",
    "    \n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\")\n",
    "    \n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "fit_intercept = True\n",
    "alpha = 0.4\n",
    "max_iter = 1000\n",
    "normalize = False\n",
    "tol = 0.01\n",
    "ridge = Ridge(solver='auto', fit_intercept=fit_intercept, alpha=alpha, max_iter=max_iter, normalize=normalize, tol=tol)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "\n",
    "for fold_no, (trn_idx, val_idx) in enumerate(kfold):\n",
    "\n",
    "    #========================================================================\n",
    "    # Make Dataset\n",
    "#     X_train, X_val = train_test_split(train, test_size=0.2)\n",
    "    X_train, y_train = train.iloc[trn_idx, :][use_cols], Y.iloc[trn_idx]\n",
    "    X_val, y_val = train.iloc[val_idx, :][use_cols], Y.iloc[val_idx]\n",
    "    \n",
    "     \n",
    "    X_train[:] = scaler.transform(X_train)\n",
    "    X_val[:] = scaler.transform(X_val)\n",
    "    X_train = X_train.as_matrix()\n",
    "    X_val = X_val.as_matrix()\n",
    "    #========================================================================\n",
    "    \n",
    "    # Fitting\n",
    "    ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    # Prediction\n",
    "    y_pred = ridge.predict(X_val)\n",
    "#     y_pred = y_pred.reshape(y_pred.shape[0], )\n",
    "    test_pred = ridge.predict(x_test)\n",
    "#     test_pred += tmp_pred.reshape(tmp_pred.shape[0], )\n",
    "    \n",
    "    # Stack Prediction\n",
    "    df_pred = train.iloc[val_idx, :][[key, target]].copy()\n",
    "    df_pred['prediction'] = y_pred\n",
    "    result_list.append(df_pred)\n",
    "    \n",
    "    # Scoring\n",
    "    err = (y_val - y_pred)\n",
    "    score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "    score_list.append(score)\n",
    "    #========================================================================\n",
    "\n",
    "cv_score = np.mean(score_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "#========================================================================\n",
    "# Stacking\n",
    "test_pred /= fold\n",
    "test['prediction'] = test_pred\n",
    "stack_test = test[[key, 'prediction']]\n",
    "result_list.append(stack_test)\n",
    "df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "df_pred = base.merge(df_pred, how='inner', on=key)\n",
    "print(f\"Stacking Shape: {df_pred.shape}\")\n",
    "\n",
    "# Save Stack\n",
    "utils.to_pkl_gzip(path=f\"../stack/{start_time[4:12]}_stack_{model_type}_alpha{alpha}_{len(use_cols)}feats_{len(seed_list)}seed_tol{tol}_iter{max_iter}_OUT{str(out_score)[:7]}_CV{str(cv_score).replace('.', '-')}_LB\"\n",
    "                          , obj=df_pred)\n",
    "#========================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
