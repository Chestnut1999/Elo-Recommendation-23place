{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 45.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature\n",
    "import time\n",
    "import glob\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "    \n",
    "    \n",
    "#========================================================================\n",
    "# Args \n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'column_0', 'index', 'first_active_month', 'no_out_flg']\n",
    "#========================================================================\n",
    "\n",
    "print(\"Preparing dataset...\")\n",
    "#========================================================================\n",
    "# Data Load \n",
    "win_path = f'../features/4_winner/*.gz'\n",
    "win_path = f'../model/LB3670_70leaves_colsam0322/*.gz'\n",
    "# win_path_list = glob.glob(win_path) + glob.glob('../features/5_tmp/*.gz')\n",
    "win_path_list = glob.glob(win_path)\n",
    "\n",
    "base = utils.read_df_pkl('../input/base_term*0*')[[key, target, 'first_active_month']]\n",
    "base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
    "df = pd.concat(feature_list, axis=1)\n",
    "train = pd.concat([base_train, df.iloc[:len(base_train), :]], axis=1)\n",
    "test = pd.concat([base_test, df.iloc[len(base_train):, :].reset_index(drop=True)], axis=1)\n",
    "#========================================================================\n",
    "\n",
    "def impute_feature(df, col):\n",
    "\n",
    "    feature = df[col].values.astype('float32')\n",
    "\n",
    "    inf_max = np.sort(feature)[::-1][0]\n",
    "    inf_min = np.sort(feature)[0]\n",
    "    \n",
    "    if inf_max == np.inf:\n",
    "        v_max = np.max(np.where(feature==inf_max, np.median(feature), feature))\n",
    "        feature = np.where(feature==inf_max, v_max, feature)\n",
    "    if inf_min == -np.inf:\n",
    "        v_min = np.min(np.where(feature==inf_min, np.median(feature), feature))\n",
    "        feature = np.where(feature==inf_min, v_min, feature)\n",
    "\n",
    "    length = len(feature)\n",
    "    null_len = len(feature[feature==feature])\n",
    "    \n",
    "    inf_max = feature.max()\n",
    "    inf_min = feature.min()\n",
    "    \n",
    "    if length - null_len==0:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        if col.count('month_diff'):\n",
    "            \n",
    "            for val_max in np.sort(feature)[::-1]:\n",
    "                if not(val_max==val_max):\n",
    "                    continue\n",
    "                feature = np.where(feature!=feature, val_max-1, feature)\n",
    "                break\n",
    "        else:\n",
    "            for val_min in np.sort(feature):\n",
    "                if not(val_min==val_min):\n",
    "                    continue\n",
    "                feature = np.where(feature!=feature, val_min-1, feature)\n",
    "                break\n",
    "            \n",
    "    if str(type(feature)).count('series'):\n",
    "        feature = feature.values\n",
    "    return feature\n",
    "\n",
    "\n",
    "# tmp_train = train.copy()\n",
    "# tmp_test = test.copy()\n",
    "#========================================================================\n",
    "# 正規化の前処理(Null埋め, inf, -infの処理) \n",
    "for col in train.columns:\n",
    "    if col in ignore_list: continue\n",
    "        \n",
    "    train[col] = impute_feature(train, col)\n",
    "    test[col] = impute_feature(test, col)\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(train)\n",
    "for col in train.columns:\n",
    "    tmp = train[col].dropna().shape[0]\n",
    "    if length - tmp>0:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 161533 samples, validate on 40384 samples\n",
      "Epoch 1/10\n",
      " - 26s - loss: 1.6302 - RMSE: 1.6302 - val_loss: 1.4485 - val_RMSE: 1.4485\n",
      "Epoch 2/10\n",
      " - 23s - loss: 1.5230 - RMSE: 1.5230 - val_loss: 1.4287 - val_RMSE: 1.4287\n",
      "Epoch 3/10\n",
      " - 23s - loss: 1.5012 - RMSE: 1.5012 - val_loss: 1.4206 - val_RMSE: 1.4206\n",
      "Epoch 4/10\n",
      " - 23s - loss: 1.4906 - RMSE: 1.4906 - val_loss: 1.4171 - val_RMSE: 1.4171\n",
      "Epoch 5/10\n",
      " - 23s - loss: 1.4811 - RMSE: 1.4811 - val_loss: 1.4138 - val_RMSE: 1.4138\n",
      "Epoch 6/10\n",
      " - 23s - loss: 1.4754 - RMSE: 1.4754 - val_loss: 1.4129 - val_RMSE: 1.4129\n",
      "Epoch 7/10\n",
      " - 23s - loss: 1.4711 - RMSE: 1.4711 - val_loss: 1.4107 - val_RMSE: 1.4107\n",
      "Epoch 8/10\n",
      " - 23s - loss: 1.4674 - RMSE: 1.4674 - val_loss: 1.4099 - val_RMSE: 1.4099\n",
      "Epoch 9/10\n",
      " - 23s - loss: 1.4634 - RMSE: 1.4634 - val_loss: 1.4103 - val_RMSE: 1.4103\n",
      "Epoch 10/10\n",
      " - 23s - loss: 1.4606 - RMSE: 1.4606 - val_loss: 1.4100 - val_RMSE: 1.4100\n",
      "RMSE: 3.6566551594853247 | SUM: -16097.1688223397\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1c9dde4a754c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m#========================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pkl_gzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_pred_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'../output/{start_time[4:11]}_elo_NN_stack_CV{score}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# Make Dataset\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "X_train, X_val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "df_pred = X_val[[key, target]].copy()\n",
    "\n",
    "y_train = X_train[target].values\n",
    "X_train = X_train[use_cols]\n",
    "test = test[use_cols]\n",
    "y_val = X_val[target].values\n",
    "X_val = X_val[use_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([X_train, X_val, test]))\n",
    "X_train[:] = scaler.transform(X_train)\n",
    "X_val[:] = scaler.transform(X_val)\n",
    "test[:] = scaler.transform(test)\n",
    "\n",
    "X_train = X_train.as_matrix()\n",
    "test = test.as_matrix()\n",
    "X_val = X_val.as_matrix()\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "test = test.reshape((test.shape[0], 1, test.shape[1]))\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# CVの準備\n",
    "pred_list = []\n",
    "score_list = []\n",
    "#========================================================================\n",
    "#========================================================================\n",
    "# Corporación Favorita Grocery Sales Forecasting 1st Place NN Model\n",
    "# https://www.kaggle.com/shixw125/1st-place-nn-model-public-0-507-private-0-513\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.1))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.1))\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.05))\n",
    "\n",
    "    # test\n",
    "#     model.add(LSTM(64, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "#     model.add(Dense(64))\n",
    "    \n",
    "    # original\n",
    "    model.add(Dense(64))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.05))\n",
    "\n",
    "    model.add(Dense(32))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.05))\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.05))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def basic_build():\n",
    "    models = Sequential()\n",
    "    models.add(Dense(output_dim=1024, input_dim=input_d, init='lecun_uniform')) \n",
    "    models.add(Activation('relu')) \n",
    "    models.add(BatchNormalization())    \n",
    "    models.add(Dropout(0.5))  \n",
    "    models.add(Dense(512, activation='relu',init='lecun_uniform'))\n",
    "    models.add(Activation('relu')) \n",
    "    models.add(BatchNormalization())    \n",
    "    models.add(Dropout(0.4))  \n",
    "    models.add(Dense(2, init='lecun_uniform'))\n",
    "    models.add(Activation('softmax'))    \n",
    "    opt = optimizers.Adam(lr=learning_rate)\n",
    "    models.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "# Check\n",
    "# X_train = X_train[:2000]\n",
    "# y_train = y_train[:2000]\n",
    "# X_val = X_val[:200]\n",
    "# y_val = y_val[:200]\n",
    "\n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "# N_EPOCHS = 2000\n",
    "N_EPOCHS = 10\n",
    "# batch_size = 65536\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "learning_rate = 0.0001\n",
    "\n",
    "val_pred_list = []\n",
    "test_pred_list = []\n",
    "\n",
    "y = y_train\n",
    "y_mean = y.mean()\n",
    "model = build_model()\n",
    "opt = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss=RMSE, optimizer=opt, metrics=[RMSE])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "    ]\n",
    "# なぜか平均を引いてる？そのほうがfitするの？\n",
    "# model.fit(X_train, y- y_mean, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "#            validation_data=(X_val, y_val - y_mean), callbacks=callbacks )\n",
    "model.fit(X_train, y, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "           validation_data=(X_val, y_val), callbacks=callbacks )\n",
    "val_pred_list.append(model.predict(X_val))\n",
    "test_pred_list.append(model.predict(test))\n",
    "\n",
    "y_pred = np.array(val_pred_list).reshape(np.array(val_pred_list).shape[1], )\n",
    "df_pred['prediction'] = y_pred\n",
    "pred_list.append(df_pred)\n",
    "\n",
    "err = (y_val - y_pred)\n",
    "score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f'RMSE: {score} | SUM: {err.sum()}')\n",
    "score_list.append(score)\n",
    "cv_score = np.mean(score_list)\n",
    "#========================================================================\n",
    "\n",
    "utils.to_pkl_gzip(obj=test_pred_list, path=f'../output/{start_time[4:11]}_elo_NN_stack_CV{score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
