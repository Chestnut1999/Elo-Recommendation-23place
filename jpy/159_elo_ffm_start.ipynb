{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFM\n",
    "FFM用のDatasetを作成し、kerasで実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 12:44:07,665 utils 366 [INFO]    [logger_func] start \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "#========================================================================\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month']\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "import utils\n",
    "from preprocessing import get_ordinal_mapping, get_dummies, outlier\n",
    "from utils import logger_func\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from keras.layers import Input, Embedding, Dense,Flatten, Activation, dot, add\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2 as l2_reg\n",
    "from keras import initializers\n",
    "import itertools\n",
    "# import keras.utils.multi_gpu_model\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "gpu_count = 1\n",
    "gpu_count = 8\n",
    "batch_size = 128 * gpu_count\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "\n",
    "def batch_generator(X,y,batch_size=128,shuffle=True):\n",
    "    sample_size = X[0].shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_array)\n",
    "        batches = make_batches(sample_size, batch_size)\n",
    "        for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "            batch_ids = index_array[batch_start:batch_end]\n",
    "            X_batch = [X[i][batch_ids] for i in range(len(X))]\n",
    "            y_batch = y[batch_ids]\n",
    "            yield X_batch,y_batch\n",
    "\n",
    "\n",
    "def test_batch_generator(X,y,batch_size=128):\n",
    "    sample_size = X[0].shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    batches = make_batches(sample_size, batch_size)\n",
    "    for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "        batch_ids = index_array[batch_start:batch_end]\n",
    "        X_batch = [X[i][batch_ids] for i in range(len(X))]\n",
    "        y_batch = y[batch_ids]\n",
    "        yield X_batch,y_batch\n",
    "\n",
    "\n",
    "def predict_batch(model,X_t,batch_size=128):\n",
    "    outcome = []\n",
    "    for X_batch,y_batch in test_batch_generator(X_t,np.zeros(X_t[0].shape[0]),batch_size=batch_size):\n",
    "        outcome.append(model.predict(X_batch,batch_size=batch_size))\n",
    "    outcome = np.concatenate(outcome).ravel()\n",
    "    return outcome\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_len, max_features,K=8,solver='adam',l2=0.0,l2_fm = 0.0):\n",
    "\n",
    "    inputs = []\n",
    "    flatten_layers=[]\n",
    "    columns = range(len(max_features))\n",
    "    for c in columns:\n",
    "        inputs_c = Input(shape=(1,), dtype='int32',name = 'input_%s'%c)\n",
    "        num_c = max_features[c]\n",
    "\n",
    "        embed_c = Embedding(\n",
    "                        input_dim=num_c, # 埋め込む特徴の次元\n",
    "                        output_dim=K, # 何次元に埋め込むか\n",
    "                        input_length=1,\n",
    "#                         input_length=1,\n",
    "                        name = 'embed_%s'%c,\n",
    "                        W_regularizer=l2_reg(l2_fm)\n",
    "                        )(inputs_c)\n",
    "\n",
    "              \n",
    "        flatten_c = Flatten()(embed_c)\n",
    "\n",
    "        inputs.append(inputs_c)\n",
    "        flatten_layers.append(flatten_c)\n",
    "\n",
    "    fm_layers = []\n",
    "\n",
    "    for emb1,emb2 in itertools.combinations(flatten_layers, 2):\n",
    "        \n",
    "#         dot_layer = merge([emb1,emb2], mode='dot', dot_axes=1)\n",
    "        dot_layer = dot(inputs=[emb1, emb2], axes=1)\n",
    "        \n",
    "        fm_layers.append(dot_layer)\n",
    "\n",
    "        \n",
    "    for c in columns:\n",
    "        num_c = max_features[c]\n",
    "        \n",
    "        embed_c = Embedding(\n",
    "                        num_c,\n",
    "                        1,\n",
    "                        input_length=1,\n",
    "#                         input_length=input_len,\n",
    "                        name = 'linear_%s'%c,\n",
    "                        W_regularizer=l2_reg(l2)\n",
    "                        )(inputs[c])\n",
    "\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "\n",
    "        fm_layers.append(flatten_c)\n",
    "        \n",
    "#     flatten = merge(fm_layers, mode='sum')\n",
    "    flatten = add(fm_layers) \n",
    "    outputs = Activation('sigmoid',name='outputs')(flatten)\n",
    "    \n",
    "    model = Model(input=inputs, output=outputs)\n",
    "    \n",
    "    if gpu_count>1:\n",
    "        model = multi_gpu_model(model, gpus=gpu_count) # add\n",
    "\n",
    "    model.compile(\n",
    "                optimizer=solver,\n",
    "                loss= 'binary_crossentropy'\n",
    "              )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class KerasFM(BaseEstimator):\n",
    "    def __init__(self, input_len, max_features=[], K=8, solver='adam', l2=0.0, l2_fm=0.0):\n",
    "        self.model = build_model(input_len, max_features,K,solver,l2=l2,l2_fm = l2_fm)\n",
    "\n",
    "    def fit(self, X, y, batch_size=128, nb_epoch=10, shuffle=True, verbose=1, validation_data=None):\n",
    "        self.model.fit(X,y,batch_size=batch_size,nb_epoch=nb_epoch,shuffle=shuffle,verbose=verbose,validation_data=None)\n",
    "\n",
    "    def fit_generator(self,X,y,batch_size=128,nb_epoch=10,shuffle=True,verbose=1,validation_data=None,callbacks=None):\n",
    "        tr_gen = batch_generator(X,y,batch_size=batch_size,shuffle=shuffle)\n",
    "        if validation_data:\n",
    "            X_test,y_test = validation_data\n",
    "            te_gen = batch_generator(X_test,y_test,batch_size=batch_size,shuffle=False)\n",
    "            nb_val_samples = X_test[-1].shape[0]\n",
    "        else:\n",
    "            te_gen = None\n",
    "            nb_val_samples = None\n",
    "\n",
    "        self.model.fit_generator(\n",
    "                tr_gen, \n",
    "                samples_per_epoch=X[-1].shape[0], \n",
    "                nb_epoch=nb_epoch, \n",
    "                verbose=verbose, \n",
    "                callbacks=callbacks, \n",
    "                validation_data=te_gen, \n",
    "                nb_val_samples=nb_val_samples, \n",
    "                max_q_size=10\n",
    "                )\n",
    "\n",
    "    def predict(self,X,batch_size=128):\n",
    "        y_preds = predict_batch(self.model,X,batch_size=batch_size)\n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 66.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 8) (123623, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#========================================================================\n",
    "# Data Load\n",
    "base = utils.read_df_pkl('../input/base*')\n",
    "win_path = f'../features/ffm_winner/*.gz'\n",
    "win_path_list = glob.glob(win_path)\n",
    "train_path_list = []\n",
    "test_path_list = []\n",
    "for path in win_path_list:\n",
    "    if path.count('train'):\n",
    "        train_path_list.append(path)\n",
    "    elif path.count('test'):\n",
    "        test_path_list.append(path)\n",
    "\n",
    "train_path_list = sorted(train_path_list)[:20]\n",
    "test_path_list  = sorted(test_path_list)[:20]\n",
    "        \n",
    "base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "train_feature_list = utils.parallel_load_data(path_list=train_path_list)\n",
    "test_feature_list = utils.parallel_load_data(path_list=test_path_list)\n",
    "train = pd.concat(train_feature_list, axis=1)\n",
    "train = pd.concat([base_train, train], axis=1)\n",
    "test = pd.concat(test_feature_list, axis=1)\n",
    "test = pd.concat([base_test, test], axis=1)\n",
    "train.set_index(key, inplace=True)\n",
    "test.set_index(key, inplace=True)\n",
    "\n",
    "train.fillna(train.median(), inplace=True)\n",
    "test.fillna(test.median(), inplace=True)\n",
    "\n",
    "num_list = [col for col in train.columns if (str(train[col].dtype).count('int') or \n",
    "                                             str(train[col].dtype).count('float')) and \n",
    "            col != target and not(col.count('amount'))]\n",
    "y = train[target]\n",
    "\n",
    "train = train[num_list]\n",
    "test = test[num_list]\n",
    "\n",
    "# amount_list = [col for col in train.columns if col.count('amount')]\n",
    "# train[amount_list] = train[amount_list].where(train[amount_list]<2, 2)\n",
    "# test[amount_list] = test[amount_list].where(train[amount_list]<2, 2)\n",
    "\n",
    "# arg_list = list(set(num_list) - set(amount_list))\n",
    "\n",
    "for col in tqdm(num_list):\n",
    "    train = outlier(df=train, col=col, replace_inner=True)\n",
    "    test = outlier(df=test, col=col, replace_inner=True)\n",
    "\n",
    "# def cleansing_outlier(col):\n",
    "#      = outlier(df=train[col].to_frame(), col=col, replace_inner=True)\n",
    "#     return train[col], test\n",
    "# from joblib import Parallel, delayed\n",
    "# p_list = Parallel(n_jobs=-1)([delayed(cleasing_outlier)(args) for args in arg_list)\n",
    "    \n",
    "train_test = pd.concat([train[num_list], test[num_list]], axis=0)\n",
    "scaler = MinMaxScaler()\n",
    "columns = train_test.columns\n",
    "train_test = scaler.fit_transform(train_test)\n",
    "train_test = pd.DataFrame(train_test, columns=columns)\n",
    "train = train_test.iloc[:len(train), :]\n",
    "test = train_test.iloc[len(train):, :]\n",
    "print(train.shape, test.shape)\n",
    "del train_test\n",
    "gc.collect()\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15999, 8) (15999, 1) (4001, 8) (4001, 1)\n",
      "8 8 8\n",
      "Epoch 1/5\n",
      "15999/15999 [==============================] - 38s 2ms/step - loss: 0.6593\n",
      "Epoch 2/5\n",
      "15999/15999 [==============================] - 0s 28us/step - loss: 0.5872\n",
      "Epoch 3/5\n",
      "15999/15999 [==============================] - 0s 29us/step - loss: 0.5158\n",
      "Epoch 4/5\n",
      "15999/15999 [==============================] - 0s 28us/step - loss: 0.4459\n",
      "Epoch 5/5\n",
      "15999/15999 [==============================] - 0s 28us/step - loss: 0.3790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 13:10:37,249 utils 63 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 0 SCORE: 0.5\n",
      "    #======================================================================== \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15999, 8) (15999, 1) (4001, 8) (4001, 1)\n",
      "8 8 8\n",
      "Epoch 1/5\n",
      "15999/15999 [==============================] - 41s 3ms/step - loss: 0.6707\n",
      "Epoch 2/5\n",
      "15999/15999 [==============================] - 0s 28us/step - loss: 0.6019\n",
      "Epoch 3/5\n",
      "15999/15999 [==============================] - 0s 28us/step - loss: 0.5337\n",
      "Epoch 4/5\n",
      "15999/15999 [==============================] - 0s 28us/step - loss: 0.4666\n",
      "Epoch 5/5\n",
      "15999/15999 [==============================] - 0s 28us/step - loss: 0.4014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 13:12:16,496 utils 63 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 1 SCORE: 0.5\n",
      "    #======================================================================== \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 8) (16000, 1) (4000, 8) (4000, 1)\n",
      "8 8 8\n",
      "Epoch 1/5\n",
      "16000/16000 [==============================] - 45s 3ms/step - loss: 0.6037\n",
      "Epoch 2/5\n",
      "16000/16000 [==============================] - 0s 29us/step - loss: 0.5380\n",
      "Epoch 3/5\n",
      "16000/16000 [==============================] - 0s 29us/step - loss: 0.4730\n",
      "Epoch 4/5\n",
      "16000/16000 [==============================] - 0s 29us/step - loss: 0.4096\n",
      "Epoch 5/5\n",
      "16000/16000 [==============================] - 0s 29us/step - loss: 0.3490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 13:14:05,403 utils 63 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 2 SCORE: 0.5\n",
      "    #======================================================================== \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16001, 8) (16001, 1) (3999, 8) (3999, 1)\n",
      "8 8 8\n",
      "Epoch 1/5\n",
      "16001/16001 [==============================] - 49s 3ms/step - loss: 0.6245\n",
      "Epoch 2/5\n",
      "16001/16001 [==============================] - 0s 29us/step - loss: 0.5564\n",
      "Epoch 3/5\n",
      "16001/16001 [==============================] - 0s 29us/step - loss: 0.4897\n",
      "Epoch 4/5\n",
      "16001/16001 [==============================] - 0s 30us/step - loss: 0.4253\n",
      "Epoch 5/5\n",
      "16001/16001 [==============================] - 0s 29us/step - loss: 0.3640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 13:16:02,007 utils 63 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 3 SCORE: 0.5\n",
      "    #======================================================================== \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16001, 8) (16001, 1) (3999, 8) (3999, 1)\n",
      "8 8 8\n",
      "Epoch 1/5\n",
      "16001/16001 [==============================] - 56s 4ms/step - loss: 0.7269\n",
      "Epoch 2/5\n",
      "16001/16001 [==============================] - 0s 29us/step - loss: 0.6509\n",
      "Epoch 3/5\n",
      "16001/16001 [==============================] - 0s 29us/step - loss: 0.5770\n",
      "Epoch 4/5\n",
      "16001/16001 [==============================] - 0s 29us/step - loss: 0.5044\n",
      "Epoch 5/5\n",
      "16001/16001 [==============================] - 0s 29us/step - loss: 0.4339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 13:18:16,397 utils 63 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 4 SCORE: 0.5\n",
      "    #======================================================================== \n",
      "2019-01-12 13:18:16,398 utils 71 [INFO]    [<module>] \n",
      "#========================================================================\n",
      "# CV SCORE: 0.5\n",
      "#======================================================================== \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-461fc66d4042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mdf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pkl_gzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"../stack/{start_time[4:12]}_stack_{model_type}_lr{learning_rate}_{len(x_train.columns)}feats_{len(seed_list)}seed_{batch_size/gpu_count}batch_OUT_CV{str(cv_score).replace('.', '-')}_LB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "train[target] = y.head(20000).values\n",
    "train = train.head(20000)\n",
    "test = test.head(5000)\n",
    "\n",
    "cv_list = []\n",
    "seed_list = [1208]\n",
    "seed = seed_list[0]\n",
    "epoch = 5\n",
    "model_type = 'keras'\n",
    "learning_rate = 0\n",
    "\n",
    "pred_list = np.zeros(len(test))\n",
    "train[target] = y.values\n",
    "train[target] = train[target].map(lambda x: 1 if x<-30 else 0)\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "kfold = list(folds.split(train, train[target].values))\n",
    "# train.drop('outliers', axis=1, inplace=True)\n",
    "y = train[target]\n",
    "\n",
    "# Test Set\n",
    "len_test = len(test)\n",
    "len_feats = len(num_list)\n",
    "x_test = test.values.reshape(len_test, len_feats)\n",
    "x_test = [i for i in x_test.T]\n",
    "\n",
    "prediction = np.zeros(len_test)\n",
    "stack_prediction = np.zeros(len(train))\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(kfold):\n",
    "    tmp_train, y_train = train[num_list].iloc[trn_idx, :], y.iloc[trn_idx]\n",
    "    x_val, y_val = train[num_list].iloc[val_idx, :], y.iloc[val_idx]\n",
    "    len_train = len(tmp_train)\n",
    "    len_valid = len(x_val)\n",
    "\n",
    "    x_train = tmp_train.values.reshape(len_train, len_feats)\n",
    "    y_train = y_train.values.reshape(len_train, 1)\n",
    "    x_val = x_val.values.reshape(len_valid, len_feats)\n",
    "    y_val = y_val.values.reshape(len_valid, 1)\n",
    "    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n",
    "\n",
    "    x_train = [i for i in x_train.T]\n",
    "    x_val = [i for i in x_val.T]\n",
    "    print(len(x_train),len(x_val), len(x_test))\n",
    "    \n",
    "    max_features = [len(tmp_train[col]) for col in num_list]\n",
    "    model = KerasFM(input_len=len(tmp_train), max_features=max_features)\n",
    "    \n",
    "    model.fit(X=x_train, y=y_train, validation_data=(x_val, y_val), batch_size=batch_size, nb_epoch=epoch)\n",
    "    \n",
    "    test_pred = model.predict(X=x_test, batch_size=batch_size)\n",
    "    prediction += test_pred\n",
    "    \n",
    "    y_pred = model.predict(X=x_val, batch_size=batch_size)\n",
    "    stack_prediction[val_idx] = y_pred\n",
    "    \n",
    "    sc_score = roc_auc_score(y_val.reshape(len(y_val),), y_pred)\n",
    "    logger.info(f'''\n",
    "    #========================================================================\n",
    "    # FOLD {n_fold} SCORE: {sc_score}\n",
    "    #========================================================================''')\n",
    "    cv_list.append(sc_score)\n",
    "    \n",
    "prediction /= len(kfold)\n",
    "cv_score = np.mean(cv_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "train_pred = pd.Series(stack_prediction, name='prediction').to_frame()\n",
    "test_pred = pd.Series(prediction, name='prediction').to_frame()\n",
    "train_pred[key] = list(train.index)\n",
    "test_pred[key] = list(test.index)\n",
    "df_pred = pd.concat([train_pred, test_pred], axis=0)\n",
    "\n",
    "utils.to_pkl_gzip(path=f\"../stack/{start_time[4:12]}_stack_{model_type}_lr{learning_rate}_{len(num_list)}feats_{len(seed_list)}seed_{batch_size/gpu_count}batch_OUT_CV{str(cv_score).replace('.', '-')}_LB\", obj=df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
