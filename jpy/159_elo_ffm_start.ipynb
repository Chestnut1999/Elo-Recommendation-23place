{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFM\n",
    "FFM用のDatasetを作成し、kerasで実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 06:35:42,866 utils 366 [INFO]    [logger_func] start \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "#========================================================================\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month']\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "import utils\n",
    "from preprocessing import get_ordinal_mapping, get_dummies, outlier\n",
    "from utils import logger_func\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "gpu_count = 1\n",
    "gpu_count = 8\n",
    "batch_size = 128 * gpu_count\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from keras.layers import Input, Embedding, Dense,Flatten, Activation, dot, add\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2 as l2_reg\n",
    "from keras import initializers\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "if gpu_count>1:\n",
    "    from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "\n",
    "def batch_generator(X,y,batch_size=128,shuffle=True):\n",
    "    sample_size = X[0].shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_array)\n",
    "        batches = make_batches(sample_size, batch_size)\n",
    "        for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "            batch_ids = index_array[batch_start:batch_end]\n",
    "            X_batch = [X[i][batch_ids] for i in range(len(X))]\n",
    "            y_batch = y[batch_ids]\n",
    "            yield X_batch,y_batch\n",
    "\n",
    "\n",
    "def test_batch_generator(X,y,batch_size=128):\n",
    "    sample_size = X[0].shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    batches = make_batches(sample_size, batch_size)\n",
    "    for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "        batch_ids = index_array[batch_start:batch_end]\n",
    "        X_batch = [X[i][batch_ids] for i in range(len(X))]\n",
    "        y_batch = y[batch_ids]\n",
    "        yield X_batch,y_batch\n",
    "\n",
    "\n",
    "def predict_batch(model,X_t,batch_size=128):\n",
    "    outcome = []\n",
    "    for X_batch,y_batch in test_batch_generator(X_t,np.zeros(X_t[0].shape[0]),batch_size=batch_size):\n",
    "        outcome.append(model.predict(X_batch,batch_size=batch_size))\n",
    "    outcome = np.concatenate(outcome).ravel()\n",
    "    return outcome\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_len, max_features,K=8,solver='adam',l2=0.0,l2_fm = 0.0):\n",
    "\n",
    "    inputs = []\n",
    "    flatten_layers=[]\n",
    "    columns = list(range(len(max_features)))\n",
    "    \n",
    "    logger.info('First Embedding Start!')\n",
    "    for c in tqdm(columns):\n",
    "        inputs_c = Input(shape=(1,), dtype='int32',name = 'input_%s'%c)\n",
    "        inputs.append(inputs_c)\n",
    "        \n",
    "        num_c = max_features[c]\n",
    "\n",
    "        embed_c = Embedding(\n",
    "                        input_dim=num_c, # 埋め込む特徴の次元\n",
    "                        output_dim=K, # 何次元に埋め込むか\n",
    "                        input_length=1,\n",
    "#                         input_length=1,\n",
    "                        name = 'embed_%s'%c,\n",
    "                        W_regularizer=l2_reg(l2_fm)\n",
    "                        )(inputs_c)\n",
    "\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "        flatten_layers.append(flatten_c)\n",
    "    \n",
    "    fm_layers = []\n",
    "    logger.info('Feature Interaction Caliculate Start!')\n",
    "    for emb1,emb2 in tqdm(itertools.combinations(flatten_layers, 2)):\n",
    "\n",
    "        dot_layer = dot(inputs=[emb1, emb2], axes=1)\n",
    "        \n",
    "        fm_layers.append(dot_layer)\n",
    "        \n",
    "\n",
    "    logger.info('Second Embedding Start!')    \n",
    "    for c in tqdm(columns):\n",
    "        num_c = max_features[c]\n",
    "        \n",
    "        embed_c = Embedding(\n",
    "                        num_c,\n",
    "                        1,\n",
    "                        input_length=1,\n",
    "                        name = 'linear_%s'%c,\n",
    "                        W_regularizer=l2_reg(l2)\n",
    "                        )(inputs[c])\n",
    "\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "\n",
    "        fm_layers.append(flatten_c)\n",
    "        \n",
    "        \n",
    "    flatten = add(fm_layers) \n",
    "    logger.info('Model Activate...')\n",
    "    outputs = Activation('sigmoid',name='outputs')(flatten)\n",
    "    \n",
    "    model = Model(input=inputs, output=outputs)\n",
    "    \n",
    "    if gpu_count>1:\n",
    "        model = multi_gpu_model(model, gpus=gpu_count)\n",
    " \n",
    "    model.compile(\n",
    "                optimizer=solver,\n",
    "                loss= 'binary_crossentropy'\n",
    "              )\n",
    "    logger.info('Model Compile Complete!!')  \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class KerasFM(BaseEstimator):\n",
    "    def __init__(self, input_len, max_features=[], K=8, solver='adam', l2=0.0, l2_fm=0.0):\n",
    "        self.model = build_model(input_len, max_features,K,solver,l2=l2,l2_fm = l2_fm)\n",
    "\n",
    "    def fit(self, X, y, batch_size=128, nb_epoch=10, shuffle=True, verbose=1, validation_data=None):\n",
    "        self.model.fit(X,y,batch_size=batch_size,nb_epoch=nb_epoch,shuffle=shuffle,verbose=verbose,validation_data=None)\n",
    "\n",
    "    def fit_generator(self,X,y,batch_size=128,nb_epoch=10,shuffle=True,verbose=1,validation_data=None,callbacks=None):\n",
    "        tr_gen = batch_generator(X,y,batch_size=batch_size,shuffle=shuffle)\n",
    "        if validation_data:\n",
    "            X_test,y_test = validation_data\n",
    "            te_gen = batch_generator(X_test,y_test,batch_size=batch_size,shuffle=False)\n",
    "            nb_val_samples = X_test[-1].shape[0]\n",
    "        else:\n",
    "            te_gen = None\n",
    "            nb_val_samples = None\n",
    "\n",
    "        self.model.fit_generator(\n",
    "                tr_gen, \n",
    "                samples_per_epoch=X[-1].shape[0], \n",
    "                nb_epoch=nb_epoch, \n",
    "                verbose=verbose, \n",
    "                callbacks=callbacks, \n",
    "                validation_data=te_gen, \n",
    "                nb_val_samples=nb_val_samples, \n",
    "                max_q_size=10\n",
    "                )\n",
    "\n",
    "    def predict(self,X,batch_size=128):\n",
    "        y_preds = predict_batch(self.model,X,batch_size=batch_size)\n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 29.38it/s]\n",
      "2019-01-14 06:35:58,015 utils 59 [INFO]    [<module>] \n",
      "Transform MinMaxScaler \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 89) (123623, 89)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#========================================================================\n",
    "# Data Load\n",
    "base = utils.read_df_pkl('../input/base*')\n",
    "win_path = f'../features/ffm_winner/*.gz'\n",
    "win_path_list = glob.glob(win_path)\n",
    "train_path_list = []\n",
    "test_path_list = []\n",
    "for path in win_path_list:\n",
    "    if path.count('train'):\n",
    "        train_path_list.append(path)\n",
    "    elif path.count('test'):\n",
    "        test_path_list.append(path)\n",
    "\n",
    "# train_path_list = sorted(train_path_list)[:5]\n",
    "# test_path_list  = sorted(test_path_list)[:5]\n",
    "        \n",
    "base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "train_ids = base_train[key].values\n",
    "test_ids = base_test[key].values\n",
    "train_feature_list = utils.parallel_load_data(path_list=train_path_list)\n",
    "test_feature_list = utils.parallel_load_data(path_list=test_path_list)\n",
    "train = pd.concat(train_feature_list, axis=1)\n",
    "train = pd.concat([base_train, train], axis=1)\n",
    "test = pd.concat(test_feature_list, axis=1)\n",
    "test = pd.concat([base_test, test], axis=1)\n",
    "train.set_index(key, inplace=True)\n",
    "test.set_index(key, inplace=True)\n",
    "\n",
    "train.fillna(train.median(), inplace=True)\n",
    "test.fillna(test.median(), inplace=True)\n",
    "\n",
    "num_list = [col for col in train.columns if (str(train[col].dtype).count('int') or \n",
    "                                             str(train[col].dtype).count('float')) and \n",
    "            col != target and not(col.count('amount'))]\n",
    "y = train[target]\n",
    "\n",
    "train = train[num_list]\n",
    "test = test[num_list]\n",
    "\n",
    "# Cleansing\n",
    "# amount_list = [col for col in train.columns if col.count('amount')]\n",
    "# train[amount_list] = train[amount_list].where(train[amount_list]<2, 2)\n",
    "# test[amount_list] = test[amount_list].where(train[amount_list]<2, 2)\n",
    "# arg_list = list(set(num_list) - set(amount_list))\n",
    "# for col in tqdm(num_list):\n",
    "#     train = outlier(df=train, col=col, replace_inner=True)\n",
    "#     test = outlier(df=test, col=col, replace_inner=True)\n",
    "# def cleansing_outlier(col):\n",
    "#      = outlier(df=train[col].to_frame(), col=col, replace_inner=True)\n",
    "#     return train[col], test\n",
    "# from joblib import Parallel, delayed\n",
    "# p_list = Parallel(n_jobs=-1)([delayed(cleasing_outlier)(args) for args in arg_list)\n",
    "    \n",
    "train_test = pd.concat([train[num_list], test[num_list]], axis=0)\n",
    "logger.info(\"\\nTransform MinMaxScaler\")\n",
    "scaler = MinMaxScaler()\n",
    "columns = train_test.columns\n",
    "train_test = scaler.fit_transform(train_test)\n",
    "train_test = pd.DataFrame(train_test, columns=columns)\n",
    "train = train_test.iloc[:len(train), :]\n",
    "test = train_test.iloc[len(train):, :]\n",
    "\n",
    "del train_test\n",
    "gc.collect()\n",
    "\n",
    "train = train.astype('float32')\n",
    "test = test.astype('float32')\n",
    "print(train.shape, test.shape)\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 06:36:12,846 utils 61 [INFO]    [build_model] First Embedding Start! \n",
      "100%|██████████| 89/89 [00:01<00:00, 59.73it/s]\n",
      "2019-01-14 06:36:14,365 utils 81 [INFO]    [build_model] Feature Interaction Caliculate Start! \n",
      "3916it [00:13, 291.13it/s]\n",
      "2019-01-14 06:36:27,818 utils 89 [INFO]    [build_model] Second Embedding Start! \n",
      "100%|██████████| 89/89 [00:01<00:00, 64.21it/s]\n",
      "2019-01-14 06:36:31,950 utils 107 [INFO]    [build_model] Model Activate... \n",
      "2019-01-14 06:39:41,372 utils 119 [INFO]    [build_model] Model Compile Complete!! \n",
      "2019-01-14 06:39:41,373 utils 54 [INFO]    [<module>] Model Fitting Setup Start!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161533, 89) (161533, 1) (40384, 89) (40384, 1)\n",
      "89 89 89\n",
      "Epoch 1/6\n",
      "161533/161533 [==============================] - 2883s 18ms/step - loss: 0.1064\n",
      "Epoch 2/6\n",
      "161533/161533 [==============================] - 240s 1ms/step - loss: 0.0603\n",
      "Epoch 3/6\n",
      "161533/161533 [==============================] - 239s 1ms/step - loss: 0.0602\n",
      "Epoch 4/6\n",
      "161533/161533 [==============================] - 240s 1ms/step - loss: 0.0602\n",
      "Epoch 5/6\n",
      "161533/161533 [==============================] - 240s 1ms/step - loss: 0.0602\n",
      "Epoch 6/6\n",
      "161533/161533 [==============================] - 240s 1ms/step - loss: 0.0602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 08:09:04,278 utils 67 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 0 SCORE: 0.4999499273947224\n",
      "    #======================================================================== \n",
      "2019-01-14 08:09:04,350 utils 54 [INFO]    [<module>] Model Fitting Setup Start!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161533, 89) (161533, 1) (40384, 89) (40384, 1)\n",
      "89 89 89\n",
      "Epoch 1/6\n",
      "161533/161533 [==============================] - 239s 1ms/step - loss: 0.0602\n",
      "Epoch 2/6\n",
      "161533/161533 [==============================] - 239s 1ms/step - loss: 0.0602\n",
      "Epoch 3/6\n",
      "161533/161533 [==============================] - 239s 1ms/step - loss: 0.0602\n",
      "Epoch 4/6\n",
      "161533/161533 [==============================] - 240s 1ms/step - loss: 0.0602\n",
      "Epoch 5/6\n",
      "161533/161533 [==============================] - 240s 1ms/step - loss: 0.0602\n",
      "Epoch 6/6\n",
      "161533/161533 [==============================] - 239s 1ms/step - loss: 0.0602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 08:33:56,579 utils 67 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 1 SCORE: 0.5011062420600368\n",
      "    #======================================================================== \n",
      "2019-01-14 08:33:56,647 utils 54 [INFO]    [<module>] Model Fitting Setup Start!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161534, 89) (161534, 1) (40383, 89) (40383, 1)\n",
      "89 89 89\n",
      "Epoch 1/6\n",
      "161534/161534 [==============================] - 240s 1ms/step - loss: 0.0603\n",
      "Epoch 2/6\n",
      "161534/161534 [==============================] - 239s 1ms/step - loss: 0.0603\n",
      "Epoch 3/6\n",
      "161534/161534 [==============================] - 239s 1ms/step - loss: 0.0603\n",
      "Epoch 4/6\n",
      "161534/161534 [==============================] - 240s 1ms/step - loss: 0.0603\n",
      "Epoch 5/6\n",
      "161534/161534 [==============================] - 239s 1ms/step - loss: 0.0603\n",
      "Epoch 6/6\n",
      "161534/161534 [==============================] - 240s 1ms/step - loss: 0.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 08:58:53,100 utils 67 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 2 SCORE: 0.500062590756597\n",
      "    #======================================================================== \n",
      "2019-01-14 08:58:53,167 utils 54 [INFO]    [<module>] Model Fitting Setup Start!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161534, 89) (161534, 1) (40383, 89) (40383, 1)\n",
      "89 89 89\n",
      "Epoch 1/6\n",
      "161534/161534 [==============================] - 240s 1ms/step - loss: 0.0603\n",
      "Epoch 2/6\n",
      "161534/161534 [==============================] - 239s 1ms/step - loss: 0.0603\n",
      "Epoch 3/6\n",
      "161534/161534 [==============================] - 238s 1ms/step - loss: 0.0603\n",
      "Epoch 4/6\n",
      "161534/161534 [==============================] - 240s 1ms/step - loss: 0.0603\n",
      "Epoch 5/6\n",
      "161534/161534 [==============================] - 238s 1ms/step - loss: 0.0603\n",
      "Epoch 6/6\n",
      "161534/161534 [==============================] - 239s 1ms/step - loss: 0.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 09:23:46,638 utils 67 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 3 SCORE: 0.5001877722697912\n",
      "    #======================================================================== \n",
      "2019-01-14 09:23:46,706 utils 54 [INFO]    [<module>] Model Fitting Setup Start!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161534, 89) (161534, 1) (40383, 89) (40383, 1)\n",
      "89 89 89\n",
      "Epoch 1/6\n",
      "161534/161534 [==============================] - 240s 1ms/step - loss: 0.0603\n",
      "Epoch 2/6\n",
      "161534/161534 [==============================] - 238s 1ms/step - loss: 0.0604\n",
      "Epoch 3/6\n",
      "161534/161534 [==============================] - 239s 1ms/step - loss: 0.0603\n",
      "Epoch 4/6\n",
      "161534/161534 [==============================] - 240s 1ms/step - loss: 0.0604\n",
      "Epoch 5/6\n",
      "161534/161534 [==============================] - 238s 1ms/step - loss: 0.0603\n",
      "Epoch 6/6\n",
      "161534/161534 [==============================] - 239s 1ms/step - loss: 0.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 09:48:39,294 utils 67 [INFO]    [<module>] \n",
      "    #========================================================================\n",
      "    # FOLD 4 SCORE: 0.5000500726052777\n",
      "    #======================================================================== \n",
      "2019-01-14 09:48:39,296 utils 75 [INFO]    [<module>] \n",
      "#========================================================================\n",
      "# CV SCORE: 0.5002713210172851\n",
      "#======================================================================== \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# train = train[train.columns[:5]]\n",
    "# test = test[train.columns[:5]]\n",
    "# train = train.head(20000)\n",
    "# train[target] = y.head(20000).values\n",
    "# test = test.head(5000)\n",
    "\n",
    "cv_list = []\n",
    "seed_list = [1208]\n",
    "seed = seed_list[0]\n",
    "epoch = 6\n",
    "model_type = 'keras'\n",
    "learning_rate = 0\n",
    "\n",
    "pred_list = np.zeros(len(test))\n",
    "train[target] = y.values\n",
    "train[target] = train[target].map(lambda x: 1 if x<-30 else 0)\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "kfold = list(folds.split(train, train[target].values))\n",
    "# train.drop('outliers', axis=1, inplace=True)\n",
    "y = train[target]\n",
    "\n",
    "# Test Set\n",
    "len_test = len(test)\n",
    "len_feats = len(num_list)\n",
    "x_test = test.values.reshape(len_test, len_feats)\n",
    "x_test = [i for i in x_test.T]\n",
    "\n",
    "prediction = np.zeros(len_test)\n",
    "stack_prediction = np.zeros(len(train))\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(kfold):\n",
    "    tmp_train, y_train = train[num_list].iloc[trn_idx, :], y.iloc[trn_idx]\n",
    "    x_val, y_val = train[num_list].iloc[val_idx, :], y.iloc[val_idx]\n",
    "    len_train = len(tmp_train)\n",
    "    len_valid = len(x_val)\n",
    "    \n",
    "    if n_fold==0:\n",
    "        max_features = [len(tmp_train[col]) for col in num_list]\n",
    "        model = KerasFM(input_len=len(tmp_train), max_features=max_features)\n",
    "\n",
    "    x_train = tmp_train.values.reshape(len_train, len_feats)\n",
    "    y_train = y_train.values.reshape(len_train, 1)\n",
    "    x_val = x_val.values.reshape(len_valid, len_feats)\n",
    "    y_val = y_val.values.reshape(len_valid, 1)\n",
    "    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n",
    "\n",
    "    x_train = [i for i in x_train.T]\n",
    "    x_val = [i for i in x_val.T]\n",
    "    print(len(x_train),len(x_val), len(x_test))\n",
    "    \n",
    "    logger.info(\"Model Fitting Setup Start!!\")\n",
    "    model.fit(X=x_train, y=y_train, validation_data=(x_val, y_val), batch_size=batch_size, nb_epoch=epoch)\n",
    "    \n",
    "    test_pred = model.predict(X=x_test, batch_size=batch_size)\n",
    "    prediction += test_pred\n",
    "    \n",
    "    y_pred = model.predict(X=x_val, batch_size=batch_size)\n",
    "    stack_prediction[val_idx] = y_pred\n",
    "    \n",
    "    sc_score = roc_auc_score(y_val.reshape(len(y_val),), y_pred)\n",
    "    logger.info(f'''\n",
    "    #========================================================================\n",
    "    # FOLD {n_fold} SCORE: {sc_score}\n",
    "    #========================================================================''')\n",
    "    cv_list.append(sc_score)\n",
    "    \n",
    "prediction /= len(kfold)\n",
    "cv_score = np.mean(cv_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "\n",
    "train_pred = pd.Series(stack_prediction, name='prediction').to_frame()\n",
    "test_pred = pd.Series(prediction, name='prediction').to_frame()\n",
    "train_pred[key] = train_ids\n",
    "test_pred[key] = test_ids\n",
    "df_pred = pd.concat([train_pred, test_pred], axis=0)\n",
    "\n",
    "utils.to_pkl_gzip(path=f\"../stack/{start_time[4:12]}_stack_{model_type}_lr{learning_rate}_{len(num_list)}feats_{len(seed_list)}seed_{batch_size/gpu_count}batch_OUT_CV{str(cv_score).replace('.', '-')}_LB\", obj=df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
