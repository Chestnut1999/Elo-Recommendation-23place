{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 40.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (201917, 238) | Test: (123623, 238)\n"
     ]
    }
   ],
   "source": [
    "is_linear = [True, False][1]\n",
    "debug = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/model')\n",
    "from nn_keras import elo_build_NN, RMSE, elo_build_linear_NN\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "out_part = ['', 'part', 'all'][0]\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "stack_name='keras'\n",
    "model_type='keras'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "print(\"Preparing dataset...\")\n",
    "# win_path = f'../features/4_winner/*.gz'\n",
    "# Ensemble 1\n",
    "win_path = f'../model/E1_set/*.gz'\n",
    "# Ensemble 2\n",
    "# win_path = f'../model/E2_set/*.gz'\n",
    "# Ensemble 3\n",
    "# win_path = f'../model/E3_set/*.gz'\n",
    "# Ensemble 4\n",
    "# win_path = f'../model/E4_set/*.gz'\n",
    "\n",
    "win_path_list = glob.glob(win_path)\n",
    "\n",
    "base = utils.read_df_pkl('../input/base_term*0*')[[key, target, 'first_active_month']]\n",
    "base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
    "df = pd.concat(feature_list, axis=1)\n",
    "train = pd.concat([base_train, df.iloc[:len(base_train), :]], axis=1)\n",
    "test = pd.concat([base_test, df.iloc[len(base_train):, :].reset_index(drop=True)], axis=1)\n",
    "\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True , drop=True)\n",
    "\n",
    "if debug:\n",
    "    train = train.head(10000)\n",
    "    test = test.head(2000)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# 正規化の前処理(Null埋め, inf, -infの処理) \n",
    "for col in train.columns:\n",
    "    if col in ignore_list: continue\n",
    "        \n",
    "    train[col] = impute_feature(train, col)\n",
    "    test[col] = impute_feature(test, col)\n",
    "#========================================================================\n",
    "\n",
    "# #========================================================================\n",
    "# # inf check\n",
    "# length = len(train)\n",
    "# for col in train.columns:\n",
    "#     tmp = train[col].dropna().shape[0]\n",
    "#     if length - tmp>0:\n",
    "#         print(col)\n",
    "        \n",
    "#     inf_max = train[col].max()\n",
    "#     inf_min = train[col].min()\n",
    "#     if inf_max==np.inf or inf_min==-np.inf:\n",
    "#         print(col, inf_max, inf_min)\n",
    "# #========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# ods.ai 3rd kernel\n",
    "# https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/78903\n",
    "# KFold: n_splits=6(or 7)!, shuffle=False!\n",
    "train['rounded_target'] = train['target'].round(0)\n",
    "train = train.sort_values('rounded_target').reset_index(drop=True)\n",
    "vc = train['rounded_target'].value_counts()\n",
    "vc = dict(sorted(vc.items()))\n",
    "df = pd.DataFrame()\n",
    "train['indexcol'],idx = 0,1\n",
    "for k,v in vc.items():\n",
    "    step = train.shape[0]/v\n",
    "    indent = train.shape[0]/(v+1)\n",
    "    df2 = train[train['rounded_target'] == k].sample(v, random_state=seed).reset_index(drop=True)\n",
    "    for j in range(0, v):\n",
    "        df2.at[j, 'indexcol'] = indent + j*step + 0.000001*idx\n",
    "    df = pd.concat([df2,df])\n",
    "    idx+=1\n",
    "train = df.sort_values('indexcol', ascending=True).reset_index(drop=True)\n",
    "del train['indexcol'], train['rounded_target']\n",
    "fold_type = 'self'\n",
    "fold = 6\n",
    "folds = KFold(n_splits=fold, shuffle=False, random_state=seed)\n",
    "kfold = list(folds.split(train, train[target].values))\n",
    "# =======================================================================\n",
    "\n",
    "#========================================================================\n",
    "# CVの準備\n",
    "\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# なぜか一回目で終わらないことがあるので。。\n",
    "try:\n",
    "    scaler.fit(pd.concat([train[use_cols], test[use_cols]]))\n",
    "except ValueError:\n",
    "    inf_col_list = []\n",
    "    for col in use_cols:\n",
    "\n",
    "        inf_max = train[col].max()\n",
    "        inf_min = train[col].min()\n",
    "        if inf_max==np.inf or inf_min==-np.inf:\n",
    "            inf_col_list.append(col)\n",
    "    \n",
    "    for col in inf_col_list:\n",
    "        train[col] = impute_feature(train, col)\n",
    "        test[col] = impute_feature(test, col)\n",
    "    scaler.fit(pd.concat([train[use_cols], test[use_cols]]))\n",
    "    \n",
    "x_test = scaler.transform(test[use_cols])\n",
    "Y = train[target]\n",
    "y_min = Y.min()\n",
    "if not(is_linear):\n",
    "    Y = Y - y_min+1\n",
    "\n",
    "# x_test = x_test.as_matrix()\n",
    "# For LSTM\n",
    "if not(is_linear):\n",
    "    x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "#========================================================================\n",
    "    \n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Train on 168264 samples, validate on 33653 samples\n",
      "Epoch 1/30\n",
      " - 40s - loss: 3.9731 - RMSE: 3.9731 - val_loss: 1.5322 - val_RMSE: 1.5322\n",
      "Epoch 2/30\n",
      " - 31s - loss: 2.2280 - RMSE: 2.2280 - val_loss: 1.5822 - val_RMSE: 1.5822\n",
      "Epoch 3/30\n",
      " - 30s - loss: inf - RMSE: inf - val_loss: inf - val_RMSE: inf\n",
      "Epoch 4/30\n",
      " - 30s - loss: inf - RMSE: inf - val_loss: inf - val_RMSE: inf\n",
      "Epoch 5/30\n",
      " - 31s - loss: inf - RMSE: inf - val_loss: inf - val_RMSE: inf\n",
      "Epoch 6/30\n",
      " - 31s - loss: inf - RMSE: inf - val_loss: inf - val_RMSE: inf\n",
      "Epoch 7/30\n",
      " - 30s - loss: inf - RMSE: inf - val_loss: inf - val_RMSE: inf\n",
      "Epoch 8/30\n",
      " - 30s - loss: inf - RMSE: inf - val_loss: inf - val_RMSE: inf\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 9/30\n",
      " - 31s - loss: inf - RMSE: inf - val_loss: inf - val_RMSE: inf\n",
      "Epoch 10/30\n",
      " - 31s - loss: inf - RMSE: inf - val_loss: inf - val_RMSE: inf\n",
      "Epoch 11/30\n",
      " - 30s - loss: inf - RMSE: inf - val_loss: inf - val_RMSE: inf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f2c3f2de4de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Scoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'RMSE: {score} | SUM ERROR: {err.sum()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mscore_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[1;32m    238\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 239\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nn_keras import elo_build_NN, RMSE, elo_build_linear_NN\n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "\n",
    "N_EPOCHS = 15\n",
    "N_EPOCHS = 30\n",
    "# batch_size = 65536\n",
    "batch_size = 1024\n",
    "batch_size = 256\n",
    "batch_size = 128\n",
    "learning_rate = 1e-2\n",
    "\n",
    "if is_linear:\n",
    "    model = elo_build_linear_NN(input_cols=len(use_cols))\n",
    "else:    \n",
    "    model = elo_build_NN(input_rows=1, input_cols=len(use_cols))\n",
    "\n",
    "opt = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss=RMSE, optimizer=opt, metrics=[RMSE])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "]\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Result Box\n",
    "model_list = []\n",
    "result_list = []\n",
    "score_list = []\n",
    "val_pred_list = []\n",
    "test_pred = np.zeros(len(test))\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "\n",
    "for fold_no, (trn_idx, val_idx) in enumerate(kfold):\n",
    "\n",
    "    #========================================================================\n",
    "    # Make Dataset\n",
    "#     X_train, X_val = train_test_split(train, test_size=0.2)\n",
    "    X_train, y_train = train.iloc[trn_idx, :][use_cols], Y.iloc[trn_idx]\n",
    "    X_val, y_val = train.iloc[val_idx, :][use_cols], Y.iloc[val_idx]\n",
    "    \n",
    "     \n",
    "    X_train[:] = scaler.transform(X_train)\n",
    "    X_val[:] = scaler.transform(X_val)\n",
    "    X_train = X_train.as_matrix()\n",
    "    X_val = X_val.as_matrix()\n",
    "    if not(is_linear):\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    #========================================================================\n",
    "    \n",
    "    # Fitting\n",
    "    # なぜか平均を引いてる？そのほうがfitするの？\n",
    "    # model.fit(X_train, y- y_mean, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "    #            validation_data=(X_val, y_val - y_mean), callbacks=callbacks )\n",
    "    model.fit(X_train, y_train, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "               validation_data=(X_val, y_val), callbacks=callbacks )\n",
    "    \n",
    "    # Prediction\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], )\n",
    "    tmp_pred = model.predict(x_test)\n",
    "    test_pred += tmp_pred.reshape(tmp_pred.shape[0], )\n",
    "    \n",
    "    if not(is_linear):\n",
    "        y_pred += (y_min-1)\n",
    "        test_pred += (y_min-1)\n",
    "#     model_list.append(model)\n",
    "    \n",
    "    # Stack Prediction\n",
    "    df_pred = train.iloc[val_idx, :][[key, target]].copy()\n",
    "    df_pred['prediction'] = y_pred\n",
    "    result_list.append(df_pred)\n",
    "    \n",
    "    # Scoring\n",
    "    err = (y_val - y_pred)\n",
    "    score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "    score_list.append(score)\n",
    "    #========================================================================\n",
    "\n",
    "cv_score = np.mean(score_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "#========================================================================\n",
    "# Stacking\n",
    "test_pred /= fold\n",
    "test['prediction'] = test_pred\n",
    "stack_test = test[[key, 'prediction']]\n",
    "result_list.append(stack_test)\n",
    "df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "df_pred = base.merge(df_pred, how='inner', on=key)\n",
    "print(f\"Stacking Shape: {df_pred.shape}\")\n",
    "\n",
    "utils.to_pkl_gzip(obj=df_pred, path=f'../stack/{start_time[4:12]}_elo_NN_stack_CV{cv_score}')\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "#========================================================================\n",
    "# Part of card_id Score\n",
    "bench = pd.read_csv('../input/bench_LB3684_FAM_cv_score.csv')\n",
    "part_score_list = []\n",
    "part_N_list = []\n",
    "fam_list = []\n",
    "#  for i in range(201101, 201713, 1):\n",
    "for i in range(201501, 201713, 1):\n",
    "    fam = str(i)[:4] + '-' + str(i)[-2:]\n",
    "    df_part = base_train[base_train['first_active_month']==fam]\n",
    "    if len(df_part)<1:\n",
    "        continue\n",
    "    part_id_list = df_part[key].values\n",
    "\n",
    "    part_train = df_pred.loc[df_pred[key].isin(part_id_list), :]\n",
    "    y_train = part_train[target].values\n",
    "    if 'pred_mean' in list(part_train.columns):\n",
    "        y_pred = part_train['pred_mean'].values\n",
    "    else:\n",
    "        y_pred = part_train['prediction'].values\n",
    "\n",
    "    y_pred = np.where(y_pred != y_pred, 0, y_pred)\n",
    "    # RMSE\n",
    "    part_score = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "    bench_score = bench[bench['FAM']==fam]['CV'].values[0]\n",
    "    part_score -= bench_score\n",
    "\n",
    "    fam_list.append(fam)\n",
    "    part_score_list.append(part_score)\n",
    "    part_N_list.append(len(part_id_list))\n",
    "\n",
    "#  for i, part_score, N in zip(fam_list, part_score_list, part_N_list):\n",
    "df = pd.DataFrame(np.asarray([fam_list, part_score_list, part_N_list]).T)\n",
    "df.columns = ['FAM', 'CV', 'N']\n",
    "\n",
    "# FAM: {i} | CV: {part_score} | N: {len(part_id_list)}\n",
    "pd.set_option('max_rows', 200)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# {df}\n",
    "#========================================================================''')\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "if len(train)>150000:\n",
    "    if len(train[train[target]<-30])>0:\n",
    "        # outlierに対するスコアを出す\n",
    "        train.reset_index(inplace=True)\n",
    "        out_ids = train.loc[train.target<-30, key].values\n",
    "        out_val = train.loc[train.target<-30, target].values\n",
    "        if len(seed_list)==1:\n",
    "            out_pred = df_pred[df_pred[key].isin(out_ids)]['prediction'].values\n",
    "        else:\n",
    "            out_pred = df_pred[df_pred[key].isin(out_ids)]['pred_mean'].values\n",
    "        out_score = np.sqrt(mean_squared_error(out_val, out_pred))\n",
    "    else:\n",
    "        out_score = 0\n",
    "else:\n",
    "    out_score = 0\n",
    "\n",
    "# Save\n",
    "try:\n",
    "    if int(sys.argv[2])==0:\n",
    "        utils.to_pkl_gzip(path=f\"../stack/{start_time[4:12]}_stack_{model_type}_lr{learning_rate}_{feature_num}feats_{len(seed_list)}seed_{num_leaves}leaves_iter{iter_avg}_OUT{str(out_score)[:7]}_CV{str(cv_score).replace('.', '-')}_LB\", obj=df_pred)\n",
    "except ValueError:\n",
    "    pass\n",
    "except TypeError:\n",
    "    pass\n",
    "\n",
    "# 不要なカラムを削除\n",
    "drop_feim_cols = [col for col in cv_feim.columns if col.count('importance_') or col.count('rank_')]\n",
    "cv_feim.drop(drop_feim_cols, axis=1, inplace=True)\n",
    "drop_feim_cols = [col for col in cv_feim.columns if col.count('importance') and not(col.count('avg'))]\n",
    "cv_feim.drop(drop_feim_cols, axis=1, inplace=True)\n",
    "cv_feim.to_csv( f'../valid/{start_time[4:12]}_valid_{model_type}_lr{learning_rate}_{feature_num}feats_{len(seed_list)}seed_{num_leaves}leaves_iter{iter_avg}_OUT{str(out_score)[:7]}_CV{cv_score}_LB.csv' , index=False)\n",
    "\n",
    "#========================================================================\n",
    "# CV INFO\n",
    "\n",
    "try:\n",
    "    if int(sys.argv[2])==0 and len(train)>150000:\n",
    "\n",
    "        import re\n",
    "        path_list = glob.glob('../log_submit/0*CV*LB*.csv')\n",
    "        path_list.append(submit_path)\n",
    "        #  path_list_2 = glob.glob('../check_submit/*.csv')\n",
    "        #  path_list += path_list_2\n",
    "\n",
    "        tmp_list = []\n",
    "        path_list = list(set(path_list))\n",
    "        for path in path_list:\n",
    "            tmp = pd.read_csv(path)\n",
    "            tmp_path = path.replace(\".\", '-')\n",
    "            cv = re.search(r'CV([^/.]*)_LB', tmp_path).group(1).replace('-', '.')\n",
    "            lb = re.search(r'LB([^/.]*).csv', tmp_path).group(1).replace('-', '.')\n",
    "            #  if lb<'3.690' and path!=submit_path:\n",
    "            #      continue\n",
    "            tmp.rename(columns={'target':f\"CV{cv[:9]}_LB{lb}\"}, inplace=True)\n",
    "            tmp.set_index('card_id', inplace=True)\n",
    "            tmp_list.append(tmp.copy())\n",
    "\n",
    "        if len(tmp_list)>0:\n",
    "            df = pd.concat(tmp_list, axis=1)\n",
    "            df_corr = df.corr(method='pearson')\n",
    "\n",
    "            logger.info(f'''\n",
    "#========================================================================\n",
    "# OUTLIER FIT SCORE: {out_score}\n",
    "# SUBMIT CORRELATION:\n",
    "{df_corr[f'CV{str(cv_score)[:9]}_LB'].sort_values()}\n",
    "#========================================================================''')\n",
    "except ValueError:\n",
    "    pass\n",
    "except TypeError:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
