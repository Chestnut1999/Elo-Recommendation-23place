{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 48.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 405, in _handle_workers\n",
      "    pool._maintain_pool()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 246, in _maintain_pool\n",
      "    self._repopulate_pool()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\", line 20, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\", line 67, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-24-d8dd50eb2c89>\", line 66, in <module>\n",
      "    feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
      "  File \"/home/ubuntu/kaggle/data_analysis/library/utils.py\", line 435, in parallel_load_data\n",
      "    p_list = p.map(load_file, path_list)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 266, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 638, in get\n",
      "    self.wait(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 635, in wait\n",
      "    self._event.wait(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 551, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/inspect.py\", line 690, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/model')\n",
    "from nn_keras import elo_build_NN, RMSE\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "out_part = ['', 'part', 'all'][0]\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "stack_name='keras'\n",
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "model_type='keras'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "print(\"Preparing dataset...\")\n",
    "win_path = f'../features/4_winner/*.gz'\n",
    "# Ensemble 1\n",
    "win_path = f'../model/LB3670_70leaves_colsam0322/*.gz'\n",
    "# Ensemble 2\n",
    "win_path = f'../model/E2_lift_set/*.gz'\n",
    "# Ensemble 3\n",
    "# win_path = f'../model/E3_PCA_set/*.gz'\n",
    "\n",
    "win_path_list = glob.glob(win_path)\n",
    "\n",
    "base = utils.read_df_pkl('../input/base_term*0*')[[key, target, 'first_active_month']]\n",
    "base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
    "df = pd.concat(feature_list, axis=1)\n",
    "train = pd.concat([base_train, df.iloc[:len(base_train), :]], axis=1)\n",
    "test = pd.concat([base_test, df.iloc[len(base_train):, :].reset_index(drop=True)], axis=1)\n",
    "\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True , drop=True)\n",
    "\n",
    "if debug:\n",
    "    train = train.head(10000)\n",
    "    test = test.head(2000)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# 正規化の前処理(Null埋め, inf, -infの処理) \n",
    "for col in train.columns:\n",
    "    if col in ignore_list: continue\n",
    "        \n",
    "    train[col] = impute_feature(train, col)\n",
    "    test[col] = impute_feature(test, col)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# ods.ai 3rd kernel\n",
    "# https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/78903\n",
    "# KFold: n_splits=6(or 7)!, shuffle=False!\n",
    "train['rounded_target'] = train['target'].round(0)\n",
    "train = train.sort_values('rounded_target').reset_index(drop=True)\n",
    "vc = train['rounded_target'].value_counts()\n",
    "vc = dict(sorted(vc.items()))\n",
    "df = pd.DataFrame()\n",
    "train['indexcol'],idx = 0,1\n",
    "for k,v in vc.items():\n",
    "    step = train.shape[0]/v\n",
    "    indent = train.shape[0]/(v+1)\n",
    "    df2 = train[train['rounded_target'] == k].sample(v, random_state=seed).reset_index(drop=True)\n",
    "    for j in range(0, v):\n",
    "        df2.at[j, 'indexcol'] = indent + j*step + 0.000001*idx\n",
    "    df = pd.concat([df2,df])\n",
    "    idx+=1\n",
    "train = df.sort_values('indexcol', ascending=True).reset_index(drop=True)\n",
    "del train['indexcol'], train['rounded_target']\n",
    "fold_type = 'self'\n",
    "fold = 6\n",
    "folds = KFold(n_splits=fold, shuffle=False, random_state=seed)\n",
    "kfold = folds.split(train, train[target].values)\n",
    "# =======================================================================\n",
    "\n",
    "#========================================================================\n",
    "# CVの準備\n",
    "model_list = []\n",
    "result_list = []\n",
    "score_list = []\n",
    "val_pred_list = []\n",
    "test_pred = np.zeros(len(test))\n",
    "\n",
    "N_EPOCHS = 30\n",
    "# batch_size = 65536\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([train[use_cols], test[use_cols]]))\n",
    "x_test = scaler.transform(test[use_cols])\n",
    "\n",
    "# x_test = x_test.as_matrix()\n",
    "x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "Y = train[target]\n",
    "y_mean = Y.mean()\n",
    "#========================================================================\n",
    "    \n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\")\n",
    "    \n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "model = elo_build_NN(input_rows=1, input_cols=len(use_cols))\n",
    "opt = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss=RMSE, optimizer=opt, metrics=[RMSE])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "]\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "\n",
    "for fold_no, (trn_idx, val_idx) in enumerate(kfold):\n",
    "\n",
    "    #========================================================================\n",
    "    # Make Dataset\n",
    "#     X_train, X_val = train_test_split(train, test_size=0.2)\n",
    "    X_train, y_train = train.iloc[trn_idx, :][use_cols], Y.iloc[trn_idx]\n",
    "    X_val, y_val = train.iloc[val_idx, :][use_cols], Y.iloc[val_idx]\n",
    "    \n",
    "     \n",
    "    X_train[:] = scaler.transform(X_train)\n",
    "    X_val[:] = scaler.transform(X_val)\n",
    "    X_train = X_train.as_matrix()\n",
    "    X_val = X_val.as_matrix()\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    #========================================================================\n",
    "    \n",
    "    # Fitting\n",
    "    # なぜか平均を引いてる？そのほうがfitするの？\n",
    "    # model.fit(X_train, y- y_mean, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "    #            validation_data=(X_val, y_val - y_mean), callbacks=callbacks )\n",
    "    model.fit(X_train, y_train, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "               validation_data=(X_val, y_val), callbacks=callbacks )\n",
    "    \n",
    "    # Prediction\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], )\n",
    "    tmp_pred = model.predict(x_test)\n",
    "    test_pred += tmp_pred.reshape(tmp_pred.shape[0], )\n",
    "    model_list.append(model)\n",
    "    \n",
    "    # Stack Prediction\n",
    "    df_pred = train.iloc[val_idx, :][[key, target]].copy()\n",
    "    df_pred['prediction'] = y_pred\n",
    "    result_list.append(df_pred)\n",
    "    \n",
    "    # Scoring\n",
    "    err = (y_val - y_pred)\n",
    "    score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "    score_list.append(score)\n",
    "    #========================================================================\n",
    "\n",
    "cv_score = np.mean(score_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "#========================================================================\n",
    "# Stacking\n",
    "test_pred /= fold\n",
    "test['prediction'] = test_pred\n",
    "stack_test = test[[key, 'prediction']]\n",
    "result_list.append(stack_test)\n",
    "df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "df_pred = base.merge(df_pred, how='inner', on=key)\n",
    "print(f\"Stacking Shape: {df_pred.shape}\")\n",
    "\n",
    "utils.to_pkl_gzip(obj=df_pred, path=f'../stack/{start_time[4:11]}_elo_NN_stack_CV{cv_score}')\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "#========================================================================\n",
    "# Part of card_id Score\n",
    "bench = pd.read_csv('../input/bench_LB3684_FAM_cv_score.csv')\n",
    "part_score_list = []\n",
    "part_N_list = []\n",
    "fam_list = []\n",
    "#  for i in range(201101, 201713, 1):\n",
    "for i in range(201501, 201713, 1):\n",
    "    fam = str(i)[:4] + '-' + str(i)[-2:]\n",
    "    df_part = base_train[base_train['first_active_month']==fam]\n",
    "    if len(df_part)<1:\n",
    "        continue\n",
    "    part_id_list = df_part[key].values\n",
    "\n",
    "    part_train = df_pred.loc[df_pred[key].isin(part_id_list), :]\n",
    "    y_train = part_train[target].values\n",
    "    if 'pred_mean' in list(part_train.columns):\n",
    "        y_pred = part_train['pred_mean'].values\n",
    "    else:\n",
    "        y_pred = part_train['prediction'].values\n",
    "\n",
    "    y_pred = np.where(y_pred != y_pred, 0, y_pred)\n",
    "    # RMSE\n",
    "    part_score = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "    bench_score = bench[bench['FAM']==fam]['CV'].values[0]\n",
    "    part_score -= bench_score\n",
    "\n",
    "    fam_list.append(fam)\n",
    "    part_score_list.append(part_score)\n",
    "    part_N_list.append(len(part_id_list))\n",
    "\n",
    "#  for i, part_score, N in zip(fam_list, part_score_list, part_N_list):\n",
    "df = pd.DataFrame(np.asarray([fam_list, part_score_list, part_N_list]).T)\n",
    "df.columns = ['FAM', 'CV', 'N']\n",
    "\n",
    "# FAM: {i} | CV: {part_score} | N: {len(part_id_list)}\n",
    "pd.set_option('max_rows', 200)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# {df}\n",
    "#========================================================================''')\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "if len(train)>150000:\n",
    "    if len(train[train[target]<-30])>0:\n",
    "        # outlierに対するスコアを出す\n",
    "        train.reset_index(inplace=True)\n",
    "        out_ids = train.loc[train.target<-30, key].values\n",
    "        out_val = train.loc[train.target<-30, target].values\n",
    "        if len(seed_list)==1:\n",
    "            out_pred = df_pred[df_pred[key].isin(out_ids)]['prediction'].values\n",
    "        else:\n",
    "            out_pred = df_pred[df_pred[key].isin(out_ids)]['pred_mean'].values\n",
    "        out_score = np.sqrt(mean_squared_error(out_val, out_pred))\n",
    "    else:\n",
    "        out_score = 0\n",
    "else:\n",
    "    out_score = 0\n",
    "\n",
    "# Save\n",
    "try:\n",
    "    if int(sys.argv[2])==0:\n",
    "        utils.to_pkl_gzip(path=f\"../stack/{start_time[4:12]}_stack_{model_type}_lr{learning_rate}_{feature_num}feats_{len(seed_list)}seed_{num_leaves}leaves_iter{iter_avg}_OUT{str(out_score)[:7]}_CV{str(cv_score).replace('.', '-')}_LB\", obj=df_pred)\n",
    "except ValueError:\n",
    "    pass\n",
    "except TypeError:\n",
    "    pass\n",
    "\n",
    "# 不要なカラムを削除\n",
    "drop_feim_cols = [col for col in cv_feim.columns if col.count('importance_') or col.count('rank_')]\n",
    "cv_feim.drop(drop_feim_cols, axis=1, inplace=True)\n",
    "drop_feim_cols = [col for col in cv_feim.columns if col.count('importance') and not(col.count('avg'))]\n",
    "cv_feim.drop(drop_feim_cols, axis=1, inplace=True)\n",
    "cv_feim.to_csv( f'../valid/{start_time[4:12]}_valid_{model_type}_lr{learning_rate}_{feature_num}feats_{len(seed_list)}seed_{num_leaves}leaves_iter{iter_avg}_OUT{str(out_score)[:7]}_CV{cv_score}_LB.csv' , index=False)\n",
    "\n",
    "#========================================================================\n",
    "# Submission\n",
    "try:\n",
    "    if int(sys.argv[2])==0:\n",
    "        test_pred = seed_pred / len(seed_list)\n",
    "        submit[target] = test_pred\n",
    "        submit_path = f'../submit/{start_time[4:12]}_submit_{model_type}_lr{learning_rate}_{feature_num}feats_{len(seed_list)}seed_{num_leaves}leaves_iter{iter_avg}_OUT{str(out_score)[:7]}_CV{cv_score}_LB.csv'\n",
    "        submit.to_csv(submit_path, index=False)\n",
    "except ValueError:\n",
    "    pass\n",
    "except TypeError:\n",
    "    pass\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# CV INFO\n",
    "\n",
    "try:\n",
    "    if int(sys.argv[2])==0 and len(train)>150000:\n",
    "\n",
    "        import re\n",
    "        path_list = glob.glob('../log_submit/0*CV*LB*.csv')\n",
    "        path_list.append(submit_path)\n",
    "        #  path_list_2 = glob.glob('../check_submit/*.csv')\n",
    "        #  path_list += path_list_2\n",
    "\n",
    "        tmp_list = []\n",
    "        path_list = list(set(path_list))\n",
    "        for path in path_list:\n",
    "            tmp = pd.read_csv(path)\n",
    "            tmp_path = path.replace(\".\", '-')\n",
    "            cv = re.search(r'CV([^/.]*)_LB', tmp_path).group(1).replace('-', '.')\n",
    "            lb = re.search(r'LB([^/.]*).csv', tmp_path).group(1).replace('-', '.')\n",
    "            #  if lb<'3.690' and path!=submit_path:\n",
    "            #      continue\n",
    "            tmp.rename(columns={'target':f\"CV{cv[:9]}_LB{lb}\"}, inplace=True)\n",
    "            tmp.set_index('card_id', inplace=True)\n",
    "            tmp_list.append(tmp.copy())\n",
    "\n",
    "        if len(tmp_list)>0:\n",
    "            df = pd.concat(tmp_list, axis=1)\n",
    "            df_corr = df.corr(method='pearson')\n",
    "\n",
    "            logger.info(f'''\n",
    "#========================================================================\n",
    "# OUTLIER FIT SCORE: {out_score}\n",
    "# SUBMIT CORRELATION:\n",
    "{df_corr[f'CV{str(cv_score)[:9]}_LB'].sort_values()}\n",
    "#========================================================================''')\n",
    "except ValueError:\n",
    "    pass\n",
    "except TypeError:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
