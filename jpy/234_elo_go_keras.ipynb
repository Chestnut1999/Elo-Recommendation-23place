{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 45.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (199710, 238) | Test: (123623, 238)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "is_LSTM = False\n",
    "is_plus = [True, False][1]\n",
    "out_part = ['', 'no_out'][1]\n",
    "set_no = [0,1,2,3][0]\n",
    "const_cnt = 1\n",
    "is_oof = [True, False]\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/model')\n",
    "from nn_keras import mercari_1st_NN, corp_1st_LSTM, RMSE\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "stack_name='keras'\n",
    "model_type='keras'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "print(\"Preparing dataset...\")\n",
    "# win_path = f'../features/4_winner/*.gz'\n",
    "# Ensemble 1\n",
    "set1 = f'../model/E1_set/*.gz'\n",
    "# Ensemble 2\n",
    "set2 = f'../model/E2_set/*.gz'\n",
    "# Ensemble 3\n",
    "set3 = f'../model/E3_set/*.gz'\n",
    "# Ensemble 4\n",
    "set4 = f'../model/E4_set/*.gz'\n",
    "\n",
    "set_list = [set1, set2, set3, set4]\n",
    "win_path = set_list[set_no]\n",
    "\n",
    "win_path_list = glob.glob(win_path)\n",
    "\n",
    "base = utils.read_df_pkl('../input/base_term*0*')[[key, target, 'first_active_month']]\n",
    "base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
    "df = pd.concat(feature_list, axis=1)\n",
    "train = pd.concat([base_train, df.iloc[:len(base_train), :]], axis=1)\n",
    "test = pd.concat([base_test, df.iloc[len(base_train):, :].reset_index(drop=True)], axis=1)\n",
    "\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True , drop=True)\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "train_ids = train[key].values\n",
    "df_stack = train[[key, target]].copy().set_index(key)\n",
    "\n",
    "if out_part=='no_out':\n",
    "    train = train[train[target]>-30]\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# 正規化の前処理(Null埋め, inf, -infの処理) \n",
    "for col in train.columns:\n",
    "    if col in ignore_list: continue\n",
    "        \n",
    "    train[col] = impute_feature(train, col)\n",
    "    test[col] = impute_feature(test, col)\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Construction Value Range Preprocessing\n",
    "def constraction(feature, is_viz=False, out_range=1.64):\n",
    "    if is_viz:\n",
    "        print('before:', feature.max(), feature.min())\n",
    "    std = feature.std()\n",
    "    avg = feature.mean()\n",
    "    z_val = (feature - avg)/std\n",
    "    \n",
    "    # Pass the Case No Outlier\n",
    "    try:\n",
    "        p_min = feature[feature>=out_range].min()\n",
    "        feature = np.where(z_val>=out_range, p_min, feature)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        m_max = feature[feature<=-1*out_range].max()\n",
    "        feature = np.where(z_val<=-1*out_range, m_max, feature)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    if is_viz:\n",
    "        print('after:', feature.max(), feature.min())   \n",
    "    return feature\n",
    "\n",
    "train_test = pd.concat([train, test], axis=0)\n",
    "for col in use_cols:\n",
    "    feature = train_test[col].values\n",
    "    \n",
    "    for i in range(const_cnt):\n",
    "        feature = constraction(feature)\n",
    "    \n",
    "    feature = feature.astype('float32')\n",
    "    train_test[col] = feature\n",
    "train = train_test[~train_test[target].isnull()]\n",
    "test = train_test[train_test[target].isnull()]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([train[use_cols], test[use_cols]]))   \n",
    "x_test = scaler.transform(test[use_cols])\n",
    "Y = train[target]\n",
    "\n",
    "# ========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# CVの準備\n",
    "fold = 6\n",
    "if out_part != 'no_out':\n",
    "    kfold = utils.read_pkl_gzip('../input/ods_kfold.gz')\n",
    "else:\n",
    "    kfold = utils.read_pkl_gzip('../input/ods_NoOut_kfold.gz')\n",
    "if is_plus:\n",
    "    y_min = Y.min()\n",
    "    Y = Y - (y_min-1)\n",
    "\n",
    "# x_test = x_test.as_matrix()\n",
    "# For LSTM\n",
    "if is_LSTM:\n",
    "    x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "    \n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\") \n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Train on 166425 samples, validate on 33285 samples\n",
      "Epoch 1/30\n",
      " - 7s - loss: 1.1165 - RMSE: 1.1165 - val_loss: 1.1055 - val_RMSE: 1.1055\n",
      "Epoch 2/30\n",
      " - 8s - loss: 1.1021 - RMSE: 1.1021 - val_loss: 1.1034 - val_RMSE: 1.1034\n",
      "Epoch 3/30\n",
      " - 7s - loss: 1.0966 - RMSE: 1.0966 - val_loss: 1.1008 - val_RMSE: 1.1008\n",
      "Epoch 4/30\n",
      " - 7s - loss: 1.0920 - RMSE: 1.0920 - val_loss: 1.0985 - val_RMSE: 1.0985\n",
      "Epoch 5/30\n",
      " - 7s - loss: 1.0881 - RMSE: 1.0881 - val_loss: 1.1009 - val_RMSE: 1.1009\n",
      "Epoch 6/30\n",
      " - 7s - loss: 1.0835 - RMSE: 1.0835 - val_loss: 1.1024 - val_RMSE: 1.1024\n",
      "Epoch 7/30\n",
      " - 7s - loss: 1.0795 - RMSE: 1.0795 - val_loss: 1.1002 - val_RMSE: 1.1002\n",
      "Epoch 8/30\n",
      " - 6s - loss: 1.0745 - RMSE: 1.0745 - val_loss: 1.1055 - val_RMSE: 1.1055\n",
      "Epoch 9/30\n",
      " - 6s - loss: 1.0695 - RMSE: 1.0695 - val_loss: 1.1030 - val_RMSE: 1.1030\n",
      "RMSE: 1138.7329612477088 | SUM ERROR: 31892963.841623794\n",
      "Train on 166425 samples, validate on 33285 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 1.0734 - RMSE: 1.0734 - val_loss: 1.0677 - val_RMSE: 1.0677\n",
      "Epoch 2/30\n",
      " - 6s - loss: 1.0673 - RMSE: 1.0673 - val_loss: 1.0732 - val_RMSE: 1.0732\n",
      "Epoch 3/30\n",
      " - 6s - loss: 1.0615 - RMSE: 1.0615 - val_loss: 1.0730 - val_RMSE: 1.0730\n",
      "Epoch 4/30\n",
      " - 6s - loss: 1.0557 - RMSE: 1.0557 - val_loss: 1.0755 - val_RMSE: 1.0755\n",
      "Epoch 5/30\n",
      " - 6s - loss: 1.0501 - RMSE: 1.0501 - val_loss: 1.0796 - val_RMSE: 1.0796\n",
      "Epoch 6/30\n",
      " - 6s - loss: 1.0442 - RMSE: 1.0442 - val_loss: 1.0805 - val_RMSE: 1.0805\n",
      "RMSE: 1298.725570034545 | SUM ERROR: 36275700.672324985\n",
      "Train on 166425 samples, validate on 33285 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 1.0484 - RMSE: 1.0484 - val_loss: 1.0486 - val_RMSE: 1.0486\n",
      "Epoch 2/30\n",
      " - 6s - loss: 1.0414 - RMSE: 1.0414 - val_loss: 1.0511 - val_RMSE: 1.0511\n",
      "Epoch 3/30\n",
      " - 6s - loss: 1.0365 - RMSE: 1.0365 - val_loss: 1.0587 - val_RMSE: 1.0587\n",
      "Epoch 4/30\n",
      " - 6s - loss: 1.0292 - RMSE: 1.0292 - val_loss: 1.0645 - val_RMSE: 1.0645\n",
      "Epoch 5/30\n",
      " - 6s - loss: 1.0241 - RMSE: 1.0241 - val_loss: 1.0653 - val_RMSE: 1.0653\n",
      "Epoch 6/30\n",
      " - 6s - loss: 1.0180 - RMSE: 1.0180 - val_loss: 1.0658 - val_RMSE: 1.0658\n",
      "RMSE: 1597.4514481704828 | SUM ERROR: 45142341.888347805\n",
      "Train on 166425 samples, validate on 33285 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 1.0285 - RMSE: 1.0285 - val_loss: 1.0014 - val_RMSE: 1.0014\n",
      "Epoch 2/30\n",
      " - 6s - loss: 1.0214 - RMSE: 1.0214 - val_loss: 1.0090 - val_RMSE: 1.0090\n",
      "Epoch 3/30\n",
      " - 6s - loss: 1.0158 - RMSE: 1.0158 - val_loss: 1.0189 - val_RMSE: 1.0189\n",
      "Epoch 4/30\n",
      " - 6s - loss: 1.0102 - RMSE: 1.0102 - val_loss: 1.0223 - val_RMSE: 1.0223\n",
      "Epoch 5/30\n",
      " - 6s - loss: 1.0038 - RMSE: 1.0038 - val_loss: 1.0305 - val_RMSE: 1.0305\n",
      "Epoch 6/30\n",
      " - 6s - loss: 0.9992 - RMSE: 0.9992 - val_loss: 1.0401 - val_RMSE: 1.0401\n",
      "RMSE: 857.4817520452965 | SUM ERROR: 24311862.282681786\n",
      "Train on 166425 samples, validate on 33285 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 1.0071 - RMSE: 1.0071 - val_loss: 0.9819 - val_RMSE: 0.9819\n",
      "Epoch 2/30\n",
      " - 6s - loss: 0.9999 - RMSE: 0.9999 - val_loss: 0.9926 - val_RMSE: 0.9926\n",
      "Epoch 3/30\n",
      " - 6s - loss: 0.9943 - RMSE: 0.9943 - val_loss: 1.0045 - val_RMSE: 1.0045\n",
      "Epoch 4/30\n",
      " - 6s - loss: 0.9885 - RMSE: 0.9885 - val_loss: 1.0089 - val_RMSE: 1.0089\n",
      "Epoch 5/30\n",
      " - 6s - loss: 0.9842 - RMSE: 0.9842 - val_loss: 1.0175 - val_RMSE: 1.0175\n",
      "Epoch 6/30\n",
      " - 6s - loss: 0.9795 - RMSE: 0.9795 - val_loss: 1.0195 - val_RMSE: 1.0195\n",
      "RMSE: 1664.0496638877332 | SUM ERROR: 47284346.12965426\n",
      "Train on 166425 samples, validate on 33285 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 0.9890 - RMSE: 0.9890 - val_loss: 0.9656 - val_RMSE: 0.9656\n",
      "Epoch 2/30\n",
      " - 6s - loss: 0.9820 - RMSE: 0.9820 - val_loss: 0.9790 - val_RMSE: 0.9790\n",
      "Epoch 3/30\n",
      " - 6s - loss: 0.9764 - RMSE: 0.9764 - val_loss: 0.9899 - val_RMSE: 0.9899\n",
      "Epoch 4/30\n",
      " - 6s - loss: 0.9718 - RMSE: 0.9718 - val_loss: 0.9955 - val_RMSE: 0.9955\n",
      "Epoch 5/30\n",
      " - 6s - loss: 0.9664 - RMSE: 0.9664 - val_loss: 1.0012 - val_RMSE: 1.0012\n",
      "Epoch 6/30\n",
      " - 6s - loss: 0.9630 - RMSE: 0.9630 - val_loss: 1.0075 - val_RMSE: 1.0075\n",
      "RMSE: 680.949463547373 | SUM ERROR: 19608906.988031838\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot insert level_0, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fe9e65a42b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mdf_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mdf_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mdf_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdf_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"DF Stack Shape: {df_stack.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill)\u001b[0m\n\u001b[1;32m   3377\u001b[0m                 \u001b[0;31m# to ndarray and maybe infer different dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3378\u001b[0m                 \u001b[0mlevel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_casted_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3379\u001b[0;31m                 \u001b[0mnew_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m         \u001b[0mnew_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   2611\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2612\u001b[0m         self._data.insert(loc, column, value,\n\u001b[0;32m-> 2613\u001b[0;31m                           allow_duplicates=allow_duplicates)\n\u001b[0m\u001b[1;32m   2614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4061\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4062\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot insert {}, already exists'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4065\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert level_0, already exists"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nn_keras import mercari_1st_NN, corp_1st_LSTM\n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "N_EPOCHS = 15\n",
    "N_EPOCHS = 30\n",
    "# N_EPOCHS = 10\n",
    "# batch_size = 65536\n",
    "# batch_size = 1024\n",
    "# batch_size = 512\n",
    "batch_size = 256\n",
    "batch_size = 128\n",
    "# learning_rate = 1e-5\n",
    "# learning_rate = 1e-4\n",
    "learning_rate = 1e-3\n",
    "# lerning_rate = 3e-3\n",
    "\n",
    "break_cnt=0\n",
    "first_batch=7 # 7: 128\n",
    "\n",
    "if is_LSTM:\n",
    "    model = corp_1st_LSTM(input_rows=1, input_cols=len(use_cols))\n",
    "else:\n",
    "    model = mercari_1st_NN(input_cols=len(use_cols))\n",
    "\n",
    "opt = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss=RMSE, optimizer=opt, metrics=[RMSE])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "]\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Result Box\n",
    "model_list = []\n",
    "result_list = []\n",
    "score_list = []\n",
    "val_pred_list = []\n",
    "test_pred = np.zeros(len(test))\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "\n",
    "for fold_no, (trn_idx, val_idx) in enumerate(zip(*kfold)):\n",
    "\n",
    "    #========================================================================\n",
    "    # Make Dataset\n",
    "    X_train, y_train = train.loc[train[key].isin(trn_idx), :][use_cols], Y.loc[train[key].isin(trn_idx)]\n",
    "    X_val, y_val = train.loc[train[key].isin(val_idx), :][use_cols], Y.loc[train[key].isin(val_idx)]\n",
    "    \n",
    "    X_train[:] = scaler.transform(X_train)\n",
    "    X_val[:] = scaler.transform(X_val)\n",
    "    X_train = X_train.as_matrix()\n",
    "    X_val = X_val.as_matrix()\n",
    "    \n",
    "    if is_LSTM:\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    #========================================================================\n",
    "    \n",
    "    cnt = -1\n",
    "    while True:\n",
    "        cnt+=1\n",
    "        # Fitting\n",
    "        # なぜか平均を引いてる？そのほうがfitするの？\n",
    "        # model.fit(X_train, y- y_mean, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "        #            validation_data=(X_val, y_val - y_mean), callbacks=callbacks )\n",
    "    #     model.fit(X_train, y_train, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "    #                validation_data=(X_val, y_val), callbacks=callbacks )\n",
    "        \n",
    "        model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val)\n",
    "                  , batch_size=2**(first_batch + cnt), epochs=N_EPOCHS\n",
    "                  , verbose=2, callbacks=callbacks)\n",
    "\n",
    "        # Prediction\n",
    "        if is_oof:\n",
    "            val_id_list = list(set(train_ids) - set(trn_idx))\n",
    "            X_val = train.loc[train[key].isin(val_id_list), :][use_cols]\n",
    "            y_val = Y.loc[train[key].isin(val_id_list)]\n",
    "            y_pred = model.predict(X_val)\n",
    "            tmp_val = train.loc[train[key].isin(val_id_list), :][[key, target]].set_index(key)\n",
    "            tmp_val['prediction'] = y_pred\n",
    "            df_stack['pred_{fold_no}'] = tmp_val['prediction']\n",
    "            del tmp_val\n",
    "            gc.collect()\n",
    "        else:\n",
    "            y_pred = model.predict(X_val)\n",
    "        out_cnt = (y_pred==np.inf).sum()+(y_pred==-np.inf).sum()+(y_pred!=y_pred).sum()\n",
    "        \n",
    "        if out_cnt>0:\n",
    "            print(\"Exist Inf or NaN\")\n",
    "            sys.exit()\n",
    "            \n",
    "        break\n",
    "    \n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], )\n",
    "    tmp_pred = model.predict(x_test)\n",
    "    test_pred += tmp_pred.reshape(tmp_pred.shape[0], )\n",
    "    test['prediction'] = test_pred\n",
    "    test = test[[key, 'prediction']]\n",
    "    \n",
    "    if is_plus:\n",
    "        y_val += (y_min-1)\n",
    "        y_pred += (y_min-1)\n",
    "        test_pred += (y_min-1)\n",
    "#     model_list.append(model)\n",
    "    \n",
    "    # Stack Prediction\n",
    "#     df_pred = train.loc[train[key].isin(val_idx), :][[key, target]].copy()\n",
    "#     df_pred['prediction'] = y_pred\n",
    "#     result_list.append(df_pred)\n",
    "    \n",
    "    # Scoring\n",
    "    err = (y_val - y_pred)\n",
    "    score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "    score_list.append(score)\n",
    "    #========================================================================\n",
    "\n",
    "if is_oof:\n",
    "    pred_cols = [col for col in df_stack.columns if col.count('pred_')]\n",
    "    df_stack['prediction'] = df_stack[pred_cols].mean(axis=1)\n",
    "    df_stack.drop(pred_cols, axis=1, inplace=True)\n",
    "    df_stack.reset_index(inplace=True)\n",
    "    df_stack = pd.concat([df_stack, test], axis=0, ignore_index=True)\n",
    "    print(f\"DF Stack Shape: {df_stack.shape}\")    \n",
    "\n",
    "cv_score = np.mean(score_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "#========================================================================\n",
    "# Stacking\n",
    "test_pred /= fold\n",
    "test['prediction'] = test_pred\n",
    "stack_test = test[[key, 'prediction']]\n",
    "result_list.append(stack_test)\n",
    "df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "df_pred = base.merge(df_pred, how='inner', on=key)\n",
    "print(f\"Stacking Shape: {df_pred.shape}\")\n",
    "\n",
    "if is_oof:\n",
    "    utils.to_pkl_gzip(obj=df_stack, path=f'../stack/{start_time[4:12]}_elo_NN_stack_E{set_no+1}_linear1_{len(use_cols)}feat_const{const_cnt}_lr{learning_rate}_batch{batch_size}_epoch{N_EPOCHS}_CV{cv_score}')\n",
    "#========================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
