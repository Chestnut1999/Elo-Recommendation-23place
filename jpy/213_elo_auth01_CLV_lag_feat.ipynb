{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "import os\n",
    "import sys\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import get_categorical_features, get_numeric_features, reduce_mem_usage\n",
    "from preprocessing import get_dummies\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "os.listdir('../input/')\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_avtive_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 41.13it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 77.88it/s]\n",
      "100%|██████████| 3/3 [00:14<00:00,  4.93s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n",
      "Mem. usage decreased to 2554.26 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 183.47 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = utils.read_df_pkl('../input/train0*')\n",
    "df_test = utils.read_df_pkl('../input/test0*')\n",
    "df_train.set_index(key, inplace=True)\n",
    "df_test.set_index(key, inplace=True)\n",
    "train_test = pd.concat([df_train, df_test], axis=0)\n",
    "df_hist = utils.read_df_pkl('../input/hist_clean*')\n",
    "df_new = utils.read_df_pkl('../input/new_clean*')\n",
    "\n",
    "df_train = utils.reduce_mem_usage(df_train)\n",
    "df_test  = utils.reduce_mem_usage(df_test )\n",
    "df_hist  = utils.reduce_mem_usage(df_hist )\n",
    "df_new   = utils.reduce_mem_usage(df_new  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_utils import date_add_days\n",
    "\n",
    "hist_max_date = df_hist.groupby(key)['purchase_date'].max()\n",
    "hist_min_date = df_hist.groupby(key)['purchase_date'].min()\n",
    "hist_max_date.name = 'hist_purchase_date_max'\n",
    "hist_min_date.name = 'hist_purchase_date_min'\n",
    "hist_max_date = hist_max_date.to_frame()\n",
    "hist_min_date = hist_min_date.to_frame()\n",
    "\n",
    "# month_maxは翌月の数字にする\n",
    "hist_max_date['hist_purchase_month_max'] = hist_max_date['hist_purchase_date_max'].map(lambda x: date_add_days(x, 32) if int(str(x)[8:10])<=15 else date_add_days(x, 20))\n",
    "hist_max_date['hist_purchase_month_max'] = hist_max_date['hist_purchase_month_max'].map(lambda x: str(x)[:7])\n",
    "\n",
    "hist_min_date['hist_purchase_month_min'] = hist_min_date['hist_purchase_date_min'].map(lambda x: str(x)[:7])\n",
    "\n",
    "\n",
    "new_max_date = df_new.groupby(key)['purchase_date'].max()\n",
    "new_min_date = df_new.groupby(key)['purchase_date'].min()\n",
    "new_max_date.name = 'new_purchase_date_max'\n",
    "new_min_date.name = 'new_purchase_date_min'\n",
    "new_max_date = new_max_date.to_frame()\n",
    "new_min_date = new_min_date.to_frame()\n",
    "\n",
    "# month_maxは翌月の数字にする\n",
    "new_max_date['new_purchase_month_max'] = new_max_date['new_purchase_date_max'].map(lambda x: date_add_days(x, 32) if int(str(x)[8:10])<=15 else date_add_days(x, 20))\n",
    "new_max_date['new_purchase_month_max'] = new_max_date['new_purchase_month_max'].map(lambda x: str(x)[:7])\n",
    "new_min_date['new_purchase_month_min'] = new_min_date['new_purchase_date_min'].map(lambda x: str(x)[:7])\n",
    "\n",
    "df_hist.set_index(key, inplace=True)\n",
    "df_new.set_index(key, inplace=True)\n",
    "df_hist = df_hist.join(hist_max_date).join(hist_min_date).join(new_max_date).join(new_min_date)\n",
    "df_new = df_new.join(hist_max_date).join(hist_min_date).join(new_max_date).join(new_min_date)\n",
    "\n",
    "df_hist['hist_purchase_month_max'] = pd.to_datetime(df_hist['hist_purchase_month_max'])\n",
    "df_hist['hist_purchase_month_min'] = pd.to_datetime(df_hist['hist_purchase_month_min'])\n",
    "df_hist['new_purchase_month_max'] = pd.to_datetime(df_hist['new_purchase_month_max'])\n",
    "df_hist['new_purchase_month_min'] = pd.to_datetime(df_hist['new_purchase_month_min'])\n",
    "\n",
    "df_new['new_purchase_month_max'] = pd.to_datetime(df_new['new_purchase_month_max'])\n",
    "df_new['new_purchase_month_min'] = pd.to_datetime(df_new['new_purchase_month_min'])\n",
    "df_new['hist_purchase_month_max'] = pd.to_datetime(df_new['hist_purchase_month_max'])\n",
    "df_new['hist_purchase_month_min'] = pd.to_datetime(df_new['hist_purchase_month_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>hist_purchase_date_max</th>\n",
       "      <th>hist_purchase_month_max</th>\n",
       "      <th>hist_regist_term</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_ID_92a2005557</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2018-02-25 09:31:15</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_3d0044924f</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2018-01-31 22:31:09</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.392822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_d639edf6cd</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>2018-02-27 19:08:25</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.687988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_186d6a6901</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2018-02-28 11:44:40</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.142456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_cdbd2c0db2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2018-02-28 20:40:41</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.159790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature_1  feature_2  feature_3 first_active_month  \\\n",
       "card_id                                                               \n",
       "C_ID_92a2005557          5          2          1         2017-06-01   \n",
       "C_ID_3d0044924f          4          1          0         2017-01-01   \n",
       "C_ID_d639edf6cd          2          2          0         2016-08-01   \n",
       "C_ID_186d6a6901          4          3          0         2017-09-01   \n",
       "C_ID_cdbd2c0db2          1          3          0         2017-11-01   \n",
       "\n",
       "                hist_purchase_date_max hist_purchase_month_max  \\\n",
       "card_id                                                          \n",
       "C_ID_92a2005557    2018-02-25 09:31:15              2018-03-01   \n",
       "C_ID_3d0044924f    2018-01-31 22:31:09              2018-02-01   \n",
       "C_ID_d639edf6cd    2018-02-27 19:08:25              2018-03-01   \n",
       "C_ID_186d6a6901    2018-02-28 11:44:40              2018-03-01   \n",
       "C_ID_cdbd2c0db2    2018-02-28 20:40:41              2018-03-01   \n",
       "\n",
       "                 hist_regist_term    target  \n",
       "card_id                                      \n",
       "C_ID_92a2005557               9.0 -0.820312  \n",
       "C_ID_3d0044924f              13.0  0.392822  \n",
       "C_ID_d639edf6cd              19.0  0.687988  \n",
       "C_ID_186d6a6901               6.0  0.142456  \n",
       "C_ID_cdbd2c0db2               4.0 -0.159790  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.join(hist_max_date)\n",
    "df_test = df_test.join(hist_max_date)\n",
    "df_train['hist_purchase_month_max'] = pd.to_datetime(df_train['hist_purchase_month_max'])\n",
    "df_train['first_active_month'] = pd.to_datetime(df_train['first_active_month'])\n",
    "df_test['hist_purchase_month_max'] = pd.to_datetime(df_test['hist_purchase_month_max'])\n",
    "df_test['first_active_month'] = pd.to_datetime(df_test['first_active_month'])\n",
    "df_train['hist_regist_term']= (df_train['hist_purchase_month_max'].map(lambda x: x.year) - df_train['first_active_month'].map(lambda x: x.year)) * 12 + (df_train['hist_purchase_month_max'].map(lambda x: x.month) - df_train['first_active_month'].map(lambda x: x.month))\n",
    "df_test['hist_regist_term']= (df_test['hist_purchase_month_max'].map(lambda x: x.year) - df_test['first_active_month'].map(lambda x: x.year)) * 12 + (df_test['hist_purchase_month_max'].map(lambda x: x.month) - df_test['first_active_month'].map(lambda x: x.month))\n",
    "train_test = pd.concat([df_train, df_test], axis=0)\n",
    "\n",
    "train_test_hist_term = train_test.copy()\n",
    "train_test_hist_term.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26595452, 28)\n",
      "(2516909, 28)\n"
     ]
    }
   ],
   "source": [
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "df_new_lag1 = df_new[df_new['month_lag']==1]\n",
    "df_new_lag2 = df_new[df_new['month_lag']==2]\n",
    "\n",
    "df = df_hist\n",
    "\n",
    "auth1 = df[df.authorized_flag==1]\n",
    "auth0 = df[df.authorized_flag==0]\n",
    "print(auth1.shape)\n",
    "print(auth0.shape)\n",
    "\n",
    "cat1_0 = False\n",
    "# cat1_0 = True\n",
    "if cat1_0:\n",
    "    auth1_cat1 = auth1[auth1.category_1==1]\n",
    "    auth1_cat0 = auth1[auth1.category_1==0]\n",
    "    auth0_cat1 = auth0[auth0.category_1==1]\n",
    "    auth0_cat0 = auth0[auth0.category_1==0]\n",
    "    new_cat1 = df_new[df_new.category_1==1]\n",
    "    new_cat0 = df_new[df_new.category_1==0]\n",
    "#     del auth1, auth0\n",
    "#     gc.collect()\n",
    "\n",
    "# auth1_lag0 = auth1[auth1['month_lag']==0]\n",
    "# auth1_lag1 = auth1[auth1['month_lag']==-1]\n",
    "# auth1_lag2 = auth1[auth1['month_lag']==-2]\n",
    "# auth1_lag02 = auth1[auth1['month_lag']>=-2]\n",
    "# auth1_lag05 = auth1[auth1['month_lag']>=-5]\n",
    "auth1_lag08 = auth1[auth1['month_lag']>=-8]\n",
    "auth1_lag011 = auth1[auth1['month_lag']>=-11]\n",
    "\n",
    "# auth0_lag0 = auth0[auth0['month_lag']==0]\n",
    "# auth0_lag1 = auth0[auth0['month_lag']==-1]\n",
    "# auth0_lag2 = auth0[auth0['month_lag']==-2]\n",
    "# auth0_lag02 = auth0[auth0['month_lag']>=-2]\n",
    "# auth0_lag05 = auth0[auth0['month_lag']>=-5]\n",
    "auth0_lag08 = auth0[auth0['month_lag']>=-8]\n",
    "auth0_lag011 = auth0[auth0['month_lag']>=-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 88.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 160.56it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Feature.\n",
      "(325540, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 1/3 [00:39<01:18, 39.12s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 90.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 160.28it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Feature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 2/3 [01:13<00:37, 37.57s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 25.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325540, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 54.05it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Feature.\n"
     ]
    }
   ],
   "source": [
    "prefix = '213_pst'\n",
    "new_df_list = [df_new, df_new_lag1, df_new_lag2]\n",
    "new_fname_list = ['new', 'new_lag1', 'new_lag2']\n",
    "\n",
    "debug = False\n",
    "if debug:\n",
    "    train_test = train_test.head(10000)\n",
    "        \n",
    "\n",
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# 集計が必要なFeature\n",
    "#========================================================================\n",
    "for df, fname in zip(tqdm(new_df_list), new_fname_list):\n",
    "    if debug:\n",
    "        df = df.head(3000)\n",
    "    \n",
    "    df_train = utils.read_df_pkl('../input/train0*')\n",
    "    df_test = utils.read_df_pkl('../input/test0*')\n",
    "    df_train.set_index(key, inplace=True)\n",
    "    df_test.set_index(key, inplace=True)\n",
    "    train_test = pd.concat([df_train, df_test], axis=0)\n",
    "    \n",
    "    # new\n",
    "    print('Make Feature.')\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "#     df['price'] = df['purchase_amount'] / df['installments']\n",
    "    \n",
    "    aggs = {}\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    df['duration'] = df['purchase_amount']*df['month_diff']\n",
    "    aggs['duration'] = ['sum', 'mean', 'std', 'max', 'skew']\n",
    "    \n",
    "#     aggs['purchase_amount'] = ['sum']\n",
    "#     aggs['card_id'] = ['count']\n",
    "    \n",
    "    new_columns = get_new_columns(fname, aggs)\n",
    "    df_agg = df.groupby(key).agg(aggs)\n",
    "    df_agg.columns = new_columns\n",
    "    \n",
    "    train_test = train_test.join(df_agg)\n",
    "    print(train_test.shape)\n",
    "    \n",
    "#     if not(fname.count('lag')):\n",
    "#         train_test[f'{fname}_CLV'] = train_test[f'{fname}_card_id_count'] * train_test[f'{fname}_purchase_amount_sum'] / 2\n",
    "#     else:\n",
    "#         train_test[f'{fname}_CLV'] = train_test[f'{fname}_card_id_count'] * train_test[f'{fname}_purchase_amount_sum'] / 1\n",
    "\n",
    "    #========================================================================\n",
    "    # Save Feature\n",
    "    ignore_features = ['first_active_month', 'card_id', target]\n",
    "    for col in train_test.columns:\n",
    "        if col in ignore_features: continue\n",
    "        if col.count('feature_'):continue\n",
    "        if not(col.count('dura')):continue\n",
    "        feature = train_test[col].astype('float32').values\n",
    "        col = col.replace('.', '_')\n",
    "    #     utils.to_pkl_gzip(path = f'../features/1_first_valid/{prefix}_{col}@', obj=feature)\n",
    "        utils.to_pkl_gzip(path = f'../features/3_third_valid/{prefix}_{col}@', obj=feature)\n",
    "    #========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Feature.\n",
      "(325540, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:45<00:45, 45.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Feature.\n",
      "(325540, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [01:37<00:00, 47.45s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "prefix = '213_pst'\n",
    "hist_df_list = [\n",
    "#     auth1, auth1_lag0, auth1_lag1, auth1_lag2, auth1_lag02, auth1_lag05\n",
    "                auth1_lag08, auth1_lag011,\n",
    "]\n",
    "#                 auth0, auth0_lag0, auth0_lag1, auth0_lag2, auth0_lag02, auth0_lag05] \n",
    "hist_fname_list = [\n",
    "#     'auth1', 'auth1_lag0', 'auth1_lag1', 'auth1_lag2', 'auth1_lag02', 'auth1_lag05'\n",
    "                   'auth1_lag08', 'auth1_lag011',\n",
    "]\n",
    "#                    'auth0', 'auth0_lag0', 'auth0_lag1', 'auth0_lag2', 'auth0_lag02', 'auth0_lag05', 'auth0_lag08', 'auth0_lag011',]\n",
    "\n",
    "for df, fname in zip(tqdm(hist_df_list), hist_fname_list):\n",
    "    if debug:\n",
    "        df = df.head(3000)\n",
    "    \n",
    "#     df_train = utils.read_df_pkl('../input/train0*')\n",
    "#     df_test = utils.read_df_pkl('../input/test0*')\n",
    "#     df_train.set_index(key, inplace=True)\n",
    "#     df_test.set_index(key, inplace=True)\n",
    "#     train_test = pd.concat([df_train, df_test], axis=0)\n",
    "    train_test = train_test_hist_term.copy()\n",
    "    \n",
    "#     hist\n",
    "    print('Make Feature.')\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    aggs = {}\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    df['duration'] = df['purchase_amount']*df['month_diff']\n",
    "    aggs['duration'] = ['sum', 'mean', 'std', 'max', 'skew']\n",
    "    \n",
    "    aggs['purchase_amount'] = ['sum']\n",
    "    aggs['card_id'] = ['count']\n",
    "    \n",
    "    new_columns = get_new_columns(fname, aggs)\n",
    "    df_agg = df.groupby(key).agg(aggs)\n",
    "    df_agg.columns = new_columns\n",
    "    \n",
    "    train_test = train_test.join(df_agg)\n",
    "    print(train_test.shape)\n",
    "    \n",
    "    if not(fname.count('lag')):\n",
    "        train_test[f'{fname}_CLV'] = train_test[f'{fname}_card_id_count'] * train_test[f'{fname}_purchase_amount_sum'] / train_test['hist_regist_term']\n",
    "    elif fname[-4:-1]=='lag':\n",
    "        train_test[f'{fname}_CLV'] = train_test[f'{fname}_card_id_count'] * train_test[f'{fname}_purchase_amount_sum'] / 1\n",
    "    elif fname[-2:]=='02':\n",
    "        train_test[f'{fname}_CLV'] = train_test[f'{fname}_card_id_count'] * train_test[f'{fname}_purchase_amount_sum'] / 3\n",
    "    elif fname[-2:]=='05':\n",
    "        train_test[f'{fname}_CLV'] = train_test[f'{fname}_card_id_count'] * train_test[f'{fname}_purchase_amount_sum'] / 6\n",
    "    elif fname[-2:]=='08':\n",
    "        train_test[f'{fname}_CLV'] = train_test[f'{fname}_card_id_count'] * train_test[f'{fname}_purchase_amount_sum'] / 9\n",
    "    elif fname[-2:]=='11':\n",
    "        train_test[f'{fname}_CLV'] = train_test[f'{fname}_card_id_count'] * train_test[f'{fname}_purchase_amount_sum'] / 12\n",
    "    \n",
    "    #========================================================================\n",
    "    # Save Feature\n",
    "    ignore_features = ['first_active_month', 'card_id', target]\n",
    "    for col in train_test.columns:\n",
    "        if col in ignore_features: continue\n",
    "        if col.count('feature_'):continue\n",
    "        if not(col.count('durat')) and not(col.count('CLV')):continue\n",
    "        feature = train_test[col].astype('float32').values\n",
    "        col = col.replace('.', '_')\n",
    "#         utils.to_pkl_gzip(path = f'../features/1_first_valid/{prefix}_{col}@', obj=feature)\n",
    "        utils.to_pkl_gzip(path = f'../features/3_third_valid/{prefix}_{col}@', obj=feature)\n",
    "    #========================================================================    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 3/3 [00:00<00:00, 27.89it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00, 151.77it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325540, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>213_pst_auth1_lag011_CLV@</th>\n",
       "      <th>213_pst_new_lag1_CLV@</th>\n",
       "      <th>213_pst_new_lag2_CLV@</th>\n",
       "      <th>213_pst_auth1_lag1_CLV@</th>\n",
       "      <th>213_pst_auth1_CLV@</th>\n",
       "      <th>213_pst_auth1_lag02_CLV@</th>\n",
       "      <th>213_pst_auth1_lag2_CLV@</th>\n",
       "      <th>213_pst_new_CLV@</th>\n",
       "      <th>213_pst_auth1_lag0_CLV@</th>\n",
       "      <th>213_pst_auth1_lag05_CLV@</th>\n",
       "      <th>213_pst_auth1_lag08_CLV@</th>\n",
       "      <th>ratio_new_auth1_lag08_CLV</th>\n",
       "      <th>diff_new_auth1_lag08_CLV</th>\n",
       "      <th>ratio_new_auth1_lag011_CLV</th>\n",
       "      <th>diff_new_auth1_lag011_CLV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>537.096375</td>\n",
       "      <td>27.304688</td>\n",
       "      <td>18.304688</td>\n",
       "      <td>44.994141</td>\n",
       "      <td>716.128479</td>\n",
       "      <td>495.257812</td>\n",
       "      <td>617.890625</td>\n",
       "      <td>45.303711</td>\n",
       "      <td>37.464844</td>\n",
       "      <td>538.687500</td>\n",
       "      <td>716.128479</td>\n",
       "      <td>0.063262</td>\n",
       "      <td>-670.824768</td>\n",
       "      <td>0.084349</td>\n",
       "      <td>-491.792664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>954.828125</td>\n",
       "      <td>0.109955</td>\n",
       "      <td>0.270996</td>\n",
       "      <td>151.523438</td>\n",
       "      <td>1067.524048</td>\n",
       "      <td>289.625000</td>\n",
       "      <td>21.687500</td>\n",
       "      <td>0.380859</td>\n",
       "      <td>154.792969</td>\n",
       "      <td>493.177094</td>\n",
       "      <td>767.059021</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>-766.678162</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>-954.447266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>5.532227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>1.321289</td>\n",
       "      <td>6.031147</td>\n",
       "      <td>1.046875</td>\n",
       "      <td>0.039124</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.085632</td>\n",
       "      <td>2.248698</td>\n",
       "      <td>2.389323</td>\n",
       "      <td>0.009809</td>\n",
       "      <td>-2.365885</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>-5.508789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>50.230469</td>\n",
       "      <td>0.423828</td>\n",
       "      <td>1.813965</td>\n",
       "      <td>4.092773</td>\n",
       "      <td>100.460938</td>\n",
       "      <td>22.604166</td>\n",
       "      <td>3.266602</td>\n",
       "      <td>2.011475</td>\n",
       "      <td>19.125000</td>\n",
       "      <td>100.460938</td>\n",
       "      <td>66.973961</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>-64.962486</td>\n",
       "      <td>0.040045</td>\n",
       "      <td>-48.218994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>212.166672</td>\n",
       "      <td>37.562500</td>\n",
       "      <td>92.421875</td>\n",
       "      <td>221.203125</td>\n",
       "      <td>636.500000</td>\n",
       "      <td>535.557312</td>\n",
       "      <td>100.689453</td>\n",
       "      <td>125.437500</td>\n",
       "      <td>215.625000</td>\n",
       "      <td>424.333344</td>\n",
       "      <td>282.888885</td>\n",
       "      <td>0.443416</td>\n",
       "      <td>-157.451385</td>\n",
       "      <td>0.591222</td>\n",
       "      <td>-86.729172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id  213_pst_auth1_lag011_CLV@  213_pst_new_lag1_CLV@  \\\n",
       "0  C_ID_92a2005557                 537.096375              27.304688   \n",
       "1  C_ID_3d0044924f                 954.828125               0.109955   \n",
       "2  C_ID_d639edf6cd                   5.532227                    NaN   \n",
       "3  C_ID_186d6a6901                  50.230469               0.423828   \n",
       "4  C_ID_cdbd2c0db2                 212.166672              37.562500   \n",
       "\n",
       "   213_pst_new_lag2_CLV@  213_pst_auth1_lag1_CLV@  213_pst_auth1_CLV@  \\\n",
       "0              18.304688                44.994141          716.128479   \n",
       "1               0.270996               151.523438         1067.524048   \n",
       "2               0.046875                 1.321289            6.031147   \n",
       "3               1.813965                 4.092773          100.460938   \n",
       "4              92.421875               221.203125          636.500000   \n",
       "\n",
       "   213_pst_auth1_lag02_CLV@  213_pst_auth1_lag2_CLV@  213_pst_new_CLV@  \\\n",
       "0                495.257812               617.890625         45.303711   \n",
       "1                289.625000                21.687500          0.380859   \n",
       "2                  1.046875                 0.039124          0.023438   \n",
       "3                 22.604166                 3.266602          2.011475   \n",
       "4                535.557312               100.689453        125.437500   \n",
       "\n",
       "   213_pst_auth1_lag0_CLV@  213_pst_auth1_lag05_CLV@  \\\n",
       "0                37.464844                538.687500   \n",
       "1               154.792969                493.177094   \n",
       "2                 0.085632                  2.248698   \n",
       "3                19.125000                100.460938   \n",
       "4               215.625000                424.333344   \n",
       "\n",
       "   213_pst_auth1_lag08_CLV@  ratio_new_auth1_lag08_CLV  \\\n",
       "0                716.128479                   0.063262   \n",
       "1                767.059021                   0.000497   \n",
       "2                  2.389323                   0.009809   \n",
       "3                 66.973961                   0.030034   \n",
       "4                282.888885                   0.443416   \n",
       "\n",
       "   diff_new_auth1_lag08_CLV  ratio_new_auth1_lag011_CLV  \\\n",
       "0               -670.824768                    0.084349   \n",
       "1               -766.678162                    0.000399   \n",
       "2                 -2.365885                    0.004237   \n",
       "3                -64.962486                    0.040045   \n",
       "4               -157.451385                    0.591222   \n",
       "\n",
       "   diff_new_auth1_lag011_CLV  \n",
       "0                -491.792664  \n",
       "1                -954.447266  \n",
       "2                  -5.508789  \n",
       "3                 -48.218994  \n",
       "4                 -86.729172  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "# #========================================================================\n",
    "# # これは作成済のFeatureを読み込んで計算する\n",
    "# #========================================================================\n",
    "combi_list = [\n",
    "#     ['new', 'new_lag1'] ,['new', 'auth1'] ,['new', 'auth1_lag0'] ,['new', 'auth1_lag02'] \n",
    "#               ,['new', 'auth1_lag05']  \n",
    "    ['new', 'auth1_lag08']  ,['new', 'auth1_lag011'] \n",
    "#               ,['new', 'auth0_lag0'] , ['new', 'auth0_lag02'] ,['new', 'auth0_lag05']\n",
    "#               ,['auth1_lag0', 'auth1_lag02'] \n",
    "#               ,['auth1_lag0', 'auth1_lag05'] \n",
    "#               ,['auth1_lag0', 'auth1_lag08'] \n",
    "#               ,['auth1_lag0', 'auth1_lag011'] \n",
    "#               ,['auth1_lag02', 'auth1_lag05']\n",
    "#               ['auth1_lag02', 'auth1_lag08']\n",
    "#               ,['auth1_lag02', 'auth1_lag011']\n",
    "#               ,['auth1_lag05', 'auth1_lag08']\n",
    "#               ,['auth1_lag05', 'auth1_lag011']\n",
    "]\n",
    "\n",
    "comp_cols = [\n",
    "     'CLV'\n",
    "]\n",
    "\n",
    "tmp_feature_list = glob.glob('../features/1_first_valid/213_*.gz') + glob.glob('../features/4_winner/213_*.gz')\n",
    "feature_list = []\n",
    "for f in tmp_feature_list:\n",
    "    if f.count('pst_ratio_') or f.count('pst_diff_'):continue\n",
    "    for col in comp_cols:\n",
    "        if f.count(col):\n",
    "            feature_list.append(f)\n",
    "            \n",
    "base = utils.read_df_pkl('../input/base_first*0*')\n",
    "p_list = utils.parallel_load_data(path_list=feature_list)\n",
    "df_feat = pd.concat(p_list, axis=1)\n",
    "train_test = pd.concat([base[key], df_feat], axis=1)\n",
    "\n",
    "for (fm1, fm2) in tqdm(combi_list):\n",
    "    for col in comp_cols:\n",
    "#         203_pst_auth0_lag02_monthly_avg_purchase_amount_min\n",
    "        try:\n",
    "            train_test[f\"ratio_{fm1}_{fm2}_{col}\"] = train_test[f\"{prefix}_{fm1}_{col}@\"] / train_test[f\"{prefix}_{fm2}_{col}@\"]\n",
    "            train_test[f\"diff_{fm1}_{fm2}_{col}\"] = train_test[f\"{prefix}_{fm1}_{col}@\"] - train_test[f\"{prefix}_{fm2}_{col}@\"]\n",
    "        except KeyError:\n",
    "            print(fm1, fm2, col)\n",
    "            continue\n",
    "print(train_test.shape)\n",
    "train_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Feature\n",
    "\n",
    "for col in train_test.columns:\n",
    "    if col in ignore_features: continue\n",
    "    if col.count('213_'):continue\n",
    "    if not(col.count('ratio')) and not(col.count('diff_')):continue\n",
    "    feature = train_test[col].astype('float32').values\n",
    "    if np.std(feature)==0:\n",
    "        continue\n",
    "    \n",
    "#     utils.to_pkl_gzip(path = f'../features/1_first_valid/{prefix}_{col}@', obj=feature)\n",
    "    utils.to_pkl_gzip(path = f'../features/3_third_valid/{prefix}_{col}@', obj=feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
