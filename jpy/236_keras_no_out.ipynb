{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 37.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "is_oof = True\n",
    "is_LSTM = False\n",
    "is_plus = [True, False][1]\n",
    "out_part = ['', 'no_out'][1]\n",
    "set_no = [0,1,2,3][1]\n",
    "const_cnt = 1\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/model')\n",
    "from nn_keras import mercari_1st_NN, corp_1st_LSTM\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg']\n",
    "stack_name='keras'\n",
    "model_type='keras'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "print(\"Preparing dataset...\")\n",
    "# win_path = f'../features/4_winner/*.gz'\n",
    "# Ensemble 1\n",
    "set1 = f'../model/E1_set/*.gz'\n",
    "# Ensemble 2\n",
    "set2 = f'../model/E2_set/*.gz'\n",
    "# Ensemble 3\n",
    "set3 = f'../model/E3_set/*.gz'\n",
    "# Ensemble 4\n",
    "set4 = f'../model/E4_set/*.gz'\n",
    "\n",
    "set_list = [set1, set2, set3, set4]\n",
    "win_path = set_list[set_no]\n",
    "\n",
    "win_path_list = glob.glob(win_path)\n",
    "\n",
    "base = utils.read_df_pkl('../input/base_term*0*')[[key, target, 'first_active_month']]\n",
    "base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
    "df = pd.concat(feature_list, axis=1)\n",
    "train = pd.concat([base_train, df.iloc[:len(base_train), :]], axis=1)\n",
    "test = pd.concat([base_test, df.iloc[len(base_train):, :].reset_index(drop=True)], axis=1)\n",
    "\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True , drop=True)\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "\n",
    "# Stacking base\n",
    "train_ids = train[key].values\n",
    "df_stack = train[[key, target]].copy().set_index(key)\n",
    "\n",
    "if out_part=='no_out':\n",
    "    train = train[train[target]>-30]\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n",
      "Ramain inf...\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# 正規化の前処理(Null埋め, inf, -infの処理) \n",
    "for col in train.columns:\n",
    "    if col in ignore_list: continue\n",
    "        \n",
    "    train[col] = impute_feature(train, col)\n",
    "    test[col] = impute_feature(test, col)\n",
    "\n",
    "while True:\n",
    "    inf_list = []\n",
    "    for col in use_cols:\n",
    "        tmp = (train[col]==np.inf).sum()\n",
    "        tmp2 = (train[col]==-np.inf).sum()\n",
    "        if tmp>0:\n",
    "            inf_list.append(col)\n",
    "        if tmp2>0:\n",
    "            inf_list.append(col)\n",
    "        \n",
    "    for col in inf_list:\n",
    "        print(\"Ramain inf...\")\n",
    "        train[col] = impute_feature(train, col)\n",
    "        test[col] = impute_feature(test, col)\n",
    "    if len(inf_list)==0:\n",
    "        break\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1ab2de7cf1cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muse_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muse_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muse_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \"\"\"\n\u001b[1;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m--> 612\u001b[0;31m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# Construction Value Range Preprocessing\n",
    "def constraction(feature, is_viz=False, out_range=1.64):\n",
    "    if is_viz:\n",
    "        print('before:', feature.max(), feature.min())\n",
    "    std = feature.std()\n",
    "    avg = feature.mean()\n",
    "    z_val = (feature - avg)/std\n",
    "    \n",
    "    # Pass the Case No Outlier\n",
    "    try:\n",
    "        p_min = feature[feature>=out_range].min()\n",
    "        feature = np.where(z_val>=out_range, p_min, feature)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        m_max = feature[feature<=-1*out_range].max()\n",
    "        feature = np.where(z_val<=-1*out_range, m_max, feature)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    if is_viz:\n",
    "        print('after:', feature.max(), feature.min())   \n",
    "    return feature\n",
    "\n",
    "train_test = pd.concat([train, test], axis=0)\n",
    "for col in use_cols:\n",
    "    feature = train_test[col].values\n",
    "    \n",
    "    for i in range(const_cnt):\n",
    "        feature = constraction(feature)\n",
    "    \n",
    "    feature = feature.astype('float32')\n",
    "    train_test[col] = feature\n",
    "train = train_test[~train_test[target].isnull()]\n",
    "test = train_test[train_test[target].isnull()]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([train[use_cols], test[use_cols]]))   \n",
    "x_test = scaler.transform(test[use_cols])\n",
    "Y = train[target]\n",
    "\n",
    "# ========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# CVの準備\n",
    "fold = 6\n",
    "if out_part != 'no_out':\n",
    "    kfold = utils.read_pkl_gzip('../input/ods_kfold.gz')\n",
    "else:\n",
    "    kfold = utils.read_pkl_gzip('../input/ods_NoOut_kfold.gz')\n",
    "if is_plus:\n",
    "    y_min = Y.min()\n",
    "    Y = Y - (y_min-1)\n",
    "\n",
    "# x_test = x_test.as_matrix()\n",
    "# For LSTM\n",
    "if is_LSTM:\n",
    "    x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "    \n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\") \n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "N_EPOCHS = 15\n",
    "N_EPOCHS = 30\n",
    "# N_EPOCHS = 10\n",
    "# batch_size = 65536\n",
    "# batch_size = 1024\n",
    "# batch_size = 512\n",
    "batch_size = 256\n",
    "batch_size = 128\n",
    "# learning_rate = 1e-5\n",
    "# learning_rate = 1e-4\n",
    "learning_rate = 1e-3\n",
    "# lerning_rate = 3e-3\n",
    "\n",
    "break_cnt=0\n",
    "first_batch=7 # 7: 128\n",
    "\n",
    "if is_LSTM:\n",
    "    model = corp_1st_LSTM(input_rows=1, input_cols=len(use_cols))\n",
    "else:\n",
    "    model = mercari_1st_NN(input_cols=len(use_cols))\n",
    "\n",
    "opt = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss=RMSE, optimizer=opt, metrics=[RMSE])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "]\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Result Box\n",
    "model_list = []\n",
    "result_list = []\n",
    "score_list = []\n",
    "val_pred_list = []\n",
    "test_pred = np.zeros(len(test))\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "\n",
    "for fold_no, (trn_idx, val_idx) in enumerate(zip(*kfold)):\n",
    "\n",
    "    #========================================================================\n",
    "    # Make Dataset\n",
    "    X_train, y_train = train.loc[train[key].isin(trn_idx), :][use_cols], Y.loc[train[key].isin(trn_idx)]\n",
    "    X_val, y_val = train.loc[train[key].isin(val_idx), :][use_cols], Y.loc[train[key].isin(val_idx)]\n",
    "    \n",
    "    X_train[:] = scaler.transform(X_train)\n",
    "    X_val[:] = scaler.transform(X_val)\n",
    "    X_train = X_train.as_matrix()\n",
    "    X_val = X_val.as_matrix()\n",
    "    \n",
    "    if is_LSTM:\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    #========================================================================\n",
    "    \n",
    "    cnt = -1\n",
    "    while True:\n",
    "        cnt+=1\n",
    "        # Fitting\n",
    "        # なぜか平均を引いてる？そのほうがfitするの？\n",
    "        # model.fit(X_train, y- y_mean, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "        #            validation_data=(X_val, y_val - y_mean), callbacks=callbacks )\n",
    "    #     model.fit(X_train, y_train, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "    #                validation_data=(X_val, y_val), callbacks=callbacks )\n",
    "        \n",
    "        model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val)\n",
    "                  , batch_size=2**(first_batch + cnt), epochs=N_EPOCHS\n",
    "                  , verbose=2, callbacks=callbacks)\n",
    "\n",
    "        # Prediction\n",
    "        if is_oof:\n",
    "            val_id_list = list(set(train_ids) - set(trn_idx))\n",
    "            X_val = train.loc[train[key].isin(val_id_list), :][use_cols]\n",
    "            y_val = Y.loc[train[key].isin(val_id_list)]\n",
    "            y_pred = model.predict(X_val)\n",
    "            tmp_val = train.loc[train[key].isin(val_id_list), :][[key, target]].set_index(key)\n",
    "            tmp_val['prediction'] = y_pred\n",
    "            df_stack['pred_{fold_no}'] = tmp_val['prediction']\n",
    "            del tmp_val\n",
    "            gc.collect()\n",
    "        else:\n",
    "            y_pred = model.predict(X_val)\n",
    "        out_cnt = (y_pred==np.inf).sum()+(y_pred==-np.inf).sum()+(y_pred!=y_pred).sum()\n",
    "        \n",
    "        if out_cnt>0:\n",
    "            print(\"Exist Inf or NaN\")\n",
    "            sys.exit()\n",
    "            \n",
    "        break\n",
    "    \n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], )\n",
    "    tmp_pred = model.predict(x_test)\n",
    "    test_pred += tmp_pred.reshape(tmp_pred.shape[0], )\n",
    "    test['prediction'] = test_pred\n",
    "    test = test[[key, 'prediction']]\n",
    "    \n",
    "    if is_plus:\n",
    "        y_val += (y_min-1)\n",
    "        y_pred += (y_min-1)\n",
    "        test_pred += (y_min-1)\n",
    "#     model_list.append(model)\n",
    "    \n",
    "    # Stack Prediction\n",
    "#     df_pred = train.loc[train[key].isin(val_idx), :][[key, target]].copy()\n",
    "#     df_pred['prediction'] = y_pred\n",
    "#     result_list.append(df_pred)\n",
    "    \n",
    "    # Scoring\n",
    "    err = (y_val - y_pred)\n",
    "    score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "    score_list.append(score)\n",
    "    #========================================================================\n",
    "\n",
    "if is_oof:\n",
    "    pred_cols = [col for col in df_stack.columns if col.count('pred_')]\n",
    "    df_stack['prediction'] = df_stack[pred_cols].mean(axis=1)\n",
    "    df_stack.drop(pred_cols, axis=1, inplace=True)\n",
    "    df_stack.reset_index(inplace=True)\n",
    "    df_stack = pd.concat([df_stack, test], axis=0, ignore_index=True)\n",
    "    print(f\"DF Stack Shape: {df_stack.shape}\")    \n",
    "\n",
    "cv_score = np.mean(score_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "#========================================================================\n",
    "# Stacking\n",
    "test_pred /= fold\n",
    "test['prediction'] = test_pred\n",
    "stack_test = test[[key, 'prediction']]\n",
    "result_list.append(stack_test)\n",
    "df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "df_pred = base.merge(df_pred, how='inner', on=key)\n",
    "print(f\"Stacking Shape: {df_pred.shape}\")\n",
    "\n",
    "if is_oof:\n",
    "    utils.to_pkl_gzip(obj=df_stack, path=f'../stack/{start_time[4:12]}_elo_NN_stack_E{set_no+1}_linear1_{len(use_cols)}feat_const{const_cnt}_lr{learning_rate}_batch{batch_size}_epoch{N_EPOCHS}_CV{cv_score}')\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
