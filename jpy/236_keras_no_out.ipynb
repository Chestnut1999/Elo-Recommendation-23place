{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-23 02:04:33,888 utils 400 [INFO]    [logger_func] start \n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "is_LSTM = False\n",
    "is_remain = [True, False][1]\n",
    "is_plus = [True, False][1]\n",
    "out_part = ['all', 'clf_out', 'no_out_flg', 'no_out', 'clf'][0]\n",
    "set_no = [0,1,2,3][1]\n",
    "const_cnt = 1\n",
    "is_oof = [True, False][1]\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import logger_func, get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature, impute_feature\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/model')\n",
    "from nn_keras import mercari_1st_NN, corp_1st_LSTM, RMSE\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Args\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_active_month', 'index', 'personal_term', 'no_out_flg', 'clf_pred']\n",
    "stack_name='keras'\n",
    "model_type='keras'\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "seed = 328\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Data Load \n",
    "print(\"Preparing dataset...\")\n",
    "# win_path = f'../features/4_winner/*.gz'\n",
    "# Ensemble 1\n",
    "set1 = f'../model/E1_set/*.gz'\n",
    "# Ensemble 2\n",
    "set2 = f'../model/E2_set/*.gz'\n",
    "# Ensemble 3\n",
    "set3 = f'../model/E3_set/*.gz'\n",
    "# Ensemble 4\n",
    "set4 = f'../model/E4_set/*.gz'\n",
    "\n",
    "set_list = [set1, set2, set3, set4]\n",
    "win_path = set_list[set_no]\n",
    "\n",
    "win_path_list = glob.glob(win_path)\n",
    "\n",
    "base = utils.read_pkl_gzip('../input/base_no_out_clf.gz')[[key, target, 'first_active_month', 'no_out_flg']]\n",
    "base_train = base[~base[target].isnull()].reset_index(drop=True)\n",
    "base_test = base[base[target].isnull()].reset_index(drop=True)\n",
    "feature_list = utils.parallel_load_data(path_list=win_path_list)\n",
    "df = pd.concat(feature_list, axis=1)\n",
    "train = pd.concat([base_train, df.iloc[:len(base_train), :]], axis=1)\n",
    "test = pd.concat([base_test, df.iloc[len(base_train):, :].reset_index(drop=True)], axis=1)\n",
    "\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True , drop=True)\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "train_ids = train[key].values\n",
    "df_stack = train[[key, target]].copy().set_index(key)\n",
    "\n",
    "if out_part=='clf_out':\n",
    "#     train = train[train[target]>-30]\n",
    "    base_clf = utils.read_pkl_gzip('../input/base_clf.gz')[[key, 'clf_pred']]\n",
    "    train = train.merge(base_clf, on=key, how='inner')\n",
    "    train = train[train['clf_pred']<0.01]\n",
    "    test = test.merge(base_clf, on=key, how='inner')\n",
    "    test = test[test['clf_pred']<0.01]\n",
    "    \n",
    "    train.drop('clf_pred', axis=1, inplace=True)\n",
    "    test.drop('clf_pred', axis=1, inplace=True)\n",
    "elif out_part=='no_out_flg':\n",
    "    train = train[train['no_out_flg']==1]\n",
    "    test = test[test['no_out_flg']==1]\n",
    "    train.drop('no_out_flg', axis=1, inplace=True)\n",
    "    test.drop('no_out_flg', axis=1, inplace=True)\n",
    "elif out_part=='no_out':\n",
    "    train = train[train[target]>-30]\n",
    "elif out_part=='clf':\n",
    "    train[target] = train[target].map(lambda x: 1 if x<-30 else 0)\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# 正規化の前処理(Null埋め, inf, -infの処理) \n",
    "for col in train.columns:\n",
    "    if col in ignore_list: continue\n",
    "        \n",
    "    train[col] = impute_feature(train, col)\n",
    "    test[col] = impute_feature(test, col)\n",
    "\n",
    "while True:\n",
    "    inf_list = []\n",
    "    for col in use_cols:\n",
    "        tmp = (train[col]==np.inf).sum()\n",
    "        tmp2 = (train[col]==-np.inf).sum()\n",
    "        if tmp>0:\n",
    "            inf_list.append(col)\n",
    "        if tmp2>0:\n",
    "            inf_list.append(col)\n",
    "        \n",
    "    for col in inf_list:\n",
    "        print(\"Ramain inf...\")\n",
    "        train[col] = impute_feature(train, col)\n",
    "        test[col] = impute_feature(test, col)\n",
    "    if len(inf_list)==0:\n",
    "        break\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (201917, 344) | Test: (123623, 344)\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# Construction Value Range Preprocessing\n",
    "def constraction(feature, is_viz=False, out_range=1.64):\n",
    "    if is_viz:\n",
    "        print('before:', feature.max(), feature.min())\n",
    "    std = feature.std()\n",
    "    avg = feature.mean()\n",
    "    z_val = (feature - avg)/std\n",
    "    \n",
    "    # Pass the Case No Outlier\n",
    "    try:\n",
    "        p_min = feature[feature>=out_range].min()\n",
    "        feature = np.where(z_val>=out_range, p_min, feature)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        m_max = feature[feature<=-1*out_range].max()\n",
    "        feature = np.where(z_val<=-1*out_range, m_max, feature)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    if is_viz:\n",
    "        print('after:', feature.max(), feature.min())   \n",
    "    return feature\n",
    "\n",
    "train_test = pd.concat([train, test], axis=0)\n",
    "for col in use_cols:\n",
    "    feature = train_test[col].values\n",
    "    \n",
    "    for i in range(const_cnt):\n",
    "        feature = constraction(feature)\n",
    "    \n",
    "    feature = feature.astype('float32')\n",
    "    train_test[col] = feature\n",
    "train = train_test[~train_test[target].isnull()]\n",
    "test = train_test[train_test[target].isnull()]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([train[use_cols], test[use_cols]]))   \n",
    "x_test = scaler.transform(test[use_cols])\n",
    "\n",
    "train[target] = 2**train[target]\n",
    "Y = train[target]\n",
    "\n",
    "# ========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# CVの準備\n",
    "fold = 6\n",
    "if out_part == 'all' or out_part=='clf':\n",
    "#     kfold = utils.read_pkl_gzip('../input/kfold_ods_all_fold6_seed328.gz')\n",
    "    kfold = utils.read_pkl_gzip('../input/kfold_ods_equal_seed328.gz')\n",
    "# elif out_part=='clf_out':\n",
    "#     kfold = utils.read_pkl_gzip('../input/ods_clf001_thres_kfold.gz')\n",
    "# elif out_part=='no_out_flg':\n",
    "#     kfold = utils.read_pkl_gzip('../input/ods_no_out_flg_kfold.gz')\n",
    "elif out_part=='no_out':\n",
    "    kfold = utils.read_pkl_gzip('../input/kfold_ods_no_out_fold6_seed328.gz')\n",
    "# elif out_part=='clf':\n",
    "    \n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\") \n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Target Min --- Train: 9.999999992192566e-11 Valid: 9.999999992192566e-11\n",
      "Target Min Count --- Train: 1839 Valid: 368\n",
      "Train on 168264 samples, validate on 33653 samples\n",
      "Epoch 1/30\n",
      " - 9s - loss: 7.0854 - RMSE: 7.0854 - val_loss: 2.1425 - val_RMSE: 2.1425\n",
      "Epoch 2/30\n",
      " - 8s - loss: 7.0625 - RMSE: 7.0625 - val_loss: 2.1389 - val_RMSE: 2.1389\n",
      "Epoch 3/30\n",
      " - 8s - loss: 7.0578 - RMSE: 7.0578 - val_loss: 2.1345 - val_RMSE: 2.1345\n",
      "Epoch 4/30\n",
      " - 8s - loss: 7.0542 - RMSE: 7.0542 - val_loss: 2.1329 - val_RMSE: 2.1329\n",
      "Epoch 5/30\n",
      " - 6s - loss: 7.0525 - RMSE: 7.0525 - val_loss: 2.1358 - val_RMSE: 2.1358\n",
      "Epoch 6/30\n",
      " - 6s - loss: 7.0498 - RMSE: 7.0498 - val_loss: 2.1332 - val_RMSE: 2.1332\n",
      "RMSE: 27.266526136454594 | SUM ERROR: 53795.58436492844\n",
      "Target Min --- Train: 9.999999992192566e-11 Valid: 9.999999992192566e-11\n",
      "Target Min Count --- Train: 1840 Valid: 367\n",
      "Train on 168264 samples, validate on 33653 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 5.0174 - RMSE: 5.0174 - val_loss: 12.2862 - val_RMSE: 12.2862\n",
      "Epoch 2/30\n",
      " - 6s - loss: 5.0148 - RMSE: 5.0148 - val_loss: 12.2861 - val_RMSE: 12.2861\n",
      "Epoch 3/30\n",
      " - 6s - loss: 5.0130 - RMSE: 5.0130 - val_loss: 12.2854 - val_RMSE: 12.2854\n",
      "Epoch 4/30\n",
      " - 6s - loss: 5.0100 - RMSE: 5.0100 - val_loss: 12.2862 - val_RMSE: 12.2862\n",
      "Epoch 5/30\n",
      " - 6s - loss: 5.0089 - RMSE: 5.0089 - val_loss: 12.2867 - val_RMSE: 12.2867\n",
      "RMSE: 1210.9915744968482 | SUM ERROR: 394385.5155201321\n",
      "Target Min --- Train: 9.999999992192566e-11 Valid: 9.999999992192566e-11\n",
      "Target Min Count --- Train: 1838 Valid: 369\n",
      "Train on 168264 samples, validate on 33653 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 6.7620 - RMSE: 6.7620 - val_loss: 3.5116 - val_RMSE: 3.5116\n",
      "Epoch 2/30\n",
      " - 6s - loss: 6.7605 - RMSE: 6.7605 - val_loss: 3.5157 - val_RMSE: 3.5157\n",
      "Epoch 3/30\n",
      " - 6s - loss: 6.7577 - RMSE: 6.7577 - val_loss: 3.5134 - val_RMSE: 3.5134\n",
      "RMSE: 222.85727892941603 | SUM ERROR: 102333.9296333215\n",
      "Target Min --- Train: 9.999999992192566e-11 Valid: 9.999999992192566e-11\n",
      "Target Min Count --- Train: 1840 Valid: 367\n",
      "Train on 168264 samples, validate on 33653 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 6.7112 - RMSE: 6.7112 - val_loss: 3.7364 - val_RMSE: 3.7364\n",
      "Epoch 2/30\n",
      " - 6s - loss: 6.7081 - RMSE: 6.7081 - val_loss: 3.7418 - val_RMSE: 3.7418\n",
      "Epoch 3/30\n",
      " - 6s - loss: 6.7054 - RMSE: 6.7054 - val_loss: 3.7402 - val_RMSE: 3.7402\n",
      "RMSE: 215.43571043105254 | SUM ERROR: 107734.15262412907\n",
      "Target Min --- Train: 9.999999992192566e-11 Valid: 9.999999992192566e-11\n",
      "Target Min Count --- Train: 1839 Valid: 368\n",
      "Train on 168264 samples, validate on 33653 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 5.0738 - RMSE: 5.0738 - val_loss: 11.8953 - val_RMSE: 11.8953\n",
      "Epoch 2/30\n",
      " - 6s - loss: 5.0712 - RMSE: 5.0712 - val_loss: 11.9026 - val_RMSE: 11.9026\n",
      "Epoch 3/30\n",
      " - 6s - loss: 5.0680 - RMSE: 5.0680 - val_loss: 11.9009 - val_RMSE: 11.9009\n",
      "RMSE: 1423.1434665255626 | SUM ERROR: 382763.4334198602\n",
      "Target Min --- Train: 9.999999992192566e-11 Valid: 9.999999992192566e-11\n",
      "Target Min Count --- Train: 1839 Valid: 368\n",
      "Train on 168265 samples, validate on 33652 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 6.6991 - RMSE: 6.6991 - val_loss: 3.7395 - val_RMSE: 3.7395\n",
      "Epoch 2/30\n",
      " - 6s - loss: 6.6955 - RMSE: 6.6955 - val_loss: 3.7425 - val_RMSE: 3.7425\n",
      "Epoch 3/30\n",
      " - 6s - loss: 6.6940 - RMSE: 6.6940 - val_loss: 3.7425 - val_RMSE: 3.7425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-23 02:08:53,947 utils 137 [INFO]    [<module>] \n",
      "#========================================================================\n",
      "# CV SCORE AVG: 549.4781753401306\n",
      "#======================================================================== \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 197.17449552144976 | SUM ERROR: 108848.03956705325\n",
      "Stacking Shape: (325540, 5)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nn_keras import mercari_1st_NN, corp_1st_LSTM, clf_NN\n",
    "#========================================================================\n",
    "# NN Model Setting \n",
    "N_EPOCHS = 15\n",
    "N_EPOCHS = 30\n",
    "# N_EPOCHS = 10\n",
    "# learning_rate = 1e-5\n",
    "# learning_rate = 1e-4\n",
    "learning_rate = 1e-3\n",
    "# lerning_rate = 3e-3\n",
    "\n",
    "is_LSTM = False\n",
    "break_cnt=0\n",
    "first_batch=6 # 7: 128\n",
    "first_batch=7 # 7: 128\n",
    "\n",
    "if is_LSTM:\n",
    "    model = corp_1st_LSTM(input_rows=1, input_cols=len(use_cols))\n",
    "    metric = RMSE\n",
    "elif out_part=='clf':\n",
    "    model = clf_NN(input_cols=len(use_cols))\n",
    "    metric = 'binary_crossentropy'\n",
    "else:\n",
    "    model = mercari_1st_NN(input_cols=len(use_cols))\n",
    "    metric = RMSE\n",
    "\n",
    "opt = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss=metric, optimizer=opt, metrics=[metric])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "]\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Result Box\n",
    "model_list = []\n",
    "result_list = []\n",
    "score_list = []\n",
    "val_pred_list = []\n",
    "test_pred = np.zeros(len(test))\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "\n",
    "for fold_no, (trn_idx, val_idx) in enumerate(zip(*kfold)):\n",
    "\n",
    "    #========================================================================\n",
    "    # Make Dataset\n",
    "    X_train, y_train = train.loc[train[key].isin(trn_idx), :][use_cols], Y.loc[train[key].isin(trn_idx)]\n",
    "    X_val, y_val = train.loc[train[key].isin(val_idx), :][use_cols], Y.loc[train[key].isin(val_idx)]\n",
    "    print(f\"Target Min --- Train: {y_train.min()} Valid: {y_val.min()}\")\n",
    "    print(f\"Target Min Count --- Train: {np.sum(y_train==y_train.min())} Valid: {np.sum(y_val==y_val.min())}\")\n",
    "    \n",
    "    X_train[:] = scaler.transform(X_train)\n",
    "    X_val[:] = scaler.transform(X_val)\n",
    "    X_train = X_train.as_matrix()\n",
    "    X_val = X_val.as_matrix()\n",
    "    \n",
    "    if is_LSTM:\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    #========================================================================\n",
    "    \n",
    "    cnt = -1\n",
    "    while True:\n",
    "        cnt+=1\n",
    "        # Fitting\n",
    "        # なぜか平均を引いてる？そのほうがfitするの？\n",
    "        # model.fit(X_train, y- y_mean, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "        #            validation_data=(X_val, y_val - y_mean), callbacks=callbacks )\n",
    "    #     model.fit(X_train, y_train, batch_size = batch_size, epochs = N_EPOCHS, verbose=2,\n",
    "    #                validation_data=(X_val, y_val), callbacks=callbacks )\n",
    "        \n",
    "        batch_size = 2**(first_batch + cnt)\n",
    "        model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val)\n",
    "                  , batch_size=2**(first_batch + cnt), epochs=N_EPOCHS\n",
    "                  , verbose=2, callbacks=callbacks)\n",
    "\n",
    "        # Prediction\n",
    "        if is_oof:\n",
    "            val_id_list = list(set(train_ids) - set(trn_idx))\n",
    "            X_val = train.loc[train[key].isin(val_id_list), :][use_cols]\n",
    "            y_val = Y.loc[train[key].isin(val_id_list)]\n",
    "            y_pred = model.predict(X_val)\n",
    "            tmp_val = train.loc[train[key].isin(val_id_list), :][[key, target]].set_index(key)\n",
    "            tmp_val['prediction'] = y_pred\n",
    "            df_stack['pred_{fold_no}'] = tmp_val['prediction']\n",
    "            del tmp_val\n",
    "            gc.collect()\n",
    "        else:\n",
    "            y_pred = model.predict(X_val)\n",
    "        out_cnt = (y_pred==np.inf).sum()+(y_pred==-np.inf).sum()+(y_pred!=y_pred).sum()\n",
    "        \n",
    "        if out_cnt>0:\n",
    "            print(\"Exist Inf or NaN\")\n",
    "            sys.exit()\n",
    "            \n",
    "        break\n",
    "    \n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], )\n",
    "    tmp_pred = model.predict(x_test)\n",
    "    test_pred += tmp_pred.reshape(tmp_pred.shape[0], )\n",
    "    test['prediction'] = test_pred\n",
    "    test = test[[key, 'prediction']]\n",
    "    \n",
    "    if is_plus:\n",
    "        y_val += (y_min-1)\n",
    "        y_pred += (y_min-1)\n",
    "        test_pred += (y_min-1)\n",
    "#     model_list.append(model)\n",
    "    \n",
    "    # Stack Prediction\n",
    "    df_pred = train.loc[train[key].isin(val_idx), :][[key, target]].copy()\n",
    "    df_pred['prediction'] = y_pred\n",
    "    result_list.append(df_pred)\n",
    "    \n",
    "    # Scoring\n",
    "    if out_part=='clf':\n",
    "        score = roc_auc_score(y_val, y_pred)\n",
    "        print(f'AUC: {score}')\n",
    "    else:\n",
    "        err = (y_val - y_pred)\n",
    "        score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        print(f'RMSE: {score} | SUM ERROR: {err.sum()}')\n",
    "    score_list.append(score)\n",
    "    #========================================================================\n",
    "\n",
    "cv_score = np.mean(score_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "#========================================================================\n",
    "# Stacking\n",
    "if is_oof:\n",
    "    pred_cols = [col for col in df_stack.columns if col.count('pred_')]\n",
    "    df_stack['prediction'] = df_stack[pred_cols].mean(axis=1)\n",
    "    df_stack.drop(pred_cols, axis=1, inplace=True)\n",
    "    df_stack.reset_index(inplace=True)\n",
    "    df_stack = pd.concat([df_stack, test], axis=0, ignore_index=True)\n",
    "    print(f\"DF Stack Shape: {df_stack.shape}\")    \n",
    "else:\n",
    "    test_pred /= fold\n",
    "    test['prediction'] = test_pred\n",
    "    stack_test = test[[key, 'prediction']]\n",
    "    result_list.append(stack_test)\n",
    "    df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "    df_stack = base.merge(df_pred, how='inner', on=key)\n",
    "    print(f\"Stacking Shape: {df_stack.shape}\")\n",
    "    del df_pred\n",
    "    gc.collect()\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.to_pkl_gzip(obj=df_stack[[key, target, 'prediction']], path=f'../stack/{start_time[4:12]}_elo_NN_stack_E{set_no+1}_row{len(train)}_outpart-{out_part}_{len(use_cols)}feat_const{const_cnt}_lr{learning_rate}_batch{batch_size}_epoch{N_EPOCHS}_CV{cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_prediction</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.052635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8703</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.023193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12033</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.036064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15899</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.156021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22270</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.023336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24212</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.261993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28197</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.511087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31233</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.101665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35783</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.403533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38861</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.012751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44679</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.201301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45540</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.320111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48573</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.059493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64558</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.095686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64993</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.034783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68911</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.102851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69442</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.220695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69680</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.091164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88784</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.234173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91246</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.054026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97166</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.138867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98923</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.057977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101853</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.027590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104852</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.015473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111328</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.005836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.024013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116431</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.232043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.675682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130469</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.061378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141580</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.166245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152736</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.189431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170042</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.082473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174994</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.019252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181589</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.290177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190466</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.044316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192291</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201877</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206394</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.487914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211197</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.220054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216716</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.601183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219638</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.005081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236740</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.024476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238024</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243915</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.045080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244274</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.028769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245229</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.109930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249303</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.128113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255347</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257718</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.007735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.035609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266429</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.019349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281664</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.049599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293809</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.080753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296651</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.107107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301466</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.522273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308065</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.007685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316023</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.015455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        new_prediction  prediction\n",
       "576                NaN   -0.052635\n",
       "8703               NaN   -0.023193\n",
       "12033              NaN   -0.036064\n",
       "15899              NaN   -0.156021\n",
       "22270              NaN   -0.023336\n",
       "24212              NaN   -0.261993\n",
       "28197              NaN   -0.511087\n",
       "31233              NaN   -0.101665\n",
       "35783              NaN   -0.403533\n",
       "38861              NaN   -0.012751\n",
       "44679              NaN   -0.201301\n",
       "45540              NaN   -0.320111\n",
       "48573              NaN   -0.059493\n",
       "64558              NaN   -0.095686\n",
       "64993              NaN   -0.034783\n",
       "68911              NaN   -0.102851\n",
       "69442              NaN   -0.220695\n",
       "69680              NaN   -0.091164\n",
       "88784              NaN   -0.234173\n",
       "91246              NaN   -0.054026\n",
       "97166              NaN   -0.138867\n",
       "98923              NaN   -0.057977\n",
       "101853             NaN   -0.027590\n",
       "104852             NaN   -0.015473\n",
       "111328             NaN   -0.005836\n",
       "113873             NaN   -0.024013\n",
       "116431             NaN   -0.232043\n",
       "126018             NaN   -0.675682\n",
       "130469             NaN   -0.061378\n",
       "141580             NaN   -0.166245\n",
       "152736             NaN   -0.189431\n",
       "170042             NaN   -0.082473\n",
       "174994             NaN   -0.019252\n",
       "181589             NaN   -0.290177\n",
       "190466             NaN   -0.044316\n",
       "192291             NaN   -0.001556\n",
       "201877             NaN   -0.016297\n",
       "206394             NaN   -0.487914\n",
       "211197             NaN   -0.220054\n",
       "216716             NaN   -0.601183\n",
       "219638             NaN   -0.005081\n",
       "236740             NaN   -0.024476\n",
       "238024             NaN   -0.001107\n",
       "243915             NaN   -0.045080\n",
       "244274             NaN   -0.028769\n",
       "245229             NaN   -0.109930\n",
       "249303             NaN   -0.128113\n",
       "255347             NaN   -0.009386\n",
       "257718             NaN   -0.007735\n",
       "264000             NaN   -0.035609\n",
       "266429             NaN   -0.019349\n",
       "281664             NaN   -0.049599\n",
       "293809             NaN   -0.080753\n",
       "296651             NaN   -0.107107\n",
       "301466             NaN   -0.522273\n",
       "308065             NaN   -0.007685\n",
       "316023             NaN   -0.015455"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stack['new_prediction'] = df_stack['prediction'].map(lambda x: np.log2(x))\n",
    "df_stack[['new_prediction', 'prediction']][df_stack['new_prediction'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prediction'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d0acfa77578c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prediction'"
     ]
    }
   ],
   "source": [
    "# utils.to_pkl_gzip(obj=df_stack, path=f'../stack/{start_time[4:12]}_elo_NN_stack_E{set_no+1}_row{len(train)}_outpart-{out_part}_{len(use_cols)}feat_const{const_cnt}_lr{learning_rate}_batch{batch_size}_epoch{N_EPOCHS}_CV{cv_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
