{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_no = '106_new_'\n",
    "stack_name='555_new_transaction'\n",
    "stack_name='555_new_transaction_noTE'\n",
    "#========================================================================\n",
    "# Dataset Load\n",
    "def elo_load_data(filename=''):\n",
    "    \n",
    "    if len(filename):\n",
    "        df = utils.read_df_pkl(path=f'../input/{filename}*.p')\n",
    "        return df\n",
    "    # read pickle\n",
    "    path_list = glob.glob(\"../input/*.p\")\n",
    "    for path in path_list:\n",
    "        filename = re.search(r'/([^/.]*).gz', path).group(1)\n",
    "        df = utils.read_df_pkl(path=f'../input/{filename}*.p')\n",
    "        \n",
    "train = elo_load_data('train')\n",
    "test = elo_load_data('test')\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#========================================================================\n",
    "# Args\n",
    "#========================================================================\n",
    "learning_rate = 0.05\n",
    "early_stopping_rounds = 150\n",
    "num_boost_round = 10000\n",
    "ignore_list = [key, target, 'merchant_id', 'purchase_date']\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/model')\n",
    "from params_lgbm import params_elo\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis')\n",
    "from model.lightgbm_ex import lightgbm_ex as lgb_ex\n",
    "\n",
    "sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "import utils\n",
    "from preprocessing import get_ordinal_mapping\n",
    "from utils import logger_func\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "if model_type=='lgb':\n",
    "    params = params_elo()\n",
    "    params['learning_rate'] = learning_rate\n",
    "\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# LGBM Setting\n",
    "model_type='lgb'\n",
    "metric = 'rmse'\n",
    "fold=5\n",
    "seed=1208\n",
    "group_col_name=key\n",
    "dummie=1\n",
    "oof_flg=True\n",
    "LGBM = lgb_ex(logger=logger, metric=metric, model_type=model_type, ignore_list=ignore_list)\n",
    "\n",
    "\n",
    "train, test, drop_list = LGBM.data_check(train=train, test=test, target=target, encode='dummie', exclude_category=True)\n",
    "\n",
    "ignore_list = [key, target, 'merchant_id', 'purchase_date']\n",
    "cat_list = utils.get_categorical_features(df=train, ignore_list=ignore_list)\n",
    "id_list = [col for col in train.columns if (col.count('_id') and col not in ignore_list) and not(col.count('TE'))]\n",
    "cat_list += id_list\n",
    "\n",
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "#========================================================================\n",
    "import lightgbm as lgb\n",
    "\n",
    "# TrainとCVのfoldを合わせる為、Train\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "raw_train = elo_load_data('train')\n",
    "y = raw_train[target].values\n",
    "tmp_train = raw_train.drop(target, axis=1).set_index(key)\n",
    "\n",
    "folds = KFold(n_splits=fold, shuffle=True, random_state=seed)\n",
    "kfold = folds.split(tmp_train, y)\n",
    "train_id_list = np.array(tmp_train.index)\n",
    "del raw_train, tmp_train, y\n",
    "gc.collect()\n",
    "\n",
    "y = train.set_index(key)[target]\n",
    "display(train.head())\n",
    "tmp_train = train.drop(target, axis=1).set_index(key)\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "\n",
    "oof = pd.Series(np.zeros(len(train)), index=tmp_train.index, name='oof_pred')\n",
    "score_list = []\n",
    "prediction = np.zeros(len(test))\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(kfold):\n",
    "    trn_idx = list(set(train_id_list[trn_idx]) & set(list(tmp_train.index)))\n",
    "    val_idx = list(set(train_id_list[val_idx]) & set(list(tmp_train.index)))\n",
    "    x_train, y_train = tmp_train[use_cols].loc[trn_idx, :], y.loc[trn_idx]\n",
    "    x_val, y_val = tmp_train[use_cols].loc[val_idx, :], y.loc[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "    lgb_eval = lgb.Dataset(data=x_val, label=y_val)\n",
    "    \n",
    "    lgbm = lgb.train(\n",
    "        train_set=lgb_train,\n",
    "        valid_sets=lgb_eval,\n",
    "        params=params,\n",
    "        verbose_eval=200,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        num_boost_round=num_boost_round,\n",
    "        categorical_feature=cat_list\n",
    "    )\n",
    "    \n",
    "    y_pred = lgbm.predict(x_val)\n",
    "    prediction += lgbm.predict(test[use_cols])\n",
    "    oof.loc[val_idx] = y_pred\n",
    "    \n",
    "    score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    logger.info(f\"Validation {n_fold}: RMSE {score}\")\n",
    "    score_list.append(score)\n",
    "\n",
    "#========================================================================\n",
    "# Result\n",
    "#========================================================================\n",
    "cv_score = np.mean(score_list)\n",
    "test['oof_pred'] = prediction/fold\n",
    "stack_pred = pd.concat([oof, test.set_index(key)['oof_pred']], axis=0)\n",
    "feature_num = len(use_cols)\n",
    "# cv_feim.to_csv(f'../valid/{start_time[4:12]}_lgb_feat{feature_num}_CV{cv_score}_lr{learning_rate}.csv', index=False)\n",
    "\n",
    "#========================================================================\n",
    "# STACKING\n",
    "logger.info(f'''\n",
    "train_stack shape: {oof.shape}\n",
    "test_stack shape: {test_pred.shape}\n",
    "''')\n",
    "utils.to_pkl_gzip(path=f\"../stack/{stack_name}_lgb_CV{str(cv_score).replace('.', '-')}_feat{feature_num}\", obj=stack_pred)\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = utils.read_pkl_gzip('../stack/555_new_transaction_lgb_CV2-9005820682193173_feat34.gz')\n",
    "stack_noTE = utils.read_pkl_gzip('../stack/555_new_transaction_noTE_lgb_CV2-9004063018367328_feat27.gz')\n",
    "print(stack.shape)\n",
    "print(stack_noTE.shape)\n",
    "stack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_agg = stack.reset_index().groupby(key)['oof_pred'].agg({\n",
    "    'oof1208_pred_mean@':'mean',\n",
    "    'oof1208_pred_std@':'std',\n",
    "    'oof1208_pred_max@':'max',\n",
    "    'oof1208_pred_min@':'min',\n",
    "    'oof1208_pred_skew@':'skew',\n",
    "})\n",
    "stack_agg['new_pred_max-min@'] =  stack_agg['oof1208_pred_max@'] - stack_agg['oof1208_pred_min@']\n",
    "\n",
    "stack_noTE_agg = stack_noTE.reset_index().groupby(key)['oof_pred'].agg({\n",
    "    'oof1208_pred_noTE_mean@':'mean',\n",
    "    'oof1208_pred_noTE_std@':'std',\n",
    "    'oof1208_pred_noTE_max@':'max',\n",
    "    'oof1208_pred_noTE_min@':'min',\n",
    "    'oof1208_pred_noTE_skew@':'skew',\n",
    "})\n",
    "stack_noTE_agg['new_pred_noTE_max-min@'] =  stack_noTE_agg['oof1208_pred_noTE_max@'] - stack_noTE_agg['oof1208_pred_noTE_min@']\n",
    "print(stack_agg.shape)\n",
    "print(stack_noTE_agg.shape)\n",
    "stack_noTE_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = utils.read_df_pkl('../input/base0*').set_index(key)\n",
    "base = base.join(stack_agg, how='left').join(stack_noTE_agg, how='left')\n",
    "prefix = ''\n",
    "\n",
    "train_id = elo_load_data('train')[key].values\n",
    "test_id = elo_load_data('test')[key].values\n",
    "for col in base.columns:\n",
    "    if col.count('@'):\n",
    "        utils.to_pkl_gzip(obj = base.loc[train_id, :][col].values, path=f'../features/1_first_valid/{feat_no}train_{prefix}{col}')\n",
    "        utils.to_pkl_gzip(obj = base.loc[test_id, :][col].values, path=f'../features/1_first_valid/{feat_no}test_{prefix}{col}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
